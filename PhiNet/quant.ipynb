{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    \"\"\"\n",
    "    Print the size of the model.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size_in_bytes = os.path.getsize(\"temp.p\")\n",
    "    if size_in_bytes < 1048576:\n",
    "        size_in_kb = size_in_bytes / 1024\n",
    "        print(\"{:.3f} KB\".format(size_in_kb))\n",
    "    else:\n",
    "        size_in_mb = size_in_bytes / 1048576\n",
    "        print(\"{:.3f} MB\".format(size_in_mb))\n",
    "    os.remove('temp.p')\n",
    "\n",
    "\n",
    "def measure_inference_latency(model, input_shape, device = None, repetitions=100, warmup_it = 10):\n",
    "    \"\"\"\n",
    "    Measures the inference time of the provided neural network model.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to evaluate.\n",
    "        input_shape: The shape of the input data expected by the model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the mean and standard deviation of the inference time\n",
    "               measured in milliseconds.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device.type  # Get the device where the model is located\n",
    "    \n",
    "    dummy_input = torch.randn(1, *input_shape, dtype=torch.float).to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # GPU warm-up\n",
    "    for _ in range(warmup_it):\n",
    "        _ = model(dummy_input)\n",
    "\n",
    "    # Measure inference time\n",
    "    timings = []\n",
    "    with torch.no_grad():\n",
    "        if device == 'cuda':\n",
    "            starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "            for rep in range(repetitions):\n",
    "                starter.record()\n",
    "                _ = model(dummy_input)\n",
    "                ender.record()\n",
    "                torch.cuda.synchronize()\n",
    "                curr_time = starter.elapsed_time(ender)\n",
    "                timings.append(curr_time)\n",
    "        else:  # CPU\n",
    "            for rep in range(repetitions):\n",
    "                start_time = time.time()\n",
    "                _ = model(dummy_input)\n",
    "                end_time = time.time()\n",
    "                elapsed_time = (end_time - start_time) * 1000.0  # Convert to milliseconds\n",
    "                timings.append(elapsed_time)\n",
    "\n",
    "    # Calculate mean and std\n",
    "    mean_time = np.mean(timings)\n",
    "    std_time = np.std(timings)\n",
    "\n",
    "    return mean_time, std_time\n",
    "\n",
    "\n",
    "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32), verbose=False):\n",
    "    \"\"\"\n",
    "    Tests whether two models are equivalent by comparing their outputs on random inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    model_1.to(device)\n",
    "    model_2.to(device)\n",
    "\n",
    "    for i in range(num_tests):\n",
    "        print(f\"Running test {i+1}/{num_tests}\") if verbose else None\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        y1 = model_1(x).detach().cpu().numpy()\n",
    "        y2 = model_2(x).detach().cpu().numpy()\n",
    "        print(\"Difference: \", np.max(np.abs(y1-y2))) if verbose else None\n",
    "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
    "            print(\"Model equivalence test sample failed: \")\n",
    "            print(y1)\n",
    "            print(y2)\n",
    "            return False\n",
    "    print(\"Model equivalence test passed!\")\n",
    "    return True\n",
    "\n",
    "def set_random_seeds(random_seed=0):\n",
    "    \"\"\"\n",
    "    Set all random seeds to a fixed value to make results reproducible.\n",
    "    \"\"\"\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantBLock(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(QuantBLock, self).__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized.\n",
    "        # This will only be used for inputs.\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "\n",
    "        # FP32 model\n",
    "        self.model_fp32 = model_fp32\n",
    "\n",
    "        # DeQuantStub converts tensors from quantized to floating point.\n",
    "        # This will only be used for outputs.\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        x = self.dequant(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization\n",
    "Original PhiNet definition not quantizable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): HSwish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): DepthwiseConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): HSwish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), groups=416, bias=False)\n",
      "        (6): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 401, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(401, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(401, 401, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=401, bias=False)\n",
      "        (5): BatchNorm2d(401, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(401, 66, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(66, 401, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(401, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 385, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(385, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(385, 385, kernel_size=(3, 3), stride=(2, 2), groups=385, bias=False)\n",
      "        (6): BatchNorm2d(385, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(385, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 385, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(385, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740, bias=False)\n",
      "        (5): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(740, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709, bias=False)\n",
      "        (6): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1357, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1357, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(1357, 1357, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1357, bias=False)\n",
      "        (5): BatchNorm2d(1357, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1357, 226, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(226, 1357, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(1357, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), groups=1296, bias=False)\n",
      "        (6): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(576, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteotremonti/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from micromind import PhiNet\n",
    "\n",
    "model = PhiNet(input_shape = [3, 224, 224], num_layers = 7, alpha = 3, beta = 0.75,t_zero = 6.0, include_top= True, num_classes = 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhiNet Convolutional Block 1\n",
    "First of two versions of PhiNet's convolutional blocks. In this case, the depth-wise convolutional block is defined as follows:\n",
    "\n",
    "```python\n",
    "DepthwiseConv2d(ch, ch, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1357, bias=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiNetConvBlock(\n",
      "  (_layers): ModuleList(\n",
      "    (0): Conv2d(144, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): HSwish()\n",
      "    (3): Dropout2d(p=0.05, inplace=False)\n",
      "    (4): DepthwiseConv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740, bias=False)\n",
      "    (5): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (6): HSwish()\n",
      "    (7): SEBlock(\n",
      "      (se_conv): Conv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (se_conv2): Conv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): HSwish()\n",
      "    )\n",
      "    (8): Conv2d(740, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "block1 = model._layers[6]\n",
    "print(block1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the original PhiNet convolutional block 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined Block\n",
      "PhiNetConv1(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740, bias=False)\n",
      "    (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(740, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Orginal BLock\n",
      "PhiNetConvBlock(\n",
      "  (_layers): ModuleList(\n",
      "    (0): Conv2d(144, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): HSwish()\n",
      "    (3): Dropout2d(p=0.05, inplace=False)\n",
      "    (4): DepthwiseConv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740, bias=False)\n",
      "    (5): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (6): HSwish()\n",
      "    (7): SEBlock(\n",
      "      (se_conv): Conv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (se_conv2): Conv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): HSwish()\n",
      "    )\n",
      "    (8): Conv2d(740, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PhiNetConv1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pointconv1 = nn.Sequential(\n",
    "                nn.Conv2d(144, 740, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(740, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "            )\n",
    "        self.drop = nn.Dropout2d(p=0.05)\n",
    "        self.depthwise = nn.Sequential(\n",
    "                nn.Conv2d(740,740,kernel_size=3,stride=1,padding=1,groups=740,bias=False),\n",
    "                nn.BatchNorm2d(740, eps=1e-3, momentum=0.999),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.se = torch.nn.Sequential(\n",
    "                nn.Conv2d(740, 123, kernel_size=1, stride=1, bias=False),\n",
    "                nn.Conv2d(123, 740, kernel_size=1, stride=1, bias=False),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.pointconv2 = nn.Sequential(\n",
    "                nn.Conv2d(740, 144, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(144, eps=1e-3, momentum=0.999),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pointconv1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pointconv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = PhiNetConv1()\n",
    "print(\"Defined Block\")\n",
    "print(test)\n",
    "print(\"Orginal BLock\")\n",
    "print(block1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization of the original convolutional block 1 of PhiNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1_fp32:\n",
      " PhiNetConv1(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740, bias=False)\n",
      "    (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(740, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Conv1_fp32_fused:\n",
      " PhiNetConv1(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 740, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740)\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(740, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " Sequential(\n",
      "  (0): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (1): PhiNetConv1(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): Conv2d(\n",
      "        144, 740, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2d(\n",
      "        740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): Conv2d(\n",
      "        740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Conv2d(\n",
      "        123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): Conv2d(\n",
      "        740, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantStub()\n",
      ")\n",
      "\n",
      "Conv1_int8: \n",
      " Sequential(\n",
      "  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (1): PhiNetConv1(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): QuantizedConv2d(144, 740, kernel_size=(1, 1), stride=(1, 1), scale=0.020500173792243004, zero_point=68)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): QuantizedConv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), scale=0.009569868445396423, zero_point=63, padding=(1, 1), groups=740)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): QuantizedConv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), scale=0.0015119541203603148, zero_point=60, bias=False)\n",
      "      (1): QuantizedConv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), scale=0.0008994026575237513, zero_point=62, bias=False)\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): QuantizedConv2d(740, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.00022830525995232165, zero_point=62)\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantize()\n",
      ")\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteotremonti/anaconda3/envs/py38/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "Conv1_fp32 = PhiNetConv1().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "Conv1_fp32_fused = copy.deepcopy(Conv1_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "Conv1_fp32.eval()\n",
    "Conv1_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "Conv1_fp32_fused = torch.ao.quantization.fuse_modules(Conv1_fp32_fused, [['pointconv1.0', 'pointconv1.1'], ['depthwise.0', 'depthwise.1'], ['pointconv2.0', 'pointconv2.1']])\n",
    "\n",
    "print(f\"Conv1_fp32:\\n {Conv1_fp32}\\n\")\n",
    "print(f\"Conv1_fp32_fused:\\n {Conv1_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=Conv1_fp32, model_2=Conv1_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,144,28,28)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "Conv1_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  Conv1_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "Conv1_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(Conv1_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {Conv1_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,144, 28, 28)\n",
    "    Conv1_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "Conv1_int8 = torch.ao.quantization.convert(Conv1_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv1_int8: \\n {Conv1_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = Conv1_int8(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the size of FP32 and INT8 blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 size:\n",
      "1.563 MB\n",
      "INT8 size:\n",
      "445.729 KB\n"
     ]
    }
   ],
   "source": [
    "print(f\"FP32 size:\")\n",
    "print_size_of_model(Conv1_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(Conv1_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the inference time of FP32 and INT8 blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 2.028 ms\n",
      "INT8 CPU Inference Latency: 1.738 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv1_fp32, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv1_int8, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dimensions of both the FP32 and INT8 models, the INT8 is indeed more compact in size, as expected. However, contrary to what the documentation suggests, the INT8 model has slower inference times compared to the original FP32 model. The documentation indicated an expected reduction in inference time of about half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification of PhiNet original convolutional block 1\n",
    "\n",
    "One of the problems could be that the number of channels is not divisible by 8. Therefore, we modify the convolutional block by making the channels divisible by 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined Block\n",
      "PhiNetConv1(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744, bias=False)\n",
      "    (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(744, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(123, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Orginal BLock\n",
      "PhiNetConvBlock(\n",
      "  (_layers): ModuleList(\n",
      "    (0): Conv2d(144, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): HSwish()\n",
      "    (3): Dropout2d(p=0.05, inplace=False)\n",
      "    (4): DepthwiseConv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740, bias=False)\n",
      "    (5): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (6): HSwish()\n",
      "    (7): SEBlock(\n",
      "      (se_conv): Conv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (se_conv2): Conv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): HSwish()\n",
      "    )\n",
      "    (8): Conv2d(740, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PhiNetConv1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pointconv1 = nn.Sequential(\n",
    "                nn.Conv2d(144, 744, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(744, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "            )\n",
    "        self.drop = nn.Dropout2d(p=0.05)\n",
    "        self.depthwise = nn.Sequential(\n",
    "                nn.Conv2d(744,744,kernel_size=3,stride=1,padding=1,groups=744,bias=False),\n",
    "                nn.BatchNorm2d(744, eps=1e-3, momentum=0.999),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.se = torch.nn.Sequential(\n",
    "                nn.Conv2d(744, 123, kernel_size=1, stride=1, bias=False),\n",
    "                nn.Conv2d(123, 744, kernel_size=1, stride=1, bias=False),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.pointconv2 = nn.Sequential(\n",
    "                nn.Conv2d(744, 144, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(144, eps=1e-3, momentum=0.999),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pointconv1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pointconv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = PhiNetConv1()\n",
    "print(\"Defined Block\")\n",
    "print(test)\n",
    "print(\"Orginal BLock\")\n",
    "print(block1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1_fp32:\n",
      " PhiNetConv1(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744, bias=False)\n",
      "    (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(744, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(123, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Conv1_fp32_fused:\n",
      " PhiNetConv1(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744)\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(744, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(123, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " Sequential(\n",
      "  (0): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (1): PhiNetConv1(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): Conv2d(\n",
      "        144, 744, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2d(\n",
      "        744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): Conv2d(\n",
      "        744, 123, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Conv2d(\n",
      "        123, 744, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): Conv2d(\n",
      "        744, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantStub()\n",
      ")\n",
      "\n",
      "Conv1_int8: \n",
      " Sequential(\n",
      "  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (1): PhiNetConv1(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): QuantizedConv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), scale=0.02039351500570774, zero_point=69)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): QuantizedConv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), scale=0.00912468321621418, zero_point=52, padding=(1, 1), groups=744)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): QuantizedConv2d(744, 123, kernel_size=(1, 1), stride=(1, 1), scale=0.001644390751607716, zero_point=66, bias=False)\n",
      "      (1): QuantizedConv2d(123, 744, kernel_size=(1, 1), stride=(1, 1), scale=0.0008745527593418956, zero_point=62, bias=False)\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): QuantizedConv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.0002712075365707278, zero_point=61)\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantize()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "Conv1_fp32 = PhiNetConv1().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "Conv1_fp32_fused = copy.deepcopy(Conv1_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "Conv1_fp32.eval()\n",
    "Conv1_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "Conv1_fp32_fused = torch.ao.quantization.fuse_modules(Conv1_fp32_fused, [['pointconv1.0', 'pointconv1.1'], ['depthwise.0', 'depthwise.1'], ['pointconv2.0', 'pointconv2.1']])\n",
    "\n",
    "print(f\"Conv1_fp32:\\n {Conv1_fp32}\\n\")\n",
    "print(f\"Conv1_fp32_fused:\\n {Conv1_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=Conv1_fp32, model_2=Conv1_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,144,28,28)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "Conv1_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  Conv1_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "Conv1_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(Conv1_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {Conv1_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,144, 28, 28)\n",
    "    Conv1_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "Conv1_int8 = torch.ao.quantization.convert(Conv1_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv1_int8: \\n {Conv1_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = Conv1_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 size:\n",
      "1.571 MB\n",
      "INT8 size:\n",
      "447.729 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(Conv1_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(Conv1_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 1.551 ms\n",
      "INT8 CPU Inference Latency: 0.776 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv1_fp32, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv1_int8, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 132.935 ms\n",
      "INT8 CPU Inference Latency: 23.820 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv1_fp32, device =\"cpu\", input_shape=(144, 224, 224))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv1_int8, device =\"cpu\", input_shape=(144, 224, 224))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, after the number of channels has been made divisible by 8, the model in INT8 is smaller in terms of memory. In contrast to the original block, the inference time is also reduced, with the reduction increasing as the input resolution increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhiNet Convolutional Block 2\n",
    "\n",
    "Second versions of PhiNet's convolutional blocks. In this case, the depth-wise convolutional block is defined as follows:\n",
    "\n",
    "```python\n",
    "DepthwiseConv2d(ch, ch, kernel_size=(3, 3), stride=(2, 2), groups=ch, bias=False)\n",
    "```\n",
    "Diffrently from the PhiNet Convolutional Block 1, here we have ` stride=(2, 2)` and `padding = 0`. Moreover, before the `Dropout2d(p=0.05)` it is inserted the following layer:\n",
    "\n",
    "```python \n",
    "ZeroPad2d((1, 1, 1, 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiNetConvBlock(\n",
      "  (_layers): ModuleList(\n",
      "    (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): HSwish()\n",
      "    (3): ZeroPad2d((1, 1, 1, 1))\n",
      "    (4): Dropout2d(p=0.05, inplace=False)\n",
      "    (5): DepthwiseConv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709, bias=False)\n",
      "    (6): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (7): HSwish()\n",
      "    (8): SEBlock(\n",
      "      (se_conv): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (se_conv2): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): HSwish()\n",
      "    )\n",
      "    (9): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "block2 = model._layers[7]\n",
    "print(block2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the original PhiNet convolutional block 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined Block\n",
      "PhiNetConv2(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709, bias=False)\n",
      "    (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Orginal BLock\n",
      "PhiNetConvBlock(\n",
      "  (_layers): ModuleList(\n",
      "    (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): HSwish()\n",
      "    (3): ZeroPad2d((1, 1, 1, 1))\n",
      "    (4): Dropout2d(p=0.05, inplace=False)\n",
      "    (5): DepthwiseConv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709, bias=False)\n",
      "    (6): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (7): HSwish()\n",
      "    (8): SEBlock(\n",
      "      (se_conv): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (se_conv2): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): HSwish()\n",
      "    )\n",
      "    (9): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PhiNetConv2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pointconv1 = nn.Sequential(\n",
    "                nn.Conv2d(144, 709, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(709, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "            )\n",
    "        self.pad = nn.ZeroPad2d(1)\n",
    "        self.drop = nn.Dropout2d(p=0.05)\n",
    "        self.depthwise = nn.Sequential(\n",
    "                nn.Conv2d(709,709,kernel_size=3,stride=2,padding=0,groups=709,bias=False),\n",
    "                nn.BatchNorm2d(709, eps=1e-3, momentum=0.999),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.se = torch.nn.Sequential(\n",
    "                nn.Conv2d(709, 118, kernel_size=1, stride=1, bias=False),\n",
    "                nn.Conv2d(118, 709, kernel_size=1, stride=1, bias=False),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.pointconv2 = nn.Sequential(\n",
    "                nn.Conv2d(709, 288, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(288, eps=1e-3, momentum=0.999),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pointconv1(x)\n",
    "        x = self.pad(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pointconv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = PhiNetConv2()\n",
    "print(\"Defined Block\")\n",
    "print(test)\n",
    "print(\"Orginal BLock\")\n",
    "print(block2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization of the original convolutional block 2 of PhiNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2_fp32:\n",
      " PhiNetConv2(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709, bias=False)\n",
      "    (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Conv2_fp32_fused:\n",
      " PhiNetConv2(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709)\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " Sequential(\n",
      "  (0): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (1): PhiNetConv2(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): Conv2d(\n",
      "        144, 709, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2d(\n",
      "        709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): Conv2d(\n",
      "        709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Conv2d(\n",
      "        118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): Conv2d(\n",
      "        709, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantStub()\n",
      ")\n",
      "\n",
      "Conv2_int8: \n",
      " Sequential(\n",
      "  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (1): PhiNetConv2(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): QuantizedConv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), scale=0.020521609112620354, zero_point=68)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): QuantizedConv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), scale=0.011568625457584858, zero_point=83, groups=709)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): QuantizedConv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), scale=0.0015906733460724354, zero_point=64, bias=False)\n",
      "      (1): QuantizedConv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), scale=0.001034931861795485, zero_point=65, bias=False)\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): QuantizedConv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.0002751421998254955, zero_point=66)\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantize()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "Conv2_fp32 = PhiNetConv2().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "Conv2_fp32_fused = copy.deepcopy(Conv2_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "Conv2_fp32.eval()\n",
    "Conv2_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "Conv2_fp32_fused = torch.ao.quantization.fuse_modules(Conv2_fp32_fused, [['pointconv1.0', 'pointconv1.1'], ['depthwise.0', 'depthwise.1'], ['pointconv2.0', 'pointconv2.1']])\n",
    "\n",
    "print(f\"Conv2_fp32:\\n {Conv2_fp32}\\n\")\n",
    "print(f\"Conv2_fp32_fused:\\n {Conv2_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=Conv2_fp32, model_2=Conv2_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,144,28,28)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "Conv2_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  Conv2_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "Conv2_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(Conv2_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {Conv2_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,144, 28, 28)\n",
    "    Conv2_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "Conv2_int8 = torch.ao.quantization.convert(Conv2_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv2_int8: \\n {Conv2_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = Conv2_int8(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the size of FP32 and INT8 blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 size:\n",
      "1.862 MB\n",
      "INT8 size:\n",
      "523.104 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(Conv2_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(Conv2_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the inference time of FP32 and INT8 blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 1.331 ms\n",
      "INT8 CPU Inference Latency: 1.118 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv2_fp32, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv2_int8, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dimensions of both the FP32 and INT8 models, the INT8 is indeed more compact in size, as expected. However, contrary to what the documentation suggests, the INT8 model has slower inference times compared to the original FP32 model. The documentation indicated an expected reduction in inference time of about half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification of PhiNet original convolutional block 2\n",
    "\n",
    "One of the problems could be that the number of channels is not divisible by 8. Therefore, we modify the convolutional block by making the channels divisible by 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined Block\n",
      "PhiNetConv2(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), groups=712, bias=False)\n",
      "    (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Orginal BLock\n",
      "PhiNetConvBlock(\n",
      "  (_layers): ModuleList(\n",
      "    (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): HSwish()\n",
      "    (3): ZeroPad2d((1, 1, 1, 1))\n",
      "    (4): Dropout2d(p=0.05, inplace=False)\n",
      "    (5): DepthwiseConv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709, bias=False)\n",
      "    (6): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (7): HSwish()\n",
      "    (8): SEBlock(\n",
      "      (se_conv): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (se_conv2): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): HSwish()\n",
      "    )\n",
      "    (9): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PhiNetConv2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pointconv1 = nn.Sequential(\n",
    "                nn.Conv2d(144, 712, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(712, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "            )\n",
    "        self.pad = nn.ZeroPad2d(1)\n",
    "        self.drop = nn.Dropout2d(p=0.05)\n",
    "        self.depthwise = nn.Sequential(\n",
    "                nn.Conv2d(712,712,kernel_size=3,stride=2,padding=0,groups=712,bias=False),\n",
    "                nn.BatchNorm2d(712, eps=1e-3, momentum=0.999),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.se = torch.nn.Sequential(\n",
    "                nn.Conv2d(712, 118, kernel_size=1, stride=1, bias=False),\n",
    "                nn.Conv2d(118, 712, kernel_size=1, stride=1, bias=False),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.pointconv2 = nn.Sequential(\n",
    "                nn.Conv2d(712, 288, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(288, eps=1e-3, momentum=0.999),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pointconv1(x)\n",
    "        x = self.pad(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pointconv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = PhiNetConv2()\n",
    "print(\"Defined Block\")\n",
    "print(test)\n",
    "print(\"Orginal BLock\")\n",
    "print(block2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2_fp32:\n",
      " PhiNetConv2(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), groups=712, bias=False)\n",
      "    (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Conv2_fp32_fused:\n",
      " PhiNetConv2(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), groups=712)\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " Sequential(\n",
      "  (0): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (1): PhiNetConv2(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): Conv2d(\n",
      "        144, 712, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2d(\n",
      "        712, 712, kernel_size=(3, 3), stride=(2, 2), groups=712\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): Conv2d(\n",
      "        712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Conv2d(\n",
      "        118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): Conv2d(\n",
      "        712, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantStub()\n",
      ")\n",
      "\n",
      "Conv2_int8: \n",
      " Sequential(\n",
      "  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (1): PhiNetConv2(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): QuantizedConv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), scale=0.02047821506857872, zero_point=68)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): QuantizedConv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), scale=0.009935283102095127, zero_point=68, groups=712)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): QuantizedConv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), scale=0.0016505611129105091, zero_point=65, bias=False)\n",
      "      (1): QuantizedConv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), scale=0.0009333100751973689, zero_point=61, bias=False)\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): QuantizedConv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.0002705159713514149, zero_point=62)\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantize()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "Conv2_fp32 = PhiNetConv2().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "Conv2_fp32_fused = copy.deepcopy(Conv2_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "Conv2_fp32.eval()\n",
    "Conv2_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "Conv2_fp32_fused = torch.ao.quantization.fuse_modules(Conv2_fp32_fused, [['pointconv1.0', 'pointconv1.1'], ['depthwise.0', 'depthwise.1'], ['pointconv2.0', 'pointconv2.1']])\n",
    "\n",
    "print(f\"Conv2_fp32:\\n {Conv2_fp32}\\n\")\n",
    "print(f\"Conv2_fp32_fused:\\n {Conv2_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=Conv2_fp32, model_2=Conv2_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,144,28,28)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "Conv2_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  Conv2_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "Conv2_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(Conv2_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {Conv2_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,144, 28, 28)\n",
    "    Conv2_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "Conv2_int8 = torch.ao.quantization.convert(Conv2_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv2_int8: \\n {Conv2_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = Conv2_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 size:\n",
      "1.870 MB\n",
      "INT8 size:\n",
      "524.979 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(Conv2_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(Conv2_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 1.310 ms\n",
      "INT8 CPU Inference Latency: 1.042 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv2_fp32, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv2_int8, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the original convolutional block 1 of PhiNet. Making the channels divisible by 8 is not enough. In fact, the model in INT8 is inferior in terms of memomics but slower in terms of inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification of PhiNet original convolutional block 2 (V2)\n",
    "The number of channels of the convolutional blocks is made divisible by 8 and `nn.ZeroPad2d(1)` is removed and placed inside the Depthwise Convolution using `Conv2d(padding = 1)`. The depth-wise convolution becomes:\n",
    "\n",
    "```python\n",
    "Conv2d(ch, ch, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=ch, bias=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined Block\n",
      "PhiNetConv2(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712, bias=False)\n",
      "    (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Orginal BLock\n",
      "PhiNetConvBlock(\n",
      "  (_layers): ModuleList(\n",
      "    (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): HSwish()\n",
      "    (3): ZeroPad2d((1, 1, 1, 1))\n",
      "    (4): Dropout2d(p=0.05, inplace=False)\n",
      "    (5): DepthwiseConv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709, bias=False)\n",
      "    (6): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (7): HSwish()\n",
      "    (8): SEBlock(\n",
      "      (se_conv): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (se_conv2): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): HSwish()\n",
      "    )\n",
      "    (9): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define PhiNet Convolutional Block without number of channel divisible by 8\n",
    "class PhiNetConv2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized\n",
    "        self.pointconv1 = nn.Sequential(\n",
    "                nn.Conv2d(144, 712, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(712, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "            )\n",
    "        self.drop = nn.Dropout2d(p=0.05)\n",
    "        self.depthwise = nn.Sequential(\n",
    "                nn.Conv2d(712,712,kernel_size=3,stride=2,padding=1,groups=712,bias=False),\n",
    "                nn.BatchNorm2d(712, eps=1e-3, momentum=0.999),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.se = torch.nn.Sequential(\n",
    "                nn.Conv2d(712, 118, kernel_size=1, stride=1, bias=False),\n",
    "                nn.Conv2d(118, 712, kernel_size=1, stride=1, bias=False),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.pointconv2 = nn.Sequential(\n",
    "                nn.Conv2d(712, 288, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(288, eps=1e-3, momentum=0.999),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pointconv1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pointconv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = PhiNetConv2()\n",
    "print(\"Defined Block\")\n",
    "print(test)\n",
    "print(\"Orginal BLock\")\n",
    "print(block2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2_fp32:\n",
      " PhiNetConv2(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712, bias=False)\n",
      "    (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Conv2_fp32_fused:\n",
      " PhiNetConv2(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712)\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " Sequential(\n",
      "  (0): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (1): PhiNetConv2(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): Conv2d(\n",
      "        144, 712, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2d(\n",
      "        712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): Conv2d(\n",
      "        712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Conv2d(\n",
      "        118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): Conv2d(\n",
      "        712, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantStub()\n",
      ")\n",
      "\n",
      "Conv2_int8: \n",
      " Sequential(\n",
      "  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (1): PhiNetConv2(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): QuantizedConv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), scale=0.02047821506857872, zero_point=68)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): QuantizedConv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), scale=0.009935283102095127, zero_point=68, padding=(1, 1), groups=712)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): QuantizedConv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), scale=0.0016505611129105091, zero_point=65, bias=False)\n",
      "      (1): QuantizedConv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), scale=0.0009333100751973689, zero_point=61, bias=False)\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): QuantizedConv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.0002705159713514149, zero_point=62)\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantize()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "Conv2_fp32 = PhiNetConv2().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "Conv2_fp32_fused = copy.deepcopy(Conv2_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "Conv2_fp32.eval()\n",
    "Conv2_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "Conv2_fp32_fused = torch.ao.quantization.fuse_modules(Conv2_fp32_fused, [['pointconv1.0', 'pointconv1.1'], ['depthwise.0', 'depthwise.1'], ['pointconv2.0', 'pointconv2.1']])\n",
    "\n",
    "print(f\"Conv2_fp32:\\n {Conv2_fp32}\\n\")\n",
    "print(f\"Conv2_fp32_fused:\\n {Conv2_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=Conv2_fp32, model_2=Conv2_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,144,28,28)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "Conv2_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  Conv2_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "Conv2_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(Conv2_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {Conv2_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,144, 28, 28)\n",
    "    Conv2_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "Conv2_int8 = torch.ao.quantization.convert(Conv2_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv2_int8: \\n {Conv2_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = Conv2_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 size:\n",
      "1.571 MB\n",
      "INT8 size:\n",
      "447.729 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(Conv1_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(Conv1_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 1.204 ms\n",
      "INT8 CPU Inference Latency: 0.551 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv2_fp32, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv2_int8, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 56.940 ms\n",
      "INT8 CPU Inference Latency: 10.360 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv2_fp32, device =\"cpu\", input_shape=(144, 224, 224))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv2_int8, device =\"cpu\", input_shape=(144, 224, 224))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, after making the number of channels divisible by 8 and removing `ZeroPad2d`, the model in INT8 is smaller in terms of memory and faster in terms of inference time, with the reduction increasing as the input resolution increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhiNet Convolutional Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the `layer` variable, I check whether the changes are effective on all PhiNet blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined Block\n",
      "ConvBlock(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "    (3): Dropout2d(p=0.05, inplace=False)\n",
      "    (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416, bias=False)\n",
      "    (5): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (6): Hardswish()\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Orginal Block\n",
      "PhiNetConvBlock(\n",
      "  (_layers): ModuleList(\n",
      "    (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): HSwish()\n",
      "    (3): ZeroPad2d((1, 1, 1, 1))\n",
      "    (4): Dropout2d(p=0.05, inplace=False)\n",
      "    (5): DepthwiseConv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), groups=416, bias=False)\n",
      "    (6): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (7): HSwish()\n",
      "    (8): SEBlock(\n",
      "      (se_conv): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (se_conv2): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): HSwish()\n",
      "    )\n",
      "    (9): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "layer = 3\n",
    "block = model._layers[layer]\n",
    "resolution = [112, 56, 56, 28, 28, 14, 14, 7]\n",
    "res = resolution[layer-3]\n",
    "\n",
    "input = block._layers[0].in_channels\n",
    "channel = _make_divisible(block._layers[0].out_channels, 8)\n",
    "stride = block._layers[-6].stride\n",
    "if isinstance(block._layers[3], nn.ZeroPad2d):\n",
    "    padding = 1\n",
    "else:\n",
    "    padding = block._layers[-6].padding\n",
    "se_ch = block._layers[-3].se_conv.out_channels\n",
    "output = block._layers[-2].out_channels\n",
    "\n",
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "                nn.Conv2d(input, channel, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(channel, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "                nn.Dropout2d(p=0.05),\n",
    "                nn.Conv2d(channel,channel,kernel_size=3,stride=stride,padding=padding,groups=channel,bias=False),\n",
    "                nn.BatchNorm2d(channel, eps=1e-3, momentum=0.999),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "                torch.nn.Sequential(\n",
    "                    nn.Conv2d(channel, se_ch, kernel_size=1, stride=1, bias=False),\n",
    "                    nn.Conv2d(se_ch, channel, kernel_size=1, stride=1, bias=False),\n",
    "                    torch.nn.Hardswish(inplace=True),\n",
    "                ),\n",
    "                nn.Conv2d(channel, output, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(output, eps=1e-3, momentum=0.999),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = ConvBlock()\n",
    "print(\"Defined Block\")\n",
    "print(test)\n",
    "print(\"Orginal Block\")\n",
    "print(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2_fp32:\n",
      " ConvBlock(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "    (3): Dropout2d(p=0.05, inplace=False)\n",
      "    (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416, bias=False)\n",
      "    (5): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (6): Hardswish()\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Conv2_fp32_fused:\n",
      " ConvBlock(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "    (3): Dropout2d(p=0.05, inplace=False)\n",
      "    (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416)\n",
      "    (5): Identity()\n",
      "    (6): Hardswish()\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (9): Identity()\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " Sequential(\n",
      "  (0): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (1): ConvBlock(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(\n",
      "        72, 416, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (3): Dropout2d(p=0.05, inplace=False)\n",
      "      (4): Conv2d(\n",
      "        416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (5): Identity()\n",
      "      (6): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): Conv2d(\n",
      "          416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "        (1): Conv2d(\n",
      "          69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "        (2): Hardswish(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (8): Conv2d(\n",
      "        416, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (9): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantStub()\n",
      ")\n",
      "\n",
      "Conv2_int8: \n",
      " Sequential(\n",
      "  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (1): ConvBlock(\n",
      "    (conv): Sequential(\n",
      "      (0): QuantizedConv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), scale=0.01926155760884285, zero_point=65)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "      (3): Dropout2d(p=0.05, inplace=False)\n",
      "      (4): QuantizedConv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), scale=0.009451590478420258, zero_point=73, padding=(1, 1), groups=416)\n",
      "      (5): Identity()\n",
      "      (6): QuantizedHardswish()\n",
      "      (7): Sequential(\n",
      "        (0): QuantizedConv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), scale=0.001384903909638524, zero_point=58, bias=False)\n",
      "        (1): QuantizedConv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), scale=0.0009201678331010044, zero_point=63, bias=False)\n",
      "        (2): QuantizedHardswish()\n",
      "      )\n",
      "      (8): QuantizedConv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.000275766768027097, zero_point=66)\n",
      "      (9): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantize()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "Conv_fp32 = ConvBlock().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "Conv_fp32_fused = copy.deepcopy(Conv_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "Conv_fp32.eval()\n",
    "Conv_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "Conv_fp32_fused = torch.ao.quantization.fuse_modules(Conv_fp32_fused, [['conv.0', 'conv.1'], ['conv.4', 'conv.5'], ['conv.8', 'conv.9']])\n",
    "\n",
    "print(f\"Conv2_fp32:\\n {Conv_fp32}\\n\")\n",
    "print(f\"Conv2_fp32_fused:\\n {Conv_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=Conv_fp32, model_2=Conv_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,input,res,res)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "Conv_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  Conv_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "Conv_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(Conv_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {Conv_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,input, res, res)\n",
    "    Conv_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "Conv_int8 = torch.ao.quantization.convert(Conv_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv2_int8: \\n {Conv_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "result = Conv_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 size:\n",
      "492.038 KB\n",
      "INT8 size:\n",
      "151.104 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(Conv_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(Conv_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 4.237 ms\n",
      "INT8 CPU Inference Latency: 1.111 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv_fp32, device =\"cpu\", input_shape=(input,res,res))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv_int8, device =\"cpu\", input_shape=(input,res,res))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SeparableConvolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also check the Separable Convolutional BLock in the PhiNet input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputBlock(\n",
      "  (pad): ZeroPad2d((0, 1, 0, 1))\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "    (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (3): Hardswish()\n",
      "  )\n",
      ")\n",
      "SeparableConv2d(\n",
      "  (_layers): ModuleList(\n",
      "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "    (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (3): HSwish()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class InputBlock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pad = nn.ZeroPad2d((0,1,0,1))\n",
    "        self.conv = nn.Sequential(\n",
    "                nn.Conv2d(3, 3, kernel_size=3, stride=2, padding=0, groups=3, bias=False),\n",
    "                nn.Conv2d(3, 144, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(144, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pad(x)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = InputBlock()\n",
    "print(test)\n",
    "\n",
    "block = model._layers[1]\n",
    "print(block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2_fp32:\n",
      " InputBlock(\n",
      "  (pad): ZeroPad2d((0, 1, 0, 1))\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "    (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (3): Hardswish()\n",
      "  )\n",
      ")\n",
      "\n",
      "Conv2_fp32_fused:\n",
      " InputBlock(\n",
      "  (pad): ZeroPad2d((0, 1, 0, 1))\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "    (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): Identity()\n",
      "    (3): Hardswish()\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " Sequential(\n",
      "  (0): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (1): InputBlock(\n",
      "    (pad): ZeroPad2d((0, 1, 0, 1))\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(\n",
      "        3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Conv2d(\n",
      "        3, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (2): Identity()\n",
      "      (3): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantStub()\n",
      ")\n",
      "\n",
      "Conv2_int8: \n",
      " Sequential(\n",
      "  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (1): InputBlock(\n",
      "    (pad): ZeroPad2d((0, 1, 0, 1))\n",
      "    (conv): Sequential(\n",
      "      (0): QuantizedConv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), scale=0.011397001333534718, zero_point=59, groups=3, bias=False)\n",
      "      (1): QuantizedConv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.00975711364299059, zero_point=63)\n",
      "      (2): Identity()\n",
      "      (3): QuantizedHardswish()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantize()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "InConv_fp32 = InputBlock().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "InConv_fp32_fused = copy.deepcopy(InConv_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "InConv_fp32.eval()\n",
    "InConv_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "InConv_fp32_fused = torch.ao.quantization.fuse_modules(InConv_fp32_fused, [['conv.1', 'conv.2']])\n",
    "\n",
    "print(f\"Conv2_fp32:\\n {InConv_fp32}\\n\")\n",
    "print(f\"Conv2_fp32_fused:\\n {InConv_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=InConv_fp32, model_2=InConv_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,3,224,224)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "InConv_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  InConv_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "InConv_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(InConv_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {InConv_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,3, 224, 224)\n",
    "    InConv_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "InConv_int8 = torch.ao.quantization.convert(InConv_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv2_int8: \\n {InConv_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "result = InConv_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 size:\n",
      "6.117 KB\n",
      "INT8 size:\n",
      "7.079 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(InConv_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(InConv_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 0.783 ms\n",
      "INT8 CPU Inference Latency: 0.460 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(InConv_fp32, device =\"cpu\", input_shape=(3,224,224))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(InConv_int8, device =\"cpu\", input_shape=(3,224,224))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the FP32 block, the INT8 block is larger in terms of B. This could be due to the addition of the two fake quantization blocks. In addition, the slightly worse inference time is due to the fact that the channels are not divisible by 8 in depth convolution. Although the performance in general is slightly worse, in the PhiNet complex, it does not affect the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete PhiNet Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phinet_quant import PhiNetConvBlock, SeparableConv2d, DepthwiseConv2d\n",
    "\n",
    "def phinet_fuse_modules(model):\n",
    "    for basic_block_name, basic_block in model._layers.named_children():\n",
    "        if isinstance(basic_block, SeparableConv2d):\n",
    "            torch.ao.quantization.fuse_modules(basic_block, [[\"_layers.1\", \"_layers.2\"]], inplace=True)\n",
    "        if isinstance(basic_block, PhiNetConvBlock) and len(basic_block._layers) == 6:\n",
    "            torch.ao.quantization.fuse_modules(basic_block, [[\"_layers.1\", \"_layers.2\"], [\"_layers.4\", \"_layers.5\"]], inplace=True)\n",
    "        elif isinstance(basic_block, PhiNetConvBlock):\n",
    "            torch.ao.quantization.fuse_modules(basic_block, [[\"_layers.0\", \"_layers.1\"], [\"_layers.4\", \"_layers.5\"], [\"_layers.8\", \"_layers.9\"]], inplace=True)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def remove_depthwise(model):\n",
    "    def convert_to_conv2d(depthwise_conv2d):\n",
    "        in_channels = depthwise_conv2d.in_channels\n",
    "        depth_multiplier = depthwise_conv2d.out_channels // in_channels\n",
    "        kernel_size = depthwise_conv2d.kernel_size\n",
    "        stride = depthwise_conv2d.stride\n",
    "        padding = depthwise_conv2d.padding\n",
    "        dilation = depthwise_conv2d.dilation\n",
    "        bias = depthwise_conv2d.bias is not None\n",
    "        padding_mode = depthwise_conv2d.padding_mode\n",
    "\n",
    "        # Create an equivalent nn.Conv2d layer\n",
    "        conv2d_layer = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=in_channels * depth_multiplier,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=in_channels,  # Set groups to in_channels for depthwise convolution\n",
    "            bias=bias,\n",
    "            padding_mode=padding_mode,\n",
    "        )\n",
    "\n",
    "        # If bias was not used in the original depthwise_conv2d, set bias to None in conv2d_layer\n",
    "        if not bias:\n",
    "            conv2d_layer.bias = None\n",
    "\n",
    "        return conv2d_layer\n",
    "\n",
    "    for name, module in model._layers.named_children():\n",
    "        if isinstance(module, PhiNetConvBlock):\n",
    "            for i, layer in enumerate(module._layers.children()):\n",
    "                if isinstance(layer, DepthwiseConv2d):\n",
    "                    module._layers[i] = convert_to_conv2d(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of PhiNet with the modifications shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal Model: PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): DepthwiseConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416, bias=False)\n",
      "        (5): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400, bias=False)\n",
      "        (5): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 66, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(66, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "        (5): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744, bias=False)\n",
      "        (5): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(124, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712, bias=False)\n",
      "        (5): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)\n",
      "        (5): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 226, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(226, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296, bias=False)\n",
      "        (5): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(576, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Model without Depthwise: PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416, bias=False)\n",
      "        (5): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400, bias=False)\n",
      "        (5): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 66, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(66, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "        (5): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744, bias=False)\n",
      "        (5): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(124, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712, bias=False)\n",
      "        (5): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)\n",
      "        (5): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 226, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(226, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296, bias=False)\n",
      "        (5): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(576, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from phinet_quant import PhiNet\n",
    "\n",
    "set_random_seeds(0)\n",
    "\n",
    "# Import the quantizeble model\n",
    "model_fp32 = PhiNet(input_shape = [3, 224, 224], num_layers=7, alpha= 3.0, beta= 0.75,t_zero= 6.0, include_top= True, num_classes=10)\n",
    "print(f\"Orginal Model: {model_fp32}\\n\")\n",
    "remove_depthwise(model_fp32)\n",
    "print(f\"Model without Depthwise: {model_fp32}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fp32:\n",
      " PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416, bias=False)\n",
      "        (5): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400, bias=False)\n",
      "        (5): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 66, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(66, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "        (5): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744, bias=False)\n",
      "        (5): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(124, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712, bias=False)\n",
      "        (5): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)\n",
      "        (5): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 226, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(226, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296, bias=False)\n",
      "        (5): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(576, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "model_fp32_fused:\n",
      " PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Identity()\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)\n",
      "        (2): Identity()\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (5): Identity()\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 66, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(66, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(124, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 226, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(226, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " Sequential(\n",
      "  (0): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (1): PhiNet(\n",
      "    (_layers): ModuleList(\n",
      "      (0): ZeroPad2d((0, 1, 0, 1))\n",
      "      (1): SeparableConv2d(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Conv2d(\n",
      "            3, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (2): Identity()\n",
      "          (3): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Dropout2d(p=0.05, inplace=False)\n",
      "          (1): Conv2d(\n",
      "            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (2): Identity()\n",
      "          (3): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (4): Conv2d(\n",
      "            144, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            72, 416, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            416, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            72, 400, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              400, 66, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              66, 400, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            400, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (5): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            72, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            384, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            144, 744, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              744, 124, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              124, 744, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            744, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (7): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            144, 712, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            712, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            288, 1360, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              1360, 226, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              226, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            1360, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (9): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            288, 1296, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            1296, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): Linear(\n",
      "        in_features=576, out_features=10, bias=True\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantStub()\n",
      ")\n",
      "\n",
      "model_int8:\n",
      " Sequential(\n",
      "  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (1): PhiNet(\n",
      "    (_layers): ModuleList(\n",
      "      (0): ZeroPad2d((0, 1, 0, 1))\n",
      "      (1): SeparableConv2d(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), scale=0.011379190720617771, zero_point=59, groups=3, bias=False)\n",
      "          (1): QuantizedConv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.00973123125731945, zero_point=63)\n",
      "          (2): Identity()\n",
      "          (3): QuantizedHardswish()\n",
      "        )\n",
      "      )\n",
      "      (2): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Dropout2d(p=0.05, inplace=False)\n",
      "          (1): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.0031032226979732513, zero_point=57, padding=(1, 1), groups=144)\n",
      "          (2): Identity()\n",
      "          (3): QuantizedHardswish()\n",
      "          (4): QuantizedConv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.000649958907160908, zero_point=61)\n",
      "          (5): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), scale=0.00037546688690781593, zero_point=64)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), scale=0.0001292256056331098, zero_point=68, padding=(1, 1), groups=416)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), scale=1.3731531907978933e-05, zero_point=69, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), scale=4.6573859435738996e-06, zero_point=58, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=3.327025115140714e-05, zero_point=68\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), scale=1.5032197552500293e-05, zero_point=66)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), scale=9.052166205947287e-06, zero_point=64)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), scale=3.314784862595843e-06, zero_point=55, padding=(1, 1), groups=400)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(400, 66, kernel_size=(1, 1), stride=(1, 1), scale=2.6996397650691506e-07, zero_point=67, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(66, 400, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=45, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=8.292685151900514e-07, zero_point=55\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), scale=3.574273250706028e-07, zero_point=63)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=1.5125696336326655e-05, zero_point=66\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), scale=8.747282663534861e-06, zero_point=65)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), scale=3.624243618105538e-06, zero_point=73, padding=(1, 1), groups=384)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=3.546007576460397e-07, zero_point=60, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=64, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=9.060358934220858e-07, zero_point=73\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), scale=3.7733263980044285e-07, zero_point=58)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), scale=2.1530435390104685e-07, zero_point=65)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=41, padding=(1, 1), groups=744)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(744, 124, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=4, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(124, 744, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=1, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=1.1920928955078125e-07, zero_point=10\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=5)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=3.780648114570795e-07, zero_point=58\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (7): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), scale=2.1146939843674772e-07, zero_point=65)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), scale=1.1920928955078125e-07, zero_point=44, padding=(1, 1), groups=712)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=4, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=2, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=1.1920928955078125e-07, zero_point=11\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=4)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=3)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=1, padding=(1, 1), groups=1360)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(1360, 226, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=0, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(226, 1360, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=0, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=1.1920928955078125e-07, zero_point=0\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=0)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=1.1920928955078125e-07, zero_point=4\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (9): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=3)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), scale=1.1920928955078125e-07, zero_point=1, padding=(1, 1), groups=1296)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=0, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=0, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=1.1920928955078125e-07, zero_point=0\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=0)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): QuantizedLinear(in_features=576, out_features=10, scale=0.0005666001234203577, zero_point=64, qscheme=torch.per_channel_affine)\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantize()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "model_fp32.to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "model_fp32_fused = copy.deepcopy(model_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "model_fp32.eval()\n",
    "model_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet model\n",
    "phinet_fuse_modules(model_fp32_fused)\n",
    "\n",
    "print(f\"model_fp32:\\n {model_fp32}\\n\")\n",
    "print(f\"model_fp32_fused:\\n {model_fp32_fused}\\n\")\n",
    "\n",
    "assert model_equivalence(model_1=model_fp32, model_2=model_fp32_fused, device=\"cpu\", rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,224,224)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "model_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  model_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "model_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(model_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {model_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,3, 224, 224)\n",
    "    model_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "model_int8 = torch.ao.quantization.convert(model_int8, inplace=True)\n",
    "\n",
    "print(f\"model_int8:\\n {model_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "result = model_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 size:\n",
      "16.936 MB\n",
      "INT8 size:\n",
      "4.577 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(model_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(model_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 14.846 ms\n",
      "FP32 GPU Inference Latency: 1.959 ms\n",
      "INT8 CPU Inference Latency: 5.947 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(model_fp32, device =\"cpu\", input_shape=(3,224,224))\n",
    "fp32_gpu_inference_latency  = measure_inference_latency(model_fp32, device =\"cuda\", input_shape=(3,224,224))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(model_int8, device =\"cpu\", input_shape=(3,224,224))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"FP32 GPU Inference Latency: {:.3f} ms\".format(fp32_gpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, after the modification, the model in INT8 is smaller in terms of memory. In contrast to the original model, the inference time on CPU is also reduced, with the reduction increasing as the input resolution increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orginal PhiNet VS Modified PhiNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of the original PhiNet model and the model modified to make quantization effective (channels divisible by 8 and `padding` within `Conv2d`). The following values are compared:\n",
    "- Number of parameters;\n",
    "- Number of operations;\n",
    "- Inference time;\n",
    "\n",
    "The comparison is made with both models in FP32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micromind import PhiNet\n",
    "\n",
    "# Orginal PhiNet\n",
    "model = PhiNet(input_shape = [3, 224, 224], num_layers = 7, alpha = 3, beta = 0.75, t_zero = 6, include_top = True, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phinet_quant import PhiNet\n",
    "\n",
    "# Modified PhiNet\n",
    "model_quant = PhiNet(input_shape = [3, 224, 224], num_layers = 7, alpha = 3, beta = 0.75, t_zero = 6, include_top = True, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      "PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): HSwish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): DepthwiseConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): HSwish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), groups=416, bias=False)\n",
      "        (6): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 401, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(401, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(401, 401, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=401, bias=False)\n",
      "        (5): BatchNorm2d(401, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(401, 66, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(66, 401, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(401, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 385, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(385, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(385, 385, kernel_size=(3, 3), stride=(2, 2), groups=385, bias=False)\n",
      "        (6): BatchNorm2d(385, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(385, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 385, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(385, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740, bias=False)\n",
      "        (5): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(740, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709, bias=False)\n",
      "        (6): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1357, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1357, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(1357, 1357, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1357, bias=False)\n",
      "        (5): BatchNorm2d(1357, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1357, 226, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(226, 1357, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(1357, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), groups=1296, bias=False)\n",
      "        (6): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(576, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Modified model:\n",
      "PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): DepthwiseConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416, bias=False)\n",
      "        (5): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400, bias=False)\n",
      "        (5): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 66, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(66, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "        (5): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744, bias=False)\n",
      "        (5): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(124, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712, bias=False)\n",
      "        (5): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)\n",
      "        (5): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 226, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(226, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296, bias=False)\n",
      "        (5): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(576, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Original model:\")\n",
    "print(model)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Modified model:\")\n",
    "print(model_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the orginal model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_model = summary(model, input_size=(1, 3, 224, 224), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the modified model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_model = summary(model_quant, input_size=(1, 3, 224, 224), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the original model: 4,394,545\n",
      "Number of MAC operations in the original model: 1,520,210,158\n",
      "Original model GPU Inference Latency: 3.121 ms\n",
      "Original model CPU Inference Latency: 18.905 ms\n",
      "\n",
      "Number of parameters in the modified model: 4,402,741\n",
      "Number of MAC operations in the modified model: 1,521,177,610\n",
      "Modified model GPU Inference Latency: 1.982 ms\n",
      "Modified model CPU Inference Latency: 14.491 ms\n"
     ]
    }
   ],
   "source": [
    "model_gpu_inference_latency = measure_inference_latency(model, device=\"cuda\", input_shape=(3, 224, 224))\n",
    "model_quant_gpu_inference_latency = measure_inference_latency(model_quant, device=\"cuda\", input_shape=(3, 224, 224))\n",
    "\n",
    "model_cpu_inference_latency = measure_inference_latency(model, device=\"cpu\", input_shape=(3, 224, 224))\n",
    "model_quant_cpu_inference_latency = measure_inference_latency(model_quant, device=\"cpu\", input_shape=(3, 224, 224))\n",
    "\n",
    "print(\"Number of parameters in the original model: {:,}\".format(org_model.total_params))\n",
    "print(\"Number of MAC operations in the original model: {:,}\".format(org_model.total_mult_adds))\n",
    "print(\"Original model GPU Inference Latency: {:.3f} ms\".format(model_gpu_inference_latency[0]))\n",
    "print(\"Original model CPU Inference Latency: {:.3f} ms\\n\".format(model_cpu_inference_latency[0]))\n",
    "\n",
    "print(\"Number of parameters in the modified model: {:,}\".format(mod_model.total_params))\n",
    "print(\"Number of MAC operations in the modified model: {:,}\".format(mod_model.total_mult_adds))\n",
    "print(\"Modified model GPU Inference Latency: {:.3f} ms\".format(model_quant_gpu_inference_latency[0]))\n",
    "print(\"Modified model CPU Inference Latency: {:.3f} ms\".format(model_quant_cpu_inference_latency[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smaller models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between the original PhiNet model and the modified model as before, now the models are smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micromind import PhiNet\n",
    "\n",
    "# Orginal PhiNet\n",
    "model_S = PhiNet(input_shape = [3, 224, 224], num_layers=7, alpha= 2.3, beta= 0.75,t_zero= 5, include_top= True, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phinet_quant import PhiNet\n",
    "\n",
    "# Modified PhiNet\n",
    "model_quant_S = PhiNet(input_shape = [3, 224, 224], num_layers=7, alpha= 2.3, beta= 0.75,t_zero= 5, include_top= True, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      "PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 110, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(110, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): HSwish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): DepthwiseConv2d(110, 110, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=110, bias=False)\n",
      "        (2): BatchNorm2d(110, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): HSwish()\n",
      "        (4): Conv2d(110, 55, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(55, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(55, 265, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(265, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(265, 265, kernel_size=(3, 3), stride=(2, 2), groups=265, bias=False)\n",
      "        (6): BatchNorm2d(265, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(265, 44, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(44, 265, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(265, 55, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(55, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(55, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(255, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=255, bias=False)\n",
      "        (5): BatchNorm2d(255, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(255, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(42, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(255, 55, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(55, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(55, 245, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(245, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(245, 245, kernel_size=(3, 3), stride=(2, 2), groups=245, bias=False)\n",
      "        (6): BatchNorm2d(245, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(245, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(40, 245, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(245, 110, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(110, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(110, 471, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(471, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(471, 471, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=471, bias=False)\n",
      "        (5): BatchNorm2d(471, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(471, 78, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(78, 471, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(471, 110, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(110, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(110, 451, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(451, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(451, 451, kernel_size=(3, 3), stride=(2, 2), groups=451, bias=False)\n",
      "        (6): BatchNorm2d(451, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(451, 75, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(75, 451, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(451, 220, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(220, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(220, 864, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(864, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(864, 864, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=864, bias=False)\n",
      "        (5): BatchNorm2d(864, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(864, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(144, 864, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(864, 220, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(220, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(220, 825, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(825, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(825, 825, kernel_size=(3, 3), stride=(2, 2), groups=825, bias=False)\n",
      "        (6): BatchNorm2d(825, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(825, 137, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(137, 825, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(825, 441, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(441, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=441, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Modified model:\n",
      "PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 110, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(110, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): DepthwiseConv2d(110, 110, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=110, bias=False)\n",
      "        (2): BatchNorm2d(110, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(110, 55, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(55, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(55, 264, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(264, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(264, 264, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=264, bias=False)\n",
      "        (5): BatchNorm2d(264, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(264, 44, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(44, 264, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(264, 55, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(55, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(55, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "        (5): BatchNorm2d(256, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(256, 55, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(55, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(55, 248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(248, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(248, 248, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=248, bias=False)\n",
      "        (5): BatchNorm2d(248, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(248, 41, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(41, 248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(248, 110, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(110, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(110, 472, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(472, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(472, 472, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=472, bias=False)\n",
      "        (5): BatchNorm2d(472, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(472, 78, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(78, 472, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(472, 110, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(110, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(110, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(448, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(448, 448, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=448, bias=False)\n",
      "        (5): BatchNorm2d(448, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(448, 74, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(74, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(448, 220, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(220, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(220, 864, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(864, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(864, 864, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=864, bias=False)\n",
      "        (5): BatchNorm2d(864, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(864, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(144, 864, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(864, 220, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(220, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(220, 824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(824, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(824, 824, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=824, bias=False)\n",
      "        (5): BatchNorm2d(824, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(824, 137, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(137, 824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(824, 441, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(441, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=441, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Original model:\")\n",
    "print(model_S)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Modified model:\")\n",
    "print(model_quant_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_model = summary(model_S, input_size=(1, 3, 224, 224), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_model = summary(model_quant_S, input_size=(1, 3, 224, 224), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the original model: 2,014,585\n",
      "Number of MAC operations in the original model: 763,280,698\n",
      "Original model GPU Inference Latency: 3.088 ms\n",
      "Original model CPU Inference Latency: 14.132 ms\n",
      "\n",
      "Number of parameters in the modified model: 2,012,917\n",
      "Number of MAC operations in the modified model: 763,280,848\n",
      "Modified model GPU Inference Latency: 1.972 ms\n",
      "Modified model CPU Inference Latency: 10.672 ms\n"
     ]
    }
   ],
   "source": [
    "model_gpu_inference_latency = measure_inference_latency(model_S, device=\"cuda\", input_shape=(3, 224, 224))\n",
    "model_quant_gpu_inference_latency = measure_inference_latency(model_quant_S, device=\"cuda\", input_shape=(3, 224, 224))\n",
    "\n",
    "model_cpu_inference_latency = measure_inference_latency(model_S, device=\"cpu\", input_shape=(3, 224, 224))\n",
    "model_quant_cpu_inference_latency = measure_inference_latency(model_quant_S, device=\"cpu\", input_shape=(3, 224, 224))\n",
    "\n",
    "print(\"Number of parameters in the original model: {:,}\".format(org_model.total_params))\n",
    "print(\"Number of MAC operations in the original model: {:,}\".format(org_model.total_mult_adds))\n",
    "print(\"Original model GPU Inference Latency: {:.3f} ms\".format(model_gpu_inference_latency[0]))\n",
    "print(\"Original model CPU Inference Latency: {:.3f} ms\\n\".format(model_cpu_inference_latency[0]))\n",
    "\n",
    "print(\"Number of parameters in the modified model: {:,}\".format(mod_model.total_params))\n",
    "print(\"Number of MAC operations in the modified model: {:,}\".format(mod_model.total_mult_adds))\n",
    "print(\"Modified model GPU Inference Latency: {:.3f} ms\".format(model_quant_gpu_inference_latency[0]))\n",
    "print(\"Modified model CPU Inference Latency: {:.3f} ms\".format(model_quant_cpu_inference_latency[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train(model, train_loader, device, learning_rate=1e-1, num_epochs=200, save_dir=\"saved_models\"):\n",
    "    # Create the save directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    tot_exp_time = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Initialize the timer\n",
    "        starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "        # Start Recording the time\n",
    "        starter.record()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            prob = nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = prob.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "\n",
    "        # Save the model state dictionary at the end of each epoch\n",
    "        model_save_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pt\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "        # Save some statics the be saved in output\n",
    "        tot_exp_time += curr_time\n",
    "        accuracy = 100.0 * correct / total\n",
    "        average_loss = train_loss / len(train_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} : Train accuracy {accuracy:.2f}%, Train loss {average_loss:.4f}, Training Time: {curr_time/1000:.3f} s\")\n",
    "\n",
    "    return model, tot_exp_time\n",
    "\n",
    "def test(model, test_loader, device, criterion= nn.CrossEntropyLoss()):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            prob = nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = prob.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    average_loss = test_loss / len(test_loader)\n",
    "\n",
    "    return accuracy, average_loss\n",
    "\n",
    "def calibrate_model(model, loader, device=torch.device(\"cpu\")):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        _ = model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256):\n",
    "\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = (0.1307,), std = (0.3081,))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_set = torchvision.datasets.MNIST(root='./data', train=True, transform=train_transform, download=True)\n",
    "    test_set = torchvision.datasets.MNIST(root='./data', train=False, transform=train_transform)\n",
    "\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set, batch_size=train_batch_size,\n",
    "        sampler=train_sampler, num_workers=num_workers)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_set, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the original PhiNet model on the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (1): Conv2d(1, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(24, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): HSwish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): DepthwiseConv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "        (2): BatchNorm2d(24, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): HSwish()\n",
      "        (4): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(12, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), groups=72, bias=False)\n",
      "        (6): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(12, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(12, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), groups=72, bias=False)\n",
      "        (6): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(24, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (5): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(24, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=24, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/10 : Train accuracy 91.79%, Train loss 0.2569, Training Time: 6.252 s\n",
      "Epoch 2/10 : Train accuracy 97.24%, Train loss 0.0896, Training Time: 5.996 s\n",
      "Epoch 3/10 : Train accuracy 97.92%, Train loss 0.0686, Training Time: 5.864 s\n",
      "Epoch 4/10 : Train accuracy 98.12%, Train loss 0.0596, Training Time: 5.833 s\n",
      "Epoch 5/10 : Train accuracy 98.39%, Train loss 0.0533, Training Time: 5.884 s\n",
      "Epoch 6/10 : Train accuracy 98.43%, Train loss 0.0505, Training Time: 5.926 s\n",
      "Epoch 7/10 : Train accuracy 98.59%, Train loss 0.0448, Training Time: 5.885 s\n",
      "Epoch 8/10 : Train accuracy 98.64%, Train loss 0.0420, Training Time: 5.923 s\n",
      "Epoch 9/10 : Train accuracy 98.72%, Train loss 0.0406, Training Time: 5.927 s\n",
      "Epoch 10/10 : Train accuracy 98.79%, Train loss 0.0386, Training Time: 6.001 s\n",
      "Training Time: 59.491 s\n",
      "Test Accuracy: 98.73%\n"
     ]
    }
   ],
   "source": [
    "from micromind import PhiNet\n",
    "\n",
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "model_mnist_fp32 = PhiNet(input_shape = [1, 28, 28], num_layers = 4, alpha = 0.5, beta = 1, t_zero = 6, include_top = True, num_classes = 10)\n",
    "print(model_mnist_fp32)\n",
    "\n",
    "train_loader, test_loader = prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_mnist_fp32, train_time_model_mnist_fp32 = train(model_mnist_fp32, train_loader, device, learning_rate=0.01, num_epochs=10, save_dir=\"orignal_model\")\n",
    "\n",
    "acc_model_mnist_fp32, _ = test(model_mnist_fp32, test_loader, device)\n",
    "\n",
    "print(f\"Training Time: {train_time_model_mnist_fp32/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_mnist_fp32:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the modified PhiNet model on the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (1): Conv2d(1, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(24, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "        (2): BatchNorm2d(24, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(12, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(12, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(12, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(24, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (5): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(24, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=24, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/10 : Train accuracy 92.39%, Train loss 0.2366, Training Time: 5.878 s\n",
      "Epoch 2/10 : Train accuracy 97.56%, Train loss 0.0802, Training Time: 5.938 s\n",
      "Epoch 3/10 : Train accuracy 98.08%, Train loss 0.0614, Training Time: 5.862 s\n",
      "Epoch 4/10 : Train accuracy 98.38%, Train loss 0.0533, Training Time: 5.845 s\n",
      "Epoch 5/10 : Train accuracy 98.46%, Train loss 0.0494, Training Time: 5.897 s\n",
      "Epoch 6/10 : Train accuracy 98.59%, Train loss 0.0448, Training Time: 5.864 s\n",
      "Epoch 7/10 : Train accuracy 98.69%, Train loss 0.0419, Training Time: 6.206 s\n",
      "Epoch 8/10 : Train accuracy 98.72%, Train loss 0.0402, Training Time: 5.908 s\n",
      "Epoch 9/10 : Train accuracy 98.71%, Train loss 0.0395, Training Time: 5.896 s\n",
      "Epoch 10/10 : Train accuracy 98.85%, Train loss 0.0351, Training Time: 5.880 s\n",
      "Training Time: 59.174 s\n",
      "Test Accuracy: 98.91%\n"
     ]
    }
   ],
   "source": [
    "from phinet_quant import PhiNet\n",
    "\n",
    "# set the seed for reproducibility\n",
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "model_quant_mnist_fp32 = PhiNet(input_shape = [1, 28, 28], num_layers = 4, alpha = 0.5, beta = 1, t_zero = 6, include_top = True, num_classes = 10)\n",
    "remove_depthwise(model_quant_mnist_fp32)\n",
    "print(model_quant_mnist_fp32)\n",
    "\n",
    "train_loader, test_loader = prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_quant_mnist_fp32, train_time_model_quant_mnist_fp32 = train(model_quant_mnist_fp32, train_loader, device, learning_rate=0.01, num_epochs=10, save_dir=\"modified_model\")\n",
    "\n",
    "acc_model_quant_mnist_fp32, _ = test(model_quant_mnist_fp32, test_loader, device)\n",
    "\n",
    "print(f\"Training Time: {train_time_model_quant_mnist_fp32/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_quant_mnist_fp32:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static Quantization of the modified PhiNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiNetQuant(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(PhiNetQuant, self).__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized.\n",
    "        # This will only be used for inputs.\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        # FP32 model\n",
    "        self.model_fp32 = model_fp32\n",
    "        # DeQuantStub converts tensors from quantized to floating point.\n",
    "        # This will only be used for outputs.\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fp32:\n",
      " PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (1): Conv2d(1, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(24, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "        (2): BatchNorm2d(24, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(12, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(12, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(12, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(24, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (5): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(24, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=24, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "model_fp32_fused:\n",
      " PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (1): Conv2d(1, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Identity()\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)\n",
      "        (2): Identity()\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (5): Identity()\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=24, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " PhiNetQuant(\n",
      "  (quant): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (model_fp32): PhiNet(\n",
      "    (_layers): ModuleList(\n",
      "      (0): ZeroPad2d((0, 1, 0, 1))\n",
      "      (1): SeparableConv2d(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            1, 1, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Conv2d(\n",
      "            1, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (2): Identity()\n",
      "          (3): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Dropout2d(p=0.05, inplace=False)\n",
      "          (1): Conv2d(\n",
      "            24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (2): Identity()\n",
      "          (3): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (4): Conv2d(\n",
      "            24, 12, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            12, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            72, 12, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            12, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            72, 12, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (5): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            12, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            72, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            144, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): Linear(\n",
      "        in_features=24, out_features=10, bias=True\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "\n",
      "model_int8:\n",
      " PhiNetQuant(\n",
      "  (quant): Quantize(scale=tensor([0.0255]), zero_point=tensor([17]), dtype=torch.quint8)\n",
      "  (model_fp32): PhiNet(\n",
      "    (_layers): ModuleList(\n",
      "      (0): ZeroPad2d((0, 1, 0, 1))\n",
      "      (1): SeparableConv2d(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), scale=0.10640737414360046, zero_point=17, bias=False)\n",
      "          (1): QuantizedConv2d(1, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.0816974863409996, zero_point=55)\n",
      "          (2): Identity()\n",
      "          (3): QuantizedHardswish()\n",
      "        )\n",
      "      )\n",
      "      (2): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Dropout2d(p=0.05, inplace=False)\n",
      "          (1): QuantizedConv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), scale=0.10361752659082413, zero_point=62, padding=(1, 1), groups=24)\n",
      "          (2): Identity()\n",
      "          (3): QuantizedHardswish()\n",
      "          (4): QuantizedConv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), scale=0.12565742433071136, zero_point=63)\n",
      "          (5): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.15467479825019836, zero_point=62)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), scale=0.13587383925914764, zero_point=75, padding=(1, 1), groups=72)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), scale=0.05042992904782295, zero_point=73, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.09193974733352661, zero_point=58, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.03207129240036011, zero_point=12\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), scale=0.14255991578102112, zero_point=71)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.2052224576473236, zero_point=61)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), scale=0.1255853921175003, zero_point=72, padding=(1, 1), groups=72)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), scale=0.060770921409130096, zero_point=63, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.08282832056283951, zero_point=65, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.04453304037451744, zero_point=8\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), scale=0.1023295670747757, zero_point=71)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=0.17347609996795654, zero_point=72\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.15436193346977234, zero_point=58)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), scale=0.14581641554832458, zero_point=65, padding=(1, 1), groups=72)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), scale=0.0705588161945343, zero_point=85, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.0778329074382782, zero_point=68, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.03680865094065666, zero_point=10\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.15515729784965515, zero_point=69)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.18001724779605865, zero_point=61)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.1358068883419037, zero_point=65, padding=(1, 1), groups=144)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.1442970484495163, zero_point=80, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.2777025103569031, zero_point=49, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.05597430467605591, zero_point=7\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.15258049964904785, zero_point=65)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=0.1935957968235016, zero_point=64\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): QuantizedLinear(in_features=24, out_features=10, scale=0.2761937081813812, zero_point=46, qscheme=torch.per_channel_affine)\n",
      "    )\n",
      "  )\n",
      "  (dequant): DeQuantize()\n",
      ")\n",
      "\n",
      "Test Accuracy: 98.87%\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "# Copy the pretrained model for static quantization\n",
    "model_st_quant_mnist_fp32 = copy.deepcopy(model_quant_mnist_fp32)\n",
    "model_st_quant_mnist_fp32.to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "model_st_quant_mnist_fp32_fused = copy.deepcopy(model_st_quant_mnist_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "model_st_quant_mnist_fp32.eval()\n",
    "model_st_quant_mnist_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet model\n",
    "phinet_fuse_modules(model_st_quant_mnist_fp32_fused)\n",
    "\n",
    "print(f\"model_fp32:\\n {model_st_quant_mnist_fp32}\\n\")\n",
    "print(f\"model_fp32_fused:\\n {model_st_quant_mnist_fp32_fused}\\n\")\n",
    "\n",
    "assert model_equivalence(model_1=model_st_quant_mnist_fp32, model_2=model_st_quant_mnist_fp32_fused, device=\"cpu\", rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,1,28,28)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "model_st_quant_mnist_int8 = PhiNetQuant(model_st_quant_mnist_fp32_fused)\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "model_st_quant_mnist_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(model_st_quant_mnist_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {model_st_quant_mnist_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "calibrate_model(model_st_quant_mnist_int8, train_loader, device=\"cpu\")\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "model_st_quant_mnist_int8 = torch.ao.quantization.convert(model_st_quant_mnist_int8, inplace=True)\n",
    "\n",
    "print(f\"model_int8:\\n {model_st_quant_mnist_int8}\\n\")\n",
    "\n",
    "acc_model_st_quant_mnist_int8, _ = test(model_st_quant_mnist_int8, test_loader, \"cpu\")\n",
    "\n",
    "print(f\"Test Accuracy: {acc_model_st_quant_mnist_int8:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization awere training of the modified model on the MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fp32:\n",
      " PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (1): Conv2d(1, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(24, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "        (2): BatchNorm2d(24, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(12, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(12, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(12, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(24, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (5): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(24, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=24, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "model_fp32_fused:\n",
      " PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (1): Conv2d(1, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Identity()\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)\n",
      "        (2): Identity()\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (5): Identity()\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=24, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "After preparation, note fake-quantization modules:\n",
      " PhiNetQuant(\n",
      "  (quant): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (model_fp32): PhiNet(\n",
      "    (_layers): ModuleList(\n",
      "      (0): ZeroPad2d((0, 1, 0, 1))\n",
      "      (1): SeparableConv2d(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            1, 1, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Conv2d(\n",
      "            1, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (2): Identity()\n",
      "          (3): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Dropout2d(p=0.05, inplace=False)\n",
      "          (1): Conv2d(\n",
      "            24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (2): Identity()\n",
      "          (3): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (4): Conv2d(\n",
      "            24, 12, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            12, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            72, 12, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            12, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            72, 12, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (5): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            12, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            72, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            144, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): Linear(\n",
      "        in_features=24, out_features=10, bias=True\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "\n",
      "Epoch 1/10 : Train accuracy 97.45%, Train loss 0.0852, Training Time: 16.199 s\n",
      "Epoch 2/10 : Train accuracy 97.81%, Train loss 0.0735, Training Time: 16.031 s\n",
      "Epoch 3/10 : Train accuracy 97.93%, Train loss 0.0668, Training Time: 16.146 s\n",
      "Epoch 4/10 : Train accuracy 97.95%, Train loss 0.0654, Training Time: 16.203 s\n",
      "Epoch 5/10 : Train accuracy 98.06%, Train loss 0.0624, Training Time: 14.828 s\n",
      "Epoch 6/10 : Train accuracy 98.25%, Train loss 0.0571, Training Time: 15.925 s\n",
      "Epoch 7/10 : Train accuracy 98.28%, Train loss 0.0578, Training Time: 16.245 s\n",
      "Epoch 8/10 : Train accuracy 98.09%, Train loss 0.0609, Training Time: 16.234 s\n",
      "Epoch 9/10 : Train accuracy 98.19%, Train loss 0.0590, Training Time: 16.238 s\n",
      "Epoch 10/10 : Train accuracy 98.31%, Train loss 0.0545, Training Time: 15.094 s\n",
      "model_int8:\n",
      " PhiNetQuant(\n",
      "  (quant): Quantize(scale=tensor([0.0255]), zero_point=tensor([17]), dtype=torch.quint8)\n",
      "  (model_fp32): PhiNet(\n",
      "    (_layers): ModuleList(\n",
      "      (0): ZeroPad2d((0, 1, 0, 1))\n",
      "      (1): SeparableConv2d(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), scale=0.05301687866449356, zero_point=18, bias=False)\n",
      "          (1): QuantizedConv2d(1, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.07652683556079865, zero_point=64)\n",
      "          (2): Identity()\n",
      "          (3): QuantizedHardswish()\n",
      "        )\n",
      "      )\n",
      "      (2): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Dropout2d(p=0.05, inplace=False)\n",
      "          (1): QuantizedConv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), scale=0.08399170637130737, zero_point=73, padding=(1, 1), groups=24)\n",
      "          (2): Identity()\n",
      "          (3): QuantizedHardswish()\n",
      "          (4): QuantizedConv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), scale=0.08308567106723785, zero_point=64)\n",
      "          (5): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.12301048636436462, zero_point=63)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), scale=0.22859670221805573, zero_point=86, padding=(1, 1), groups=72)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), scale=0.07785918563604355, zero_point=76, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.07909128069877625, zero_point=71, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.03235623612999916, zero_point=12\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), scale=0.08629003167152405, zero_point=66)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.14669929444789886, zero_point=60)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), scale=0.20895390212535858, zero_point=88, padding=(1, 1), groups=72)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), scale=0.054609283804893494, zero_point=67, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.06690813601016998, zero_point=90, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.025650668889284134, zero_point=15\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), scale=0.0768449604511261, zero_point=68)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=0.11577553302049637, zero_point=67\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.11847018450498581, zero_point=63)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), scale=0.18663284182548523, zero_point=74, padding=(1, 1), groups=72)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), scale=0.06765639036893845, zero_point=82, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.057027313858270645, zero_point=77, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.032774489372968674, zero_point=11\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.11843585222959518, zero_point=64)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.25904515385627747, zero_point=62)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.33321166038513184, zero_point=85, padding=(1, 1), groups=144)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.19475853443145752, zero_point=85, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.1843334138393402, zero_point=63, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.060210276395082474, zero_point=6\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.36343082785606384, zero_point=62)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=0.42045119404792786, zero_point=59\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): QuantizedLinear(in_features=24, out_features=10, scale=0.3136826455593109, zero_point=48, qscheme=torch.per_channel_affine)\n",
      "    )\n",
      "  )\n",
      "  (dequant): DeQuantize()\n",
      ")\n",
      "\n",
      "Training Time: 159.143 s\n",
      "Test Accuracy: 98.25%\n"
     ]
    }
   ],
   "source": [
    "from phinet_quant import PhiNet\n",
    "\n",
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "model_qat_quant_mnist_fp32 = PhiNet(input_shape = [1, 28, 28], num_layers = 4, alpha = 0.5, beta = 1, t_zero = 6, include_top = True, num_classes = 10)\n",
    "remove_depthwise(model_qat_quant_mnist_fp32)\n",
    "\n",
    "load_epoch = 2\n",
    "\n",
    "if load_epoch is not None:\n",
    "    saved_model_path = f\"modified_model/model_epoch_{load_epoch}.pt\" \n",
    "    checkpoint = torch.load(saved_model_path)\n",
    "    model_qat_quant_mnist_fp32.load_state_dict(checkpoint)\n",
    "\n",
    "model_qat_quant_mnist_fp32.to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "model_qat_quant_mnist_fp32_fused = copy.deepcopy(model_qat_quant_mnist_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "model_qat_quant_mnist_fp32.eval()\n",
    "model_qat_quant_mnist_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet model\n",
    "phinet_fuse_modules(model_qat_quant_mnist_fp32_fused)\n",
    "\n",
    "print(f\"model_fp32:\\n {model_qat_quant_mnist_fp32}\\n\")\n",
    "print(f\"model_fp32_fused:\\n {model_qat_quant_mnist_fp32_fused}\\n\")\n",
    "\n",
    "# Insert stubs ion the model and in the forward pass\n",
    "model_qat_quant_mnist_int8 = PhiNetQuant(model_qat_quant_mnist_fp32_fused)\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "model_qat_quant_mnist_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare_qat(model_qat_quant_mnist_int8.train(), inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {model_qat_quant_mnist_int8}\\n\")\n",
    "\n",
    "# Quantization awere training and calibrate the model\n",
    "model_qat_quant_mnist_int8, train_time_model_qat_quant_mnist_int8 = train(model_qat_quant_mnist_int8, train_loader, device = \"cuda\", learning_rate=0.01, num_epochs=10, save_dir=\"qat_model\")\n",
    "\n",
    "model_qat_quant_mnist_int8.to(\"cpu\")\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "model_qat_quant_mnist_int8 = torch.ao.quantization.convert(model_qat_quant_mnist_int8, inplace=True)\n",
    "\n",
    "print(f\"model_int8:\\n {model_qat_quant_mnist_int8}\\n\")\n",
    "\n",
    "acc_model_qat_quant_mnist_int8, _ = test(model_qat_quant_mnist_int8, test_loader, \"cpu\")\n",
    "\n",
    "print(f\"Training Time: {train_time_model_qat_quant_mnist_int8/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_qat_quant_mnist_int8:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comper the results:\n",
    "- Orginal model FP32;\n",
    "- Modified model FP32;\n",
    "- Static Quantization of the modified model INT8;\n",
    "- Quantization aware training of the modified model INT8;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal FP32 model CPU Inference Latency: 2.542 ms\n",
      "Orginal FP32 model test accuracy: 98.73%\n",
      "\n",
      "Modified FP32 model CPU Inference Latency: 1.868 ms\n",
      "Modified FP32 model test accuracy: 98.91%\n",
      "\n",
      "Modified INT8 model static quant CPU Inference Latency: 1.545 ms\n",
      "Modified INT8 model static quant test accuracy: 98.87%\n",
      "\n",
      "Modified INT8 model QAT CPU Inference Latency: 1.303 ms\n",
      "Modified INT8 model QAT test accuracy: 98.25%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_fp32_cpu_inference_latency  = measure_inference_latency(model_mnist_fp32, device=\"cpu\", input_shape=(1, 28, 28))\n",
    "model_quant_fp32_cpu_inference_latency  = measure_inference_latency(model_quant_mnist_fp32, device=\"cpu\", input_shape=(1, 28, 28))\n",
    "model_st_quant_int8_cpu_inference_latency  = measure_inference_latency(model_st_quant_mnist_int8, device=\"cpu\", input_shape=(1, 28, 28))\n",
    "model_qat_quant_int8_cpu_inference_latency  = measure_inference_latency(model_qat_quant_mnist_int8, device=\"cpu\", input_shape=(1, 28, 28))\n",
    "\n",
    "print(\"Orginal FP32 model CPU Inference Latency: {:.3f} ms\".format(model_fp32_cpu_inference_latency[0]))\n",
    "print(\"Orginal FP32 model test accuracy: {:.2f}%\\n\".format(acc_model_mnist_fp32))\n",
    "\n",
    "print(\"Modified FP32 model CPU Inference Latency: {:.3f} ms\".format(model_quant_fp32_cpu_inference_latency[0]))\n",
    "print(\"Modified FP32 model test accuracy: {:.2f}%\\n\".format(acc_model_quant_mnist_fp32))\n",
    "\n",
    "print(\"Modified INT8 model static quant CPU Inference Latency: {:.3f} ms\".format(model_st_quant_int8_cpu_inference_latency[0]))\n",
    "print(\"Modified INT8 model static quant test accuracy: {:.2f}%\\n\".format(acc_model_st_quant_mnist_int8))\n",
    "\n",
    "print(\"Modified INT8 model QAT CPU Inference Latency: {:.3f} ms\".format(model_qat_quant_int8_cpu_inference_latency[0]))\n",
    "print(\"Modified INT8 model QAT test accuracy: {:.2f}%\\n\".format(acc_model_qat_quant_mnist_int8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the CIFAR10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256):\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    train_set = torchvision.datasets.CIFAR10(root='./data', train=True, transform=train_transform, download=True)\n",
    "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, transform=train_transform)\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set, batch_size=train_batch_size,\n",
    "        sampler=train_sampler, num_workers=num_workers)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_set, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the original PhiNet model on the CIFAR10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): HSwish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): DepthwiseConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): HSwish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), groups=416, bias=False)\n",
      "        (6): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 401, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(401, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(401, 401, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=401, bias=False)\n",
      "        (5): BatchNorm2d(401, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(401, 66, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(66, 401, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(401, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 385, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(385, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(385, 385, kernel_size=(3, 3), stride=(2, 2), groups=385, bias=False)\n",
      "        (6): BatchNorm2d(385, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(385, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 385, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(385, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740, bias=False)\n",
      "        (5): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(740, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709, bias=False)\n",
      "        (6): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1357, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1357, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(1357, 1357, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1357, bias=False)\n",
      "        (5): BatchNorm2d(1357, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1357, 226, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(226, 1357, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(1357, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), groups=1296, bias=False)\n",
      "        (6): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(576, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [01:14<00:00, 2293901.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Epoch 1/10 : Train accuracy 35.68%, Train loss 1.8888, Training Time: 8.705 s\n",
      "Epoch 2/10 : Train accuracy 49.99%, Train loss 1.3782, Training Time: 8.549 s\n",
      "Epoch 3/10 : Train accuracy 58.83%, Train loss 1.1505, Training Time: 8.563 s\n",
      "Epoch 4/10 : Train accuracy 65.01%, Train loss 0.9916, Training Time: 8.589 s\n",
      "Epoch 5/10 : Train accuracy 68.85%, Train loss 0.8827, Training Time: 8.596 s\n",
      "Epoch 6/10 : Train accuracy 72.01%, Train loss 0.7965, Training Time: 8.615 s\n",
      "Epoch 7/10 : Train accuracy 74.31%, Train loss 0.7361, Training Time: 8.641 s\n",
      "Epoch 8/10 : Train accuracy 76.41%, Train loss 0.6756, Training Time: 8.589 s\n",
      "Epoch 9/10 : Train accuracy 78.13%, Train loss 0.6278, Training Time: 8.652 s\n",
      "Epoch 10/10 : Train accuracy 79.40%, Train loss 0.5899, Training Time: 8.676 s\n",
      "Training Time: 86.174 s\n",
      "Test Accuracy: 75.72%\n"
     ]
    }
   ],
   "source": [
    "from micromind import PhiNet\n",
    "\n",
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "model_cifar_fp32 = PhiNet(input_shape = [3, 32, 32], num_layers = 7, alpha = 3, beta = 0.75, t_zero = 6, include_top = True, num_classes = 10)\n",
    "print(model_cifar_fp32)\n",
    "\n",
    "train_loader, test_loader = prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_cifar_fp32, train_time_model_cifar_fp32 = train(model_cifar_fp32, train_loader, device, learning_rate=0.01, num_epochs=10, save_dir=\"orignal_model\")\n",
    "\n",
    "acc_model_cifar_fp32, _ = test(model_cifar_fp32, test_loader, device)\n",
    "\n",
    "print(f\"Training Time: {train_time_model_cifar_fp32/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_cifar_fp32:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the modified PhiNet model on the CIFAR10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416, bias=False)\n",
      "        (5): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400, bias=False)\n",
      "        (5): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 66, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(66, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "        (5): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744, bias=False)\n",
      "        (5): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(124, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712, bias=False)\n",
      "        (5): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)\n",
      "        (5): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 226, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(226, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296, bias=False)\n",
      "        (5): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(576, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Epoch 1/10 : Train accuracy 36.00%, Train loss 1.8926, Training Time: 6.206 s\n",
      "Epoch 2/10 : Train accuracy 53.08%, Train loss 1.3088, Training Time: 6.124 s\n",
      "Epoch 3/10 : Train accuracy 61.16%, Train loss 1.0816, Training Time: 6.138 s\n",
      "Epoch 4/10 : Train accuracy 66.58%, Train loss 0.9340, Training Time: 6.149 s\n",
      "Epoch 5/10 : Train accuracy 71.39%, Train loss 0.8165, Training Time: 6.154 s\n",
      "Epoch 6/10 : Train accuracy 73.82%, Train loss 0.7438, Training Time: 6.160 s\n",
      "Epoch 7/10 : Train accuracy 75.93%, Train loss 0.6900, Training Time: 6.162 s\n",
      "Epoch 8/10 : Train accuracy 77.63%, Train loss 0.6371, Training Time: 6.158 s\n",
      "Epoch 9/10 : Train accuracy 79.19%, Train loss 0.5940, Training Time: 6.151 s\n",
      "Epoch 10/10 : Train accuracy 80.59%, Train loss 0.5529, Training Time: 6.148 s\n",
      "Training Time: 61.548 s\n",
      "Test Accuracy: 77.33%\n"
     ]
    }
   ],
   "source": [
    "from phinet_quant import PhiNet\n",
    "\n",
    "# set the seed for reproducibility\n",
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "model_quant_cifar_fp32 = PhiNet(input_shape = [3, 32, 32], num_layers = 7, alpha = 3, beta = 0.75, t_zero = 6, include_top = True, num_classes = 10)\n",
    "remove_depthwise(model_quant_cifar_fp32)\n",
    "print(model_quant_cifar_fp32)\n",
    "\n",
    "train_loader, test_loader = prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_quant_cifar_fp32, train_time_model_quant_cifar_fp32 = train(model_quant_cifar_fp32, train_loader, device, learning_rate=0.01, num_epochs=10, save_dir=\"modified_model\")\n",
    "\n",
    "acc_model_quant_cifar_fp32, _ = test(model_quant_cifar_fp32, test_loader, device)\n",
    "\n",
    "print(f\"Training Time: {train_time_model_quant_cifar_fp32/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_quant_cifar_fp32:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static Quantization of the modified PhiNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiNetQuant(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(PhiNetQuant, self).__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized.\n",
    "        # This will only be used for inputs.\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        # FP32 model\n",
    "        self.model_fp32 = model_fp32\n",
    "        # DeQuantStub converts tensors from quantized to floating point.\n",
    "        # This will only be used for outputs.\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fp32:\n",
      " PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416, bias=False)\n",
      "        (5): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400, bias=False)\n",
      "        (5): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 66, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(66, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "        (5): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744, bias=False)\n",
      "        (5): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(124, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712, bias=False)\n",
      "        (5): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)\n",
      "        (5): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 226, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(226, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296, bias=False)\n",
      "        (5): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(576, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "model_fp32_fused:\n",
      " PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Identity()\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)\n",
      "        (2): Identity()\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (5): Identity()\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 66, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(66, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(124, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 226, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(226, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " PhiNetQuant(\n",
      "  (quant): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (model_fp32): PhiNet(\n",
      "    (_layers): ModuleList(\n",
      "      (0): ZeroPad2d((0, 1, 0, 1))\n",
      "      (1): SeparableConv2d(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Conv2d(\n",
      "            3, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (2): Identity()\n",
      "          (3): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Dropout2d(p=0.05, inplace=False)\n",
      "          (1): Conv2d(\n",
      "            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (2): Identity()\n",
      "          (3): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (4): Conv2d(\n",
      "            144, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            72, 416, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            416, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            72, 400, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              400, 66, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              66, 400, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            400, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (5): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            72, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            384, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            144, 744, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              744, 124, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              124, 744, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            744, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (7): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            144, 712, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            712, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            288, 1360, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              1360, 226, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              226, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            1360, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (9): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            288, 1296, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            1296, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): Linear(\n",
      "        in_features=576, out_features=10, bias=True\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "\n",
      "model_int8:\n",
      " PhiNetQuant(\n",
      "  (quant): Quantize(scale=tensor([0.0374]), zero_point=tensor([57]), dtype=torch.quint8)\n",
      "  (model_fp32): PhiNet(\n",
      "    (_layers): ModuleList(\n",
      "      (0): ZeroPad2d((0, 1, 0, 1))\n",
      "      (1): SeparableConv2d(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), scale=0.082307368516922, zero_point=61, groups=3, bias=False)\n",
      "          (1): QuantizedConv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.14681167900562286, zero_point=56)\n",
      "          (2): Identity()\n",
      "          (3): QuantizedHardswish()\n",
      "        )\n",
      "      )\n",
      "      (2): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Dropout2d(p=0.05, inplace=False)\n",
      "          (1): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.27193158864974976, zero_point=66, padding=(1, 1), groups=144)\n",
      "          (2): Identity()\n",
      "          (3): QuantizedHardswish()\n",
      "          (4): QuantizedConv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.1436218023300171, zero_point=67)\n",
      "          (5): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), scale=0.17918869853019714, zero_point=63)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), scale=0.22205601632595062, zero_point=71, padding=(1, 1), groups=416)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), scale=0.538780152797699, zero_point=59, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), scale=2.45733642578125, zero_point=66, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.09895668178796768, zero_point=4\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.15655682981014252, zero_point=64)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), scale=0.15593869984149933, zero_point=62)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), scale=0.17136019468307495, zero_point=69, padding=(1, 1), groups=400)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(400, 66, kernel_size=(1, 1), stride=(1, 1), scale=0.578287661075592, zero_point=53, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(66, 400, kernel_size=(1, 1), stride=(1, 1), scale=4.191625595092773, zero_point=56, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.08189166337251663, zero_point=5\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.09044328331947327, zero_point=63)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=0.18204841017723083, zero_point=63\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.13603025674819946, zero_point=65)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), scale=0.2101128101348877, zero_point=61, padding=(1, 1), groups=384)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=1.6347569227218628, zero_point=54, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=9.137063980102539, zero_point=58, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.0991574302315712, zero_point=4\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.16738823056221008, zero_point=69)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), scale=0.1374010145664215, zero_point=61)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), scale=0.1714201271533966, zero_point=66, padding=(1, 1), groups=744)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(744, 124, kernel_size=(1, 1), stride=(1, 1), scale=1.539083480834961, zero_point=52, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(124, 744, kernel_size=(1, 1), stride=(1, 1), scale=9.717344284057617, zero_point=56, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.08874890208244324, zero_point=4\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.10811252892017365, zero_point=66)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=0.17399689555168152, zero_point=64\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (7): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), scale=0.13140715658664703, zero_point=64)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), scale=0.22104233503341675, zero_point=64, padding=(1, 1), groups=712)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), scale=0.9028798341751099, zero_point=55, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), scale=5.160098075866699, zero_point=62, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.0901762992143631, zero_point=4\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.17941269278526306, zero_point=63)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), scale=0.14775824546813965, zero_point=62)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), scale=0.2563837468624115, zero_point=51, padding=(1, 1), groups=1360)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(1360, 226, kernel_size=(1, 1), stride=(1, 1), scale=3.7762928009033203, zero_point=62, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(226, 1360, kernel_size=(1, 1), stride=(1, 1), scale=80.34761810302734, zero_point=56, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.13954411447048187, zero_point=3\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.1036730706691742, zero_point=66)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=0.19016993045806885, zero_point=62\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (9): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), scale=0.1772586852312088, zero_point=57)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), scale=0.2484627217054367, zero_point=70, padding=(1, 1), groups=1296)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), scale=4.598127365112305, zero_point=67, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), scale=57.7437858581543, zero_point=69, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.20299585163593292, zero_point=2\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.033592600375413895, zero_point=62)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): QuantizedLinear(in_features=576, out_features=10, scale=0.221867635846138, zero_point=39, qscheme=torch.per_channel_affine)\n",
      "    )\n",
      "  )\n",
      "  (dequant): DeQuantize()\n",
      ")\n",
      "\n",
      "Test Accuracy: 76.04%\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "# Copy the pretrained model for static quantization\n",
    "model_st_quant_cifar_fp32 = copy.deepcopy(model_quant_cifar_fp32)\n",
    "model_st_quant_cifar_fp32.to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "model_st_quant_cifar_fp32_fused = copy.deepcopy(model_st_quant_cifar_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "model_st_quant_cifar_fp32.eval()\n",
    "model_st_quant_cifar_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet model\n",
    "phinet_fuse_modules(model_st_quant_cifar_fp32_fused)\n",
    "\n",
    "print(f\"model_fp32:\\n {model_st_quant_cifar_fp32}\\n\")\n",
    "print(f\"model_fp32_fused:\\n {model_st_quant_cifar_fp32_fused}\\n\")\n",
    "\n",
    "assert model_equivalence(model_1=model_st_quant_cifar_fp32, model_2=model_st_quant_cifar_fp32_fused, device=\"cpu\", rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "model_st_quant_cifar_int8 = PhiNetQuant(model_st_quant_cifar_fp32_fused)\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "model_st_quant_cifar_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(model_st_quant_cifar_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {model_st_quant_cifar_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "calibrate_model(model_st_quant_cifar_int8, train_loader, device=\"cpu\")\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "model_st_quant_cifar_int8 = torch.ao.quantization.convert(model_st_quant_cifar_int8, inplace=True)\n",
    "\n",
    "print(f\"model_int8:\\n {model_st_quant_cifar_int8}\\n\")\n",
    "\n",
    "acc_model_st_quant_cifar_int8, _ = test(model_st_quant_cifar_int8, test_loader, \"cpu\")\n",
    "\n",
    "print(f\"Test Accuracy: {acc_model_st_quant_cifar_int8:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization awere training of the modified model on the CIFAR10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fp32:\n",
      " PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416, bias=False)\n",
      "        (5): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400, bias=False)\n",
      "        (5): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 66, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(66, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "        (5): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744, bias=False)\n",
      "        (5): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(124, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712, bias=False)\n",
      "        (5): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)\n",
      "        (5): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 226, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(226, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296, bias=False)\n",
      "        (5): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(576, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "model_fp32_fused:\n",
      " PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Identity()\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)\n",
      "        (2): Identity()\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (5): Identity()\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 66, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(66, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(124, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 226, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(226, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "After preparation, note fake-quantization modules:\n",
      " PhiNetQuant(\n",
      "  (quant): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (model_fp32): PhiNet(\n",
      "    (_layers): ModuleList(\n",
      "      (0): ZeroPad2d((0, 1, 0, 1))\n",
      "      (1): SeparableConv2d(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Conv2d(\n",
      "            3, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (2): Identity()\n",
      "          (3): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Dropout2d(p=0.05, inplace=False)\n",
      "          (1): Conv2d(\n",
      "            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (2): Identity()\n",
      "          (3): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (4): Conv2d(\n",
      "            144, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            72, 416, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            416, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            72, 400, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              400, 66, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              66, 400, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            400, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (5): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            72, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            384, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            144, 744, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              744, 124, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              124, 744, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            744, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (7): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            144, 712, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            712, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            288, 1360, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              1360, 226, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              226, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            1360, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (9): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            288, 1296, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            1296, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): Linear(\n",
      "        in_features=576, out_features=10, bias=True\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "\n",
      "Epoch 1/10 : Train accuracy 61.15%, Train loss 1.0958, Training Time: 36.357 s\n",
      "Epoch 2/10 : Train accuracy 66.46%, Train loss 0.9555, Training Time: 36.214 s\n",
      "Epoch 3/10 : Train accuracy 69.63%, Train loss 0.8659, Training Time: 36.222 s\n",
      "Epoch 4/10 : Train accuracy 72.24%, Train loss 0.7898, Training Time: 36.629 s\n",
      "Epoch 5/10 : Train accuracy 74.59%, Train loss 0.7304, Training Time: 36.636 s\n",
      "Epoch 6/10 : Train accuracy 76.10%, Train loss 0.6818, Training Time: 36.758 s\n",
      "Epoch 7/10 : Train accuracy 78.10%, Train loss 0.6208, Training Time: 37.055 s\n",
      "Epoch 8/10 : Train accuracy 79.45%, Train loss 0.5864, Training Time: 37.045 s\n",
      "Epoch 9/10 : Train accuracy 81.00%, Train loss 0.5425, Training Time: 37.327 s\n",
      "Epoch 10/10 : Train accuracy 81.85%, Train loss 0.5166, Training Time: 36.878 s\n",
      "model_int8:\n",
      " PhiNetQuant(\n",
      "  (quant): Quantize(scale=tensor([0.0374]), zero_point=tensor([57]), dtype=torch.quint8)\n",
      "  (model_fp32): PhiNet(\n",
      "    (_layers): ModuleList(\n",
      "      (0): ZeroPad2d((0, 1, 0, 1))\n",
      "      (1): SeparableConv2d(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), scale=0.05701439827680588, zero_point=61, groups=3, bias=False)\n",
      "          (1): QuantizedConv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.10704753547906876, zero_point=60)\n",
      "          (2): Identity()\n",
      "          (3): QuantizedHardswish()\n",
      "        )\n",
      "      )\n",
      "      (2): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Dropout2d(p=0.05, inplace=False)\n",
      "          (1): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.24992352724075317, zero_point=70, padding=(1, 1), groups=144)\n",
      "          (2): Identity()\n",
      "          (3): QuantizedHardswish()\n",
      "          (4): QuantizedConv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.12505824863910675, zero_point=61)\n",
      "          (5): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), scale=0.2116697132587433, zero_point=63)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), scale=0.30800142884254456, zero_point=71, padding=(1, 1), groups=416)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), scale=0.3686175048351288, zero_point=53, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), scale=0.8723419308662415, zero_point=59, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.10066268593072891, zero_point=4\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.13794061541557312, zero_point=63)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), scale=0.2013607919216156, zero_point=62)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), scale=0.2779911756515503, zero_point=72, padding=(1, 1), groups=400)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(400, 66, kernel_size=(1, 1), stride=(1, 1), scale=0.4943675100803375, zero_point=56, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(66, 400, kernel_size=(1, 1), stride=(1, 1), scale=1.687003493309021, zero_point=56, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.07803258299827576, zero_point=5\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.0660691186785698, zero_point=64)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=0.13720935583114624, zero_point=64\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.13714462518692017, zero_point=64)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), scale=0.20913684368133545, zero_point=75, padding=(1, 1), groups=384)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.35815364122390747, zero_point=66, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.8943082690238953, zero_point=65, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.07193729281425476, zero_point=5\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.08733467757701874, zero_point=65)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), scale=0.14266113936901093, zero_point=61)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), scale=0.29754018783569336, zero_point=66, padding=(1, 1), groups=744)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(744, 124, kernel_size=(1, 1), stride=(1, 1), scale=1.0369744300842285, zero_point=39, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(124, 744, kernel_size=(1, 1), stride=(1, 1), scale=3.170830249786377, zero_point=68, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.05366344004869461, zero_point=7\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.06205359473824501, zero_point=65)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=0.09962867945432663, zero_point=64\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (7): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), scale=0.13490043580532074, zero_point=63)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), scale=0.4128626585006714, zero_point=68, padding=(1, 1), groups=712)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), scale=0.939403772354126, zero_point=51, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), scale=3.159916400909424, zero_point=67, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.2488006055355072, zero_point=2\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.1701580286026001, zero_point=57)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), scale=0.4784890115261078, zero_point=58)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), scale=1.49311363697052, zero_point=38, padding=(1, 1), groups=1360)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(1360, 226, kernel_size=(1, 1), stride=(1, 1), scale=4.447453022003174, zero_point=47, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(226, 1360, kernel_size=(1, 1), stride=(1, 1), scale=37.03371810913086, zero_point=68, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.048829615116119385, zero_point=8\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.07449629157781601, zero_point=65)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=0.16228587925434113, zero_point=59\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (9): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), scale=0.29199013113975525, zero_point=59)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), scale=0.7750792503356934, zero_point=70, padding=(1, 1), groups=1296)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), scale=3.876459836959839, zero_point=60, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), scale=32.29719161987305, zero_point=63, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.11643602699041367, zero_point=3\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.13541610538959503, zero_point=63)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): QuantizedLinear(in_features=576, out_features=10, scale=0.2719227373600006, zero_point=47, qscheme=torch.per_channel_affine)\n",
      "    )\n",
      "  )\n",
      "  (dequant): DeQuantize()\n",
      ")\n",
      "\n",
      "Training Time: 367.120 s\n",
      "Test Accuracy: 72.21%\n"
     ]
    }
   ],
   "source": [
    "from phinet_quant import PhiNet\n",
    "\n",
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "model_qat_quant_cifar_fp32 = PhiNet(input_shape = [3, 32, 32], num_layers = 7, alpha = 3, beta = 0.75, t_zero = 6, include_top = True, num_classes = 10)\n",
    "remove_depthwise(model_qat_quant_cifar_fp32)\n",
    "\n",
    "load_epoch = 2\n",
    "\n",
    "if load_epoch is not None:\n",
    "    saved_model_path = f\"modified_model/model_epoch_{load_epoch}.pt\" \n",
    "    checkpoint = torch.load(saved_model_path)\n",
    "    model_qat_quant_cifar_fp32.load_state_dict(checkpoint)\n",
    "\n",
    "model_qat_quant_cifar_fp32.to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "model_qat_quant_cifar_fp32_fused = copy.deepcopy(model_qat_quant_cifar_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "model_qat_quant_cifar_fp32.eval()\n",
    "model_qat_quant_cifar_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet model\n",
    "phinet_fuse_modules(model_qat_quant_cifar_fp32_fused)\n",
    "\n",
    "print(f\"model_fp32:\\n {model_qat_quant_cifar_fp32}\\n\")\n",
    "print(f\"model_fp32_fused:\\n {model_qat_quant_cifar_fp32_fused}\\n\")\n",
    "\n",
    "# Insert stubs ion the model and in the forward pass\n",
    "model_qat_quant_cifar_int8 = PhiNetQuant(model_qat_quant_cifar_fp32_fused)\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "model_qat_quant_cifar_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare_qat(model_qat_quant_cifar_int8.train(), inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {model_qat_quant_cifar_int8}\\n\")\n",
    "\n",
    "# Quantization awere training and calibrate the model\n",
    "model_qat_quant_cifar_int8, train_time_model_qat_quant_cifar_int8 = train(model_qat_quant_cifar_int8, train_loader, device = \"cuda\", learning_rate=0.001, num_epochs=10, save_dir=\"qat_model\")\n",
    "\n",
    "model_qat_quant_cifar_int8.to(\"cpu\")\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "model_qat_quant_cifar_int8 = torch.ao.quantization.convert(model_qat_quant_cifar_int8, inplace=True)\n",
    "\n",
    "print(f\"model_int8:\\n {model_qat_quant_cifar_int8}\\n\")\n",
    "\n",
    "acc_model_qat_quant_cifar_int8, _ = test(model_qat_quant_cifar_int8, test_loader, \"cpu\")\n",
    "\n",
    "print(f\"Training Time: {train_time_model_qat_quant_cifar_int8/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_qat_quant_cifar_int8:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comper the results:\n",
    "- Orginal model FP32;\n",
    "- Modified model FP32;\n",
    "- Static Quantization of the modified model INT8;\n",
    "- Quantization aware training of the modified model INT8;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal FP32 model CPU Inference Latency: 5.938 ms\n",
      "Orginal FP32 model test accuracy: 75.72%\n",
      "\n",
      "Modified FP32 model CPU Inference Latency: 4.429 ms\n",
      "Modified FP32 model test accuracy: 77.33%\n",
      "\n",
      "Modified INT8 model static quant CPU Inference Latency: 2.710 ms\n",
      "Modified INT8 model static quant test accuracy: 76.04%\n",
      "\n",
      "Modified INT8 model QAT CPU Inference Latency: 2.390 ms\n",
      "Modified INT8 model QAT test accuracy: 72.21%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_fp32_cpu_inference_latency  = measure_inference_latency(model_cifar_fp32, device=\"cpu\", input_shape=(3, 32, 32))\n",
    "model_quant_fp32_cpu_inference_latency  = measure_inference_latency(model_quant_cifar_fp32, device=\"cpu\", input_shape=(3, 32, 32))\n",
    "model_st_quant_int8_cpu_inference_latency  = measure_inference_latency(model_st_quant_cifar_int8, device=\"cpu\", input_shape=(3, 32, 32))\n",
    "model_qat_quant_int8_cpu_inference_latency  = measure_inference_latency(model_qat_quant_cifar_int8, device=\"cpu\", input_shape=(3, 32, 32))\n",
    "\n",
    "print(\"Orginal FP32 model CPU Inference Latency: {:.3f} ms\".format(model_fp32_cpu_inference_latency[0]))\n",
    "print(\"Orginal FP32 model test accuracy: {:.2f}%\\n\".format(acc_model_cifar_fp32))\n",
    "\n",
    "print(\"Modified FP32 model CPU Inference Latency: {:.3f} ms\".format(model_quant_fp32_cpu_inference_latency[0]))\n",
    "print(\"Modified FP32 model test accuracy: {:.2f}%\\n\".format(acc_model_quant_cifar_fp32))\n",
    "\n",
    "print(\"Modified INT8 model static quant CPU Inference Latency: {:.3f} ms\".format(model_st_quant_int8_cpu_inference_latency[0]))\n",
    "print(\"Modified INT8 model static quant test accuracy: {:.2f}%\\n\".format(acc_model_st_quant_cifar_int8))\n",
    "\n",
    "print(\"Modified INT8 model QAT CPU Inference Latency: {:.3f} ms\".format(model_qat_quant_int8_cpu_inference_latency[0]))\n",
    "print(\"Modified INT8 model QAT test accuracy: {:.2f}%\\n\".format(acc_model_qat_quant_cifar_int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# Set random seed for reproducibility\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "x = torch.rand(1, 1, 5, 5)\n",
    "\n",
    "conv_layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, bias = False)\n",
    "\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1, bias = False)\n",
    "conv2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=0, bias = False)\n",
    "pad = nn.ZeroPad2d((1,1,1,1))\n",
    "\n",
    "conv1.weight.data = conv_layer.weight.data.clone()\n",
    "conv2.weight.data = conv_layer.weight.data.clone()\n",
    "\n",
    "print(\"Common Conv Layer Weight:\")\n",
    "print(conv_layer.weight.data)\n",
    "\n",
    "print(\"\\nConv1 Weight:\")\n",
    "print(conv1.weight.data)\n",
    "\n",
    "print(\"\\nConv2 Weight:\")\n",
    "print(conv2.weight.data)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Output: {conv_layer(x)}\")\n",
    "print(f\"Output conv1: {conv1(x)}\")\n",
    "print(f\"Output conv2: {conv2(pad(x))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
