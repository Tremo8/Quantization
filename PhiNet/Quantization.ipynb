{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook to demonstrate PhiNet's problems with quantization and the changes adopted to solve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import phinet_quant\n",
    "import micromind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    \"\"\"\n",
    "    Print the size of the model.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size_in_bytes = os.path.getsize(\"temp.p\")\n",
    "    if size_in_bytes < 1048576:\n",
    "        size_in_kb = size_in_bytes / 1024\n",
    "        print(\"{:.3f} KB\".format(size_in_kb))\n",
    "    else:\n",
    "        size_in_mb = size_in_bytes / 1048576\n",
    "        print(\"{:.3f} MB\".format(size_in_mb))\n",
    "    os.remove('temp.p')\n",
    "\n",
    "\n",
    "def measure_inference_latency(model, input_shape, device = None, repetitions=100, warmup_it = 10):\n",
    "    \"\"\"\n",
    "    Measures the inference time of the provided neural network model.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to evaluate.\n",
    "        input_shape: The shape of the input data expected by the model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the mean and standard deviation of the inference time\n",
    "               measured in milliseconds.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device.type  # Get the device where the model is located\n",
    "    \n",
    "    dummy_input = torch.randn(1, *input_shape, dtype=torch.float).to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # GPU warm-up\n",
    "    for _ in range(warmup_it):\n",
    "        _ = model(dummy_input)\n",
    "\n",
    "    # Measure inference time\n",
    "    timings = []\n",
    "    with torch.no_grad():\n",
    "        if device == 'cuda':\n",
    "            starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "            for rep in range(repetitions):\n",
    "                starter.record()\n",
    "                _ = model(dummy_input)\n",
    "                ender.record()\n",
    "                torch.cuda.synchronize()\n",
    "                curr_time = starter.elapsed_time(ender)\n",
    "                timings.append(curr_time)\n",
    "        else:  # CPU\n",
    "            for rep in range(repetitions):\n",
    "                start_time = time.time()\n",
    "                _ = model(dummy_input)\n",
    "                end_time = time.time()\n",
    "                elapsed_time = (end_time - start_time) * 1000.0  # Convert to milliseconds\n",
    "                timings.append(elapsed_time)\n",
    "\n",
    "    # Calculate mean and std\n",
    "    mean_time = np.mean(timings)\n",
    "    std_time = np.std(timings)\n",
    "\n",
    "    return mean_time, std_time\n",
    "\n",
    "\n",
    "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32), verbose=False):\n",
    "    \"\"\"\n",
    "    Tests whether two models are equivalent by comparing their outputs on random inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    model_1.to(device)\n",
    "    model_2.to(device)\n",
    "\n",
    "    for i in range(num_tests):\n",
    "        print(f\"Running test {i+1}/{num_tests}\") if verbose else None\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        y1 = model_1(x).detach().cpu().numpy()\n",
    "        y2 = model_2(x).detach().cpu().numpy()\n",
    "        print(\"Difference: \", np.max(np.abs(y1-y2))) if verbose else None\n",
    "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
    "            print(\"Model equivalence test sample failed: \")\n",
    "            print(y1)\n",
    "            print(y2)\n",
    "            return False\n",
    "    print(\"Model equivalence test passed!\")\n",
    "    return True\n",
    "\n",
    "def set_random_seeds(random_seed=0):\n",
    "    \"\"\"\n",
    "    Set all random seeds to a fixed value to make results reproducible.\n",
    "    \"\"\"\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization\n",
    "Original PhiNet definition not quantizable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = micromind.PhiNet(input_shape = [3, 224, 224], num_layers = 7, alpha = 3, beta = 0.75,t_zero = 6.0, include_top= True, num_classes = 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhiNet Convolutional Block 1\n",
    "First of two versions of PhiNet's convolutional blocks. In this case, the depth-wise convolutional block is defined as follows:\n",
    "\n",
    "```python\n",
    "DepthwiseConv2d(ch, ch, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1357, bias=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block1 = model._layers[6]\n",
    "print(block1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the original PhiNet convolutional block 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiNetConv1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pointconv1 = nn.Sequential(\n",
    "                nn.Conv2d(144, 740, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(740, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "            )\n",
    "        self.drop = nn.Dropout2d(p=0.05)\n",
    "        self.depthwise = nn.Sequential(\n",
    "                nn.Conv2d(740,740,kernel_size=3,stride=1,padding=1,groups=740,bias=False),\n",
    "                nn.BatchNorm2d(740, eps=1e-3, momentum=0.999),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.se = torch.nn.Sequential(\n",
    "                nn.Conv2d(740, 123, kernel_size=1, stride=1, bias=False),\n",
    "                nn.Conv2d(123, 740, kernel_size=1, stride=1, bias=False),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.pointconv2 = nn.Sequential(\n",
    "                nn.Conv2d(740, 144, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(144, eps=1e-3, momentum=0.999),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pointconv1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pointconv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = PhiNetConv1()\n",
    "print(\"Defined Block\")\n",
    "print(test)\n",
    "print(\"Orginal BLock\")\n",
    "print(block1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization of the original convolutional block 1 of PhiNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "Conv1_fp32 = PhiNetConv1().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "Conv1_fp32_fused = copy.deepcopy(Conv1_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "Conv1_fp32.eval()\n",
    "Conv1_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "Conv1_fp32_fused = torch.ao.quantization.fuse_modules(Conv1_fp32_fused, [['pointconv1.0', 'pointconv1.1'], ['depthwise.0', 'depthwise.1'], ['pointconv2.0', 'pointconv2.1']])\n",
    "\n",
    "print(f\"Conv1_fp32:\\n {Conv1_fp32}\\n\")\n",
    "print(f\"Conv1_fp32_fused:\\n {Conv1_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=Conv1_fp32, model_2=Conv1_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,144,28,28)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "Conv1_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  Conv1_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "Conv1_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(Conv1_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {Conv1_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,144, 28, 28)\n",
    "    Conv1_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "Conv1_int8 = torch.ao.quantization.convert(Conv1_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv1_int8: \\n {Conv1_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = Conv1_int8(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the size of FP32 and INT8 blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"FP32 size:\")\n",
    "print_size_of_model(Conv1_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(Conv1_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the inference time of FP32 and INT8 blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv1_fp32, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv1_int8, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dimensions of both the FP32 and INT8 models, the INT8 is indeed more compact in size, as expected. However, contrary to what the documentation suggests, the INT8 model has slower inference times compared to the original FP32 model. The documentation indicated an expected reduction in inference time of about half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification of PhiNet original convolutional block 1\n",
    "\n",
    "One of the problems could be that the number of channels is not divisible by 8. Therefore, we modify the convolutional block by making the channels divisible by 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiNetConv1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pointconv1 = nn.Sequential(\n",
    "                nn.Conv2d(144, 744, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(744, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "            )\n",
    "        self.drop = nn.Dropout2d(p=0.05)\n",
    "        self.depthwise = nn.Sequential(\n",
    "                nn.Conv2d(744,744,kernel_size=3,stride=1,padding=1,groups=744,bias=False),\n",
    "                nn.BatchNorm2d(744, eps=1e-3, momentum=0.999),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.se = torch.nn.Sequential(\n",
    "                nn.Conv2d(744, 123, kernel_size=1, stride=1, bias=False),\n",
    "                nn.Conv2d(123, 744, kernel_size=1, stride=1, bias=False),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.pointconv2 = nn.Sequential(\n",
    "                nn.Conv2d(744, 144, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(144, eps=1e-3, momentum=0.999),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pointconv1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pointconv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = PhiNetConv1()\n",
    "print(\"Defined Block\")\n",
    "print(test)\n",
    "print(\"Orginal BLock\")\n",
    "print(block1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "Conv1_fp32 = PhiNetConv1().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "Conv1_fp32_fused = copy.deepcopy(Conv1_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "Conv1_fp32.eval()\n",
    "Conv1_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "Conv1_fp32_fused = torch.ao.quantization.fuse_modules(Conv1_fp32_fused, [['pointconv1.0', 'pointconv1.1'], ['depthwise.0', 'depthwise.1'], ['pointconv2.0', 'pointconv2.1']])\n",
    "\n",
    "print(f\"Conv1_fp32:\\n {Conv1_fp32}\\n\")\n",
    "print(f\"Conv1_fp32_fused:\\n {Conv1_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=Conv1_fp32, model_2=Conv1_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,144,28,28)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "Conv1_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  Conv1_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "Conv1_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(Conv1_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {Conv1_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,144, 28, 28)\n",
    "    Conv1_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "Conv1_int8 = torch.ao.quantization.convert(Conv1_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv1_int8: \\n {Conv1_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = Conv1_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(Conv1_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(Conv1_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv1_fp32, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv1_int8, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv1_fp32, device =\"cpu\", input_shape=(144, 224, 224))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv1_int8, device =\"cpu\", input_shape=(144, 224, 224))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, after the number of channels has been made divisible by 8, the model in INT8 is smaller in terms of memory. In contrast to the original block, the inference time is also reduced, with the reduction increasing as the input resolution increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhiNet Convolutional Block 2\n",
    "\n",
    "Second versions of PhiNet's convolutional blocks. In this case, the depth-wise convolutional block is defined as follows:\n",
    "\n",
    "```python\n",
    "DepthwiseConv2d(ch, ch, kernel_size=(3, 3), stride=(2, 2), groups=ch, bias=False)\n",
    "```\n",
    "Diffrently from the PhiNet Convolutional Block 1, here we have ` stride=(2, 2)` and `padding = 0`. Moreover, before the `Dropout2d(p=0.05)` it is inserted the following layer:\n",
    "\n",
    "```python \n",
    "ZeroPad2d((1, 1, 1, 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block2 = model._layers[7]\n",
    "print(block2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the original PhiNet convolutional block 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiNetConv2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pointconv1 = nn.Sequential(\n",
    "                nn.Conv2d(144, 709, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(709, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "            )\n",
    "        self.pad = nn.ZeroPad2d(1)\n",
    "        self.drop = nn.Dropout2d(p=0.05)\n",
    "        self.depthwise = nn.Sequential(\n",
    "                nn.Conv2d(709,709,kernel_size=3,stride=2,padding=0,groups=709,bias=False),\n",
    "                nn.BatchNorm2d(709, eps=1e-3, momentum=0.999),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.se = torch.nn.Sequential(\n",
    "                nn.Conv2d(709, 118, kernel_size=1, stride=1, bias=False),\n",
    "                nn.Conv2d(118, 709, kernel_size=1, stride=1, bias=False),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.pointconv2 = nn.Sequential(\n",
    "                nn.Conv2d(709, 288, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(288, eps=1e-3, momentum=0.999),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pointconv1(x)\n",
    "        x = self.pad(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pointconv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = PhiNetConv2()\n",
    "print(\"Defined Block\")\n",
    "print(test)\n",
    "print(\"Orginal BLock\")\n",
    "print(block2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization of the original convolutional block 2 of PhiNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "Conv2_fp32 = PhiNetConv2().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "Conv2_fp32_fused = copy.deepcopy(Conv2_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "Conv2_fp32.eval()\n",
    "Conv2_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "Conv2_fp32_fused = torch.ao.quantization.fuse_modules(Conv2_fp32_fused, [['pointconv1.0', 'pointconv1.1'], ['depthwise.0', 'depthwise.1'], ['pointconv2.0', 'pointconv2.1']])\n",
    "\n",
    "print(f\"Conv2_fp32:\\n {Conv2_fp32}\\n\")\n",
    "print(f\"Conv2_fp32_fused:\\n {Conv2_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=Conv2_fp32, model_2=Conv2_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,144,28,28)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "Conv2_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  Conv2_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "Conv2_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(Conv2_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {Conv2_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,144, 28, 28)\n",
    "    Conv2_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "Conv2_int8 = torch.ao.quantization.convert(Conv2_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv2_int8: \\n {Conv2_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = Conv2_int8(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the size of FP32 and INT8 blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(Conv2_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(Conv2_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the inference time of FP32 and INT8 blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv2_fp32, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv2_int8, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dimensions of both the FP32 and INT8 models, the INT8 is indeed more compact in size, as expected. However, contrary to what the documentation suggests, the INT8 model has slower inference times compared to the original FP32 model. The documentation indicated an expected reduction in inference time of about half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification of PhiNet original convolutional block 2\n",
    "\n",
    "One of the problems could be that the number of channels is not divisible by 8. Therefore, we modify the convolutional block by making the channels divisible by 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiNetConv2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pointconv1 = nn.Sequential(\n",
    "                nn.Conv2d(144, 712, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(712, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "            )\n",
    "        self.pad = nn.ZeroPad2d(1)\n",
    "        self.drop = nn.Dropout2d(p=0.05)\n",
    "        self.depthwise = nn.Sequential(\n",
    "                nn.Conv2d(712,712,kernel_size=3,stride=2,padding=0,groups=712,bias=False),\n",
    "                nn.BatchNorm2d(712, eps=1e-3, momentum=0.999),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.se = torch.nn.Sequential(\n",
    "                nn.Conv2d(712, 118, kernel_size=1, stride=1, bias=False),\n",
    "                nn.Conv2d(118, 712, kernel_size=1, stride=1, bias=False),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.pointconv2 = nn.Sequential(\n",
    "                nn.Conv2d(712, 288, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(288, eps=1e-3, momentum=0.999),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pointconv1(x)\n",
    "        x = self.pad(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pointconv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = PhiNetConv2()\n",
    "print(\"Defined Block\")\n",
    "print(test)\n",
    "print(\"Orginal BLock\")\n",
    "print(block2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "Conv2_fp32 = PhiNetConv2().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "Conv2_fp32_fused = copy.deepcopy(Conv2_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "Conv2_fp32.eval()\n",
    "Conv2_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "Conv2_fp32_fused = torch.ao.quantization.fuse_modules(Conv2_fp32_fused, [['pointconv1.0', 'pointconv1.1'], ['depthwise.0', 'depthwise.1'], ['pointconv2.0', 'pointconv2.1']])\n",
    "\n",
    "print(f\"Conv2_fp32:\\n {Conv2_fp32}\\n\")\n",
    "print(f\"Conv2_fp32_fused:\\n {Conv2_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=Conv2_fp32, model_2=Conv2_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,144,28,28)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "Conv2_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  Conv2_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "Conv2_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(Conv2_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {Conv2_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,144, 28, 28)\n",
    "    Conv2_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "Conv2_int8 = torch.ao.quantization.convert(Conv2_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv2_int8: \\n {Conv2_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = Conv2_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(Conv2_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(Conv2_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv2_fp32, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv2_int8, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the original convolutional block 1 of PhiNet. Making the channels divisible by 8 is not enough. In fact, the model in INT8 is inferior in terms of memomics but slower in terms of inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification of PhiNet original convolutional block 2 (V2)\n",
    "The number of channels of the convolutional blocks is made divisible by 8 and `nn.ZeroPad2d(1)` is removed and placed inside the Depthwise Convolution using `Conv2d(padding = 1)`. The depth-wise convolution becomes:\n",
    "\n",
    "```python\n",
    "Conv2d(ch, ch, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=ch, bias=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PhiNet Convolutional Block without number of channel divisible by 8\n",
    "class PhiNetConv2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized\n",
    "        self.pointconv1 = nn.Sequential(\n",
    "                nn.Conv2d(144, 712, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(712, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "            )\n",
    "        self.drop = nn.Dropout2d(p=0.05)\n",
    "        self.depthwise = nn.Sequential(\n",
    "                nn.Conv2d(712,712,kernel_size=3,stride=2,padding=1,groups=712,bias=False),\n",
    "                nn.BatchNorm2d(712, eps=1e-3, momentum=0.999),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.se = torch.nn.Sequential(\n",
    "                nn.Conv2d(712, 118, kernel_size=1, stride=1, bias=False),\n",
    "                nn.Conv2d(118, 712, kernel_size=1, stride=1, bias=False),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.pointconv2 = nn.Sequential(\n",
    "                nn.Conv2d(712, 288, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(288, eps=1e-3, momentum=0.999),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pointconv1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pointconv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = PhiNetConv2()\n",
    "print(\"Defined Block\")\n",
    "print(test)\n",
    "print(\"Orginal BLock\")\n",
    "print(block2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "Conv2_fp32 = PhiNetConv2().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "Conv2_fp32_fused = copy.deepcopy(Conv2_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "Conv2_fp32.eval()\n",
    "Conv2_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "Conv2_fp32_fused = torch.ao.quantization.fuse_modules(Conv2_fp32_fused, [['pointconv1.0', 'pointconv1.1'], ['depthwise.0', 'depthwise.1'], ['pointconv2.0', 'pointconv2.1']])\n",
    "\n",
    "print(f\"Conv2_fp32:\\n {Conv2_fp32}\\n\")\n",
    "print(f\"Conv2_fp32_fused:\\n {Conv2_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=Conv2_fp32, model_2=Conv2_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,144,28,28)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "Conv2_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  Conv2_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "Conv2_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(Conv2_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {Conv2_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,144, 28, 28)\n",
    "    Conv2_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "Conv2_int8 = torch.ao.quantization.convert(Conv2_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv2_int8: \\n {Conv2_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = Conv2_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(Conv1_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(Conv1_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv2_fp32, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv2_int8, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv2_fp32, device =\"cpu\", input_shape=(144, 224, 224))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv2_int8, device =\"cpu\", input_shape=(144, 224, 224))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, after making the number of channels divisible by 8 and removing `ZeroPad2d`, the model in INT8 is smaller in terms of memory and faster in terms of inference time, with the reduction increasing as the input resolution increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhiNet Convolutional Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the `layer` variable, I check whether the changes are effective on all PhiNet blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 3\n",
    "block = model._layers[layer]\n",
    "resolution = [112, 56, 56, 28, 28, 14, 14, 7]\n",
    "res = resolution[layer-3]\n",
    "\n",
    "input = block._layers[0].in_channels\n",
    "channel = _make_divisible(block._layers[0].out_channels, 8)\n",
    "stride = block._layers[-6].stride\n",
    "if isinstance(block._layers[3], nn.ZeroPad2d):\n",
    "    padding = 1\n",
    "else:\n",
    "    padding = block._layers[-6].padding\n",
    "se_ch = block._layers[-3].se_conv.out_channels\n",
    "output = block._layers[-2].out_channels\n",
    "\n",
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "                nn.Conv2d(input, channel, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(channel, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "                nn.Dropout2d(p=0.05),\n",
    "                nn.Conv2d(channel,channel,kernel_size=3,stride=stride,padding=padding,groups=channel,bias=False),\n",
    "                nn.BatchNorm2d(channel, eps=1e-3, momentum=0.999),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "                torch.nn.Sequential(\n",
    "                    nn.Conv2d(channel, se_ch, kernel_size=1, stride=1, bias=False),\n",
    "                    nn.Conv2d(se_ch, channel, kernel_size=1, stride=1, bias=False),\n",
    "                    torch.nn.Hardswish(inplace=True),\n",
    "                ),\n",
    "                nn.Conv2d(channel, output, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(output, eps=1e-3, momentum=0.999),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = ConvBlock()\n",
    "print(\"Defined Block\")\n",
    "print(test)\n",
    "print(\"Orginal Block\")\n",
    "print(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "Conv_fp32 = ConvBlock().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "Conv_fp32_fused = copy.deepcopy(Conv_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "Conv_fp32.eval()\n",
    "Conv_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "Conv_fp32_fused = torch.ao.quantization.fuse_modules(Conv_fp32_fused, [['conv.0', 'conv.1'], ['conv.4', 'conv.5'], ['conv.8', 'conv.9']])\n",
    "\n",
    "print(f\"Conv2_fp32:\\n {Conv_fp32}\\n\")\n",
    "print(f\"Conv2_fp32_fused:\\n {Conv_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=Conv_fp32, model_2=Conv_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,input,res,res)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "Conv_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  Conv_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "Conv_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(Conv_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {Conv_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,input, res, res)\n",
    "    Conv_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "Conv_int8 = torch.ao.quantization.convert(Conv_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv2_int8: \\n {Conv_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "result = Conv_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(Conv_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(Conv_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv_fp32, device =\"cpu\", input_shape=(input,res,res))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv_int8, device =\"cpu\", input_shape=(input,res,res))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SeparableConvolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also check the Separable Convolutional BLock in the PhiNet input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputBlock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pad = nn.ZeroPad2d((0,1,0,1))\n",
    "        self.conv = nn.Sequential(\n",
    "                nn.Conv2d(3, 3, kernel_size=3, stride=2, padding=0, groups=3, bias=False),\n",
    "                nn.Conv2d(3, 144, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(144, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pad(x)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = InputBlock()\n",
    "print(test)\n",
    "\n",
    "block = model._layers[1]\n",
    "print(block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "InConv_fp32 = InputBlock().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "InConv_fp32_fused = copy.deepcopy(InConv_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "InConv_fp32.eval()\n",
    "InConv_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "InConv_fp32_fused = torch.ao.quantization.fuse_modules(InConv_fp32_fused, [['conv.1', 'conv.2']])\n",
    "\n",
    "print(f\"Conv2_fp32:\\n {InConv_fp32}\\n\")\n",
    "print(f\"Conv2_fp32_fused:\\n {InConv_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=InConv_fp32, model_2=InConv_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,3,224,224)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "InConv_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  InConv_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "InConv_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(InConv_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {InConv_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,3, 224, 224)\n",
    "    InConv_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "InConv_int8 = torch.ao.quantization.convert(InConv_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv2_int8: \\n {InConv_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "result = InConv_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(InConv_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(InConv_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(InConv_fp32, device =\"cpu\", input_shape=(3,224,224))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(InConv_int8, device =\"cpu\", input_shape=(3,224,224))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the FP32 block, the INT8 block is larger in terms of B. This could be due to the addition of the two fake quantization blocks. In addition, the slightly worse inference time is due to the fact that the channels are not divisible by 8 in depth convolution. Although the performance in general is slightly worse, in the PhiNet complex, it does not affect the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete PhiNet Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phinet_quant import PhiNetConvBlock, SeparableConv2d, DepthwiseConv2d\n",
    "\n",
    "def phinet_fuse_modules(model):\n",
    "    for basic_block_name, basic_block in model._layers.named_children():\n",
    "        if isinstance(basic_block, SeparableConv2d):\n",
    "            torch.ao.quantization.fuse_modules(basic_block, [[\"_layers.1\", \"_layers.2\"]], inplace=True)\n",
    "        if isinstance(basic_block, PhiNetConvBlock) and len(basic_block._layers) == 6:\n",
    "            torch.ao.quantization.fuse_modules(basic_block, [[\"_layers.1\", \"_layers.2\"], [\"_layers.4\", \"_layers.5\"]], inplace=True)\n",
    "        elif isinstance(basic_block, PhiNetConvBlock):\n",
    "            torch.ao.quantization.fuse_modules(basic_block, [[\"_layers.0\", \"_layers.1\"], [\"_layers.4\", \"_layers.5\"], [\"_layers.8\", \"_layers.9\"]], inplace=True)\n",
    "\n",
    "def remove_depthwise(model):\n",
    "    def convert_to_conv2d(depthwise_conv2d):\n",
    "        in_channels = depthwise_conv2d.in_channels\n",
    "        depth_multiplier = depthwise_conv2d.out_channels // in_channels\n",
    "        kernel_size = depthwise_conv2d.kernel_size\n",
    "        stride = depthwise_conv2d.stride\n",
    "        padding = depthwise_conv2d.padding\n",
    "        dilation = depthwise_conv2d.dilation\n",
    "        bias = depthwise_conv2d.bias is not None\n",
    "        padding_mode = depthwise_conv2d.padding_mode\n",
    "\n",
    "        # Create an equivalent nn.Conv2d layer\n",
    "        conv2d_layer = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=in_channels * depth_multiplier,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=in_channels,  # Set groups to in_channels for depthwise convolution\n",
    "            bias=bias,\n",
    "            padding_mode=padding_mode,\n",
    "        )\n",
    "\n",
    "        # If bias was not used in the original depthwise_conv2d, set bias to None in conv2d_layer\n",
    "        if not bias:\n",
    "            conv2d_layer.bias = None\n",
    "\n",
    "        return conv2d_layer\n",
    "\n",
    "    for name, module in model._layers.named_children():\n",
    "        if isinstance(module, PhiNetConvBlock):\n",
    "            for i, layer in enumerate(module._layers.children()):\n",
    "                if isinstance(layer, DepthwiseConv2d):\n",
    "                    module._layers[i] = convert_to_conv2d(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of PhiNet with the modifications shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(0)\n",
    "\n",
    "# Import the quantizeble model\n",
    "model_fp32 = phinet_quant.PhiNet(input_shape = [3, 224, 224], num_layers=7, alpha= 3.0, beta= 0.75,t_zero= 6.0, include_top= True, num_classes=10, divisor=8)\n",
    "print(f\"Orginal Model: {model_fp32}\\n\")\n",
    "remove_depthwise(model_fp32)\n",
    "print(f\"Model without Depthwise: {model_fp32}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "model_fp32.to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "model_fp32_fused = copy.deepcopy(model_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "model_fp32.eval()\n",
    "model_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet model\n",
    "phinet_fuse_modules(model_fp32_fused)\n",
    "\n",
    "print(f\"model_fp32:\\n {model_fp32}\\n\")\n",
    "print(f\"model_fp32_fused:\\n {model_fp32_fused}\\n\")\n",
    "\n",
    "assert model_equivalence(model_1=model_fp32, model_2=model_fp32_fused, device=\"cpu\", rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,224,224)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "model_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  model_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "model_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(model_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {model_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,3, 224, 224)\n",
    "    model_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "model_int8 = torch.ao.quantization.convert(model_int8, inplace=True)\n",
    "\n",
    "print(f\"model_int8:\\n {model_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "result = model_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(model_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(model_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(model_fp32, device =\"cpu\", input_shape=(3,224,224))\n",
    "fp32_gpu_inference_latency  = measure_inference_latency(model_fp32, device =\"cuda\", input_shape=(3,224,224))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(model_int8, device =\"cpu\", input_shape=(3,224,224))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"FP32 GPU Inference Latency: {:.3f} ms\".format(fp32_gpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, after the modification, the model in INT8 is smaller in terms of memory. In contrast to the original model, the inference time on CPU is also reduced, with the reduction increasing as the input resolution increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train(model, train_loader, device, learning_rate=1e-1, num_epochs=200, save_dir=\"saved_models\"):\n",
    "    # Create the save directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    tot_exp_time = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Initialize the timer\n",
    "        starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "        # Start Recording the time\n",
    "        starter.record()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            prob = nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = prob.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "\n",
    "        # Save the model state dictionary at the end of each epoch\n",
    "        model_save_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pt\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "        # Save some statics the be saved in output\n",
    "        tot_exp_time += curr_time\n",
    "        accuracy = 100.0 * correct / total\n",
    "        average_loss = train_loss / len(train_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} : Train accuracy {accuracy:.2f}%, Train loss {average_loss:.4f}, Training Time: {curr_time/1000:.3f} s\")\n",
    "\n",
    "    return model, tot_exp_time\n",
    "\n",
    "def test(model, test_loader, device, criterion= nn.CrossEntropyLoss()):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            prob = nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = prob.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    average_loss = test_loss / len(test_loader)\n",
    "\n",
    "    return accuracy, average_loss\n",
    "\n",
    "def calibrate_model(model, loader, device=torch.device(\"cpu\")):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        _ = model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 7 #4\n",
    "alpha = 3 # 0.5\n",
    "beta = 0.75 # 1\n",
    "t_zero = 6 # 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256):\n",
    "\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = (0.1307,), std = (0.3081,))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_set = torchvision.datasets.MNIST(root='./data', train=True, transform=train_transform, download=True)\n",
    "    test_set = torchvision.datasets.MNIST(root='./data', train=False, transform=train_transform)\n",
    "\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set, batch_size=train_batch_size,\n",
    "        sampler=train_sampler, num_workers=num_workers)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_set, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the original PhiNet model on the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "model_mnist_fp32 = micromind.PhiNet(input_shape = [1, 28, 28], num_layers = num_layers, alpha = alpha, beta = beta, t_zero = t_zero, include_top = True, num_classes = 10)\n",
    "print(model_mnist_fp32)\n",
    "\n",
    "train_loader, test_loader = prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_mnist_fp32, train_time_model_mnist_fp32 = train(model_mnist_fp32, train_loader, device, learning_rate=0.001, num_epochs=10, save_dir=\"orignal_model\")\n",
    "\n",
    "acc_model_mnist_fp32, _ = test(model_mnist_fp32, test_loader, device)\n",
    "\n",
    "print(f\"Training Time: {train_time_model_mnist_fp32/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_mnist_fp32:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the modified PhiNet model on the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed for reproducibility\n",
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "model_quant_mnist_fp32 = phinet_quant.PhiNet(input_shape = [1, 28, 28], num_layers = num_layers, alpha = alpha, beta = beta, t_zero = t_zero, include_top = True, num_classes = 10, divisor=8)\n",
    "remove_depthwise(model_quant_mnist_fp32)\n",
    "print(model_quant_mnist_fp32)\n",
    "\n",
    "train_loader, test_loader = prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_quant_mnist_fp32, train_time_model_quant_mnist_fp32 = train(model_quant_mnist_fp32, train_loader, device, learning_rate=0.001, num_epochs=10, save_dir=\"modified_model\")\n",
    "\n",
    "acc_model_quant_mnist_fp32, _ = test(model_quant_mnist_fp32, test_loader, device)\n",
    "\n",
    "print(f\"Training Time: {train_time_model_quant_mnist_fp32/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_quant_mnist_fp32:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static Quantization of the modified PhiNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiNetQuant(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(PhiNetQuant, self).__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized.\n",
    "        # This will only be used for inputs.\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        # FP32 model\n",
    "        self.model_fp32 = model_fp32\n",
    "        # DeQuantStub converts tensors from quantized to floating point.\n",
    "        # This will only be used for outputs.\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "# Copy the pretrained model for static quantization\n",
    "model_st_quant_mnist_fp32 = copy.deepcopy(model_quant_mnist_fp32)\n",
    "model_st_quant_mnist_fp32.to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "model_st_quant_mnist_fp32_fused = copy.deepcopy(model_st_quant_mnist_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "model_st_quant_mnist_fp32.eval()\n",
    "model_st_quant_mnist_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet model\n",
    "phinet_fuse_modules(model_st_quant_mnist_fp32_fused)\n",
    "\n",
    "print(f\"model_fp32:\\n {model_st_quant_mnist_fp32}\\n\")\n",
    "print(f\"model_fp32_fused:\\n {model_st_quant_mnist_fp32_fused}\\n\")\n",
    "\n",
    "assert model_equivalence(model_1=model_st_quant_mnist_fp32, model_2=model_st_quant_mnist_fp32_fused, device=\"cpu\", rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,1,28,28)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "model_st_quant_mnist_int8 = PhiNetQuant(model_st_quant_mnist_fp32_fused)\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "model_st_quant_mnist_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(model_st_quant_mnist_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {model_st_quant_mnist_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "calibrate_model(model_st_quant_mnist_int8, train_loader, device=\"cpu\")\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "model_st_quant_mnist_int8 = torch.ao.quantization.convert(model_st_quant_mnist_int8, inplace=True)\n",
    "\n",
    "print(f\"model_int8:\\n {model_st_quant_mnist_int8}\\n\")\n",
    "\n",
    "acc_model_st_quant_mnist_int8, _ = test(model_st_quant_mnist_int8, test_loader, \"cpu\")\n",
    "\n",
    "print(f\"Test Accuracy: {acc_model_st_quant_mnist_int8:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization awere training of the modified model on the MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "model_qat_quant_mnist_fp32 = phinet_quant.PhiNet(input_shape = [1, 28, 28], num_layers = num_layers, alpha = alpha, beta = beta, t_zero = t_zero, include_top = True, num_classes = 10, divisor=8)\n",
    "remove_depthwise(model_qat_quant_mnist_fp32)\n",
    "\n",
    "load_epoch = 2\n",
    "\n",
    "if load_epoch is not None:\n",
    "    saved_model_path = f\"modified_model/model_epoch_{load_epoch}.pt\" \n",
    "    checkpoint = torch.load(saved_model_path)\n",
    "    model_qat_quant_mnist_fp32.load_state_dict(checkpoint)\n",
    "\n",
    "model_qat_quant_mnist_fp32.to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "model_qat_quant_mnist_fp32_fused = copy.deepcopy(model_qat_quant_mnist_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "model_qat_quant_mnist_fp32.eval()\n",
    "model_qat_quant_mnist_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet model\n",
    "phinet_fuse_modules(model_qat_quant_mnist_fp32_fused)\n",
    "\n",
    "print(f\"model_fp32:\\n {model_qat_quant_mnist_fp32}\\n\")\n",
    "print(f\"model_fp32_fused:\\n {model_qat_quant_mnist_fp32_fused}\\n\")\n",
    "\n",
    "# Insert stubs ion the model and in the forward pass\n",
    "model_qat_quant_mnist_int8 = PhiNetQuant(model_qat_quant_mnist_fp32_fused)\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "model_qat_quant_mnist_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare_qat(model_qat_quant_mnist_int8.train(), inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {model_qat_quant_mnist_int8}\\n\")\n",
    "\n",
    "# Quantization awere training and calibrate the model\n",
    "model_qat_quant_mnist_int8, train_time_model_qat_quant_mnist_int8 = train(model_qat_quant_mnist_int8, train_loader, device = \"cuda\", learning_rate=0.001, num_epochs=10, save_dir=\"qat_model\")\n",
    "\n",
    "model_qat_quant_mnist_int8.to(\"cpu\")\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "model_qat_quant_mnist_int8 = torch.ao.quantization.convert(model_qat_quant_mnist_int8, inplace=True)\n",
    "\n",
    "print(f\"model_int8:\\n {model_qat_quant_mnist_int8}\\n\")\n",
    "\n",
    "acc_model_qat_quant_mnist_int8, _ = test(model_qat_quant_mnist_int8, test_loader, \"cpu\")\n",
    "\n",
    "print(f\"Training Time: {train_time_model_qat_quant_mnist_int8/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_qat_quant_mnist_int8:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare results on **MNIST**:\n",
    "- Orginal model FP32;\n",
    "- Modified model FP32;\n",
    "- Static Quantization of the modified model INT8;\n",
    "- Quantization aware training of the modified model INT8;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32_cpu_inference_latency  = measure_inference_latency(model_mnist_fp32, device=\"cpu\", input_shape=(1, 28, 28))\n",
    "model_fp32_gpu_inference_latency  = measure_inference_latency(model_mnist_fp32, device=\"cuda\", input_shape=(1, 28, 28))\n",
    "\n",
    "model_quant_fp32_cpu_inference_latency  = measure_inference_latency(model_quant_mnist_fp32, device=\"cpu\", input_shape=(1, 28, 28))\n",
    "model_quant_fp32_gpu_inference_latency  = measure_inference_latency(model_quant_mnist_fp32, device=\"cuda\", input_shape=(1, 28, 28))\n",
    "\n",
    "model_st_quant_int8_cpu_inference_latency  = measure_inference_latency(model_st_quant_mnist_int8, device=\"cpu\", input_shape=(1, 28, 28))\n",
    "model_qat_quant_int8_cpu_inference_latency  = measure_inference_latency(model_qat_quant_mnist_int8, device=\"cpu\", input_shape=(1, 28, 28))\n",
    "\n",
    "print(\"Original FP32 model CPU Inference Latency: {:.3f} ms\".format(model_fp32_cpu_inference_latency[0]))\n",
    "print(\"Original FP32 model GPU Inference Latency: {:.3f} ms\".format(model_fp32_gpu_inference_latency[0]))\n",
    "print(\"Original FP32 model test accuracy: {:.2f}%\".format(acc_model_mnist_fp32))\n",
    "print(\"Original FP32 model training time: {:.3f} s\\n\".format(train_time_model_mnist_fp32/1000))\n",
    "\n",
    "print(\"Optimized FP32 model CPU Inference Latency: {:.3f} ms\".format(model_quant_fp32_cpu_inference_latency[0]))\n",
    "print(\"Optimized FP32 model GPU Inference Latency: {:.3f} ms\".format(model_quant_fp32_gpu_inference_latency[0]))\n",
    "print(\"Optimized FP32 model test accuracy: {:.2f}%\".format(acc_model_quant_mnist_fp32))\n",
    "print(\"Optimized FP32 model training time: {:.3f} s\\n\".format(train_time_model_quant_mnist_fp32/1000))\n",
    "\n",
    "print(\"Optimized INT8 model static quant CPU Inference Latency: {:.3f} ms\".format(model_st_quant_int8_cpu_inference_latency[0]))\n",
    "print(\"Optimized INT8 model static quant test accuracy: {:.2f}%\\n\".format(acc_model_st_quant_mnist_int8))\n",
    "\n",
    "print(\"Optimized INT8 model QAT CPU Inference Latency: {:.3f} ms\".format(model_qat_quant_int8_cpu_inference_latency[0]))\n",
    "print(\"Optimized INT8 model QAT test accuracy: {:.2f}%\".format(acc_model_qat_quant_mnist_int8))\n",
    "print(\"Optimized INT8 model QAT training time: {:.3f} s\\n\".format(train_time_model_qat_quant_mnist_int8/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the CIFAR10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256):\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    train_set = torchvision.datasets.CIFAR10(root='./data', train=True, transform=train_transform, download=True)\n",
    "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, transform=train_transform)\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set, batch_size=train_batch_size,\n",
    "        sampler=train_sampler, num_workers=num_workers)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_set, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the original PhiNet model on the CIFAR10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "model_cifar_fp32 = micromind.PhiNet(input_shape = [3, 32, 32], num_layers = 7, alpha = 3, beta = 0.75, t_zero = 6, include_top = True, num_classes = 10)\n",
    "print(model_cifar_fp32)\n",
    "\n",
    "train_loader, test_loader = prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_cifar_fp32, train_time_model_cifar_fp32 = train(model_cifar_fp32, train_loader, device, learning_rate=0.001, num_epochs=10, save_dir=\"orignal_model\")\n",
    "\n",
    "acc_model_cifar_fp32, _ = test(model_cifar_fp32, test_loader, device)\n",
    "\n",
    "print(f\"Training Time: {train_time_model_cifar_fp32/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_cifar_fp32:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the modified PhiNet model on the CIFAR10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed for reproducibility\n",
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "model_quant_cifar_fp32 = phinet_quant.PhiNet(input_shape = [3, 32, 32], num_layers = 7, alpha = 3, beta = 0.75, t_zero = 6, include_top = True, num_classes = 10, divisor=8)\n",
    "remove_depthwise(model_quant_cifar_fp32)\n",
    "print(model_quant_cifar_fp32)\n",
    "\n",
    "train_loader, test_loader = prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_quant_cifar_fp32, train_time_model_quant_cifar_fp32 = train(model_quant_cifar_fp32, train_loader, device, learning_rate=0.001, num_epochs=10, save_dir=\"modified_model\")\n",
    "\n",
    "acc_model_quant_cifar_fp32, _ = test(model_quant_cifar_fp32, test_loader, device)\n",
    "\n",
    "print(f\"Training Time: {train_time_model_quant_cifar_fp32/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_quant_cifar_fp32:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static Quantization of the modified PhiNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiNetQuant(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(PhiNetQuant, self).__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized.\n",
    "        # This will only be used for inputs.\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        # FP32 model\n",
    "        self.model_fp32 = model_fp32\n",
    "        # DeQuantStub converts tensors from quantized to floating point.\n",
    "        # This will only be used for outputs.\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "# Copy the pretrained model for static quantization\n",
    "model_st_quant_cifar_fp32 = copy.deepcopy(model_quant_cifar_fp32)\n",
    "model_st_quant_cifar_fp32.to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "model_st_quant_cifar_fp32_fused = copy.deepcopy(model_st_quant_cifar_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "model_st_quant_cifar_fp32.eval()\n",
    "model_st_quant_cifar_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet model\n",
    "phinet_fuse_modules(model_st_quant_cifar_fp32_fused)\n",
    "\n",
    "print(f\"model_fp32:\\n {model_st_quant_cifar_fp32}\\n\")\n",
    "print(f\"model_fp32_fused:\\n {model_st_quant_cifar_fp32_fused}\\n\")\n",
    "\n",
    "assert model_equivalence(model_1=model_st_quant_cifar_fp32, model_2=model_st_quant_cifar_fp32_fused, device=\"cpu\", rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "model_st_quant_cifar_int8 = PhiNetQuant(model_st_quant_cifar_fp32_fused)\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "model_st_quant_cifar_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(model_st_quant_cifar_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {model_st_quant_cifar_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "calibrate_model(model_st_quant_cifar_int8, train_loader, device=\"cpu\")\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "model_st_quant_cifar_int8 = torch.ao.quantization.convert(model_st_quant_cifar_int8, inplace=True)\n",
    "\n",
    "print(f\"model_int8:\\n {model_st_quant_cifar_int8}\\n\")\n",
    "\n",
    "acc_model_st_quant_cifar_int8, _ = test(model_st_quant_cifar_int8, test_loader, \"cpu\")\n",
    "\n",
    "print(f\"Test Accuracy: {acc_model_st_quant_cifar_int8:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization awere training of the modified model on the CIFAR10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "model_qat_quant_cifar_fp32 = phinet_quant.PhiNet(input_shape = [3, 32, 32], num_layers = 7, alpha = 3, beta = 0.75, t_zero = 6, include_top = True, num_classes = 10, divisor=8)\n",
    "remove_depthwise(model_qat_quant_cifar_fp32)\n",
    "\n",
    "load_epoch = 2\n",
    "\n",
    "if load_epoch is not None:\n",
    "    saved_model_path = f\"modified_model/model_epoch_{load_epoch}.pt\" \n",
    "    checkpoint = torch.load(saved_model_path)\n",
    "    model_qat_quant_cifar_fp32.load_state_dict(checkpoint)\n",
    "\n",
    "model_qat_quant_cifar_fp32.to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "model_qat_quant_cifar_fp32_fused = copy.deepcopy(model_qat_quant_cifar_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "model_qat_quant_cifar_fp32.eval()\n",
    "model_qat_quant_cifar_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet model\n",
    "phinet_fuse_modules(model_qat_quant_cifar_fp32_fused)\n",
    "\n",
    "print(f\"model_fp32:\\n {model_qat_quant_cifar_fp32}\\n\")\n",
    "print(f\"model_fp32_fused:\\n {model_qat_quant_cifar_fp32_fused}\\n\")\n",
    "\n",
    "# Insert stubs ion the model and in the forward pass\n",
    "model_qat_quant_cifar_int8 = PhiNetQuant(model_qat_quant_cifar_fp32_fused)\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "model_qat_quant_cifar_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare_qat(model_qat_quant_cifar_int8.train(), inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {model_qat_quant_cifar_int8}\\n\")\n",
    "\n",
    "# Quantization awere training and calibrate the model\n",
    "model_qat_quant_cifar_int8, train_time_model_qat_quant_cifar_int8 = train(model_qat_quant_cifar_int8, train_loader, device = \"cuda\", learning_rate=0.001, num_epochs=10, save_dir=\"qat_model\")\n",
    "\n",
    "model_qat_quant_cifar_int8.to(\"cpu\")\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "model_qat_quant_cifar_int8 = torch.ao.quantization.convert(model_qat_quant_cifar_int8, inplace=True)\n",
    "\n",
    "print(f\"model_int8:\\n {model_qat_quant_cifar_int8}\\n\")\n",
    "\n",
    "acc_model_qat_quant_cifar_int8, _ = test(model_qat_quant_cifar_int8, test_loader, \"cpu\")\n",
    "\n",
    "print(f\"Training Time: {train_time_model_qat_quant_cifar_int8/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_qat_quant_cifar_int8:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare results on **CIFAR10**:\n",
    "- Orginal model FP32;\n",
    "- Modified model FP32;\n",
    "- Static Quantization of the modified model INT8;\n",
    "- Quantization aware training of the modified model INT8;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32_cpu_inference_latency  = measure_inference_latency(model_cifar_fp32, device=\"cpu\", input_shape=(3, 32, 32))\n",
    "model_fp32_gpu_inference_latency  = measure_inference_latency(model_cifar_fp32, device=\"cuda\", input_shape=(3, 32, 32))\n",
    "\n",
    "model_quant_fp32_cpu_inference_latency  = measure_inference_latency(model_quant_cifar_fp32, device=\"cpu\", input_shape=(3, 32, 32))\n",
    "model_quant_fp32_gpu_inference_latency  = measure_inference_latency(model_quant_cifar_fp32, device=\"cuda\", input_shape=(3, 32, 32))\n",
    "\n",
    "model_st_quant_int8_cpu_inference_latency  = measure_inference_latency(model_st_quant_cifar_int8, device=\"cpu\", input_shape=(3, 32, 32))\n",
    "model_qat_quant_int8_cpu_inference_latency  = measure_inference_latency(model_qat_quant_cifar_int8, device=\"cpu\", input_shape=(3, 32, 32))\n",
    "\n",
    "print(\"Original FP32 model CPU Inference Latency: {:.3f} ms\".format(model_fp32_cpu_inference_latency[0]))\n",
    "print(\"Original FP32 model GPU Inference Latency: {:.3f} ms\".format(model_fp32_gpu_inference_latency[0]))\n",
    "print(\"Original FP32 model test accuracy: {:.2f}%\".format(acc_model_cifar_fp32))\n",
    "print(\"Original FP32 model training time: {:.3f} s\\n\".format(train_time_model_cifar_fp32/1000))\n",
    "\n",
    "print(\"Modified FP32 model CPU Inference Latency: {:.3f} ms\".format(model_quant_fp32_cpu_inference_latency[0]))\n",
    "print(\"Modified FP32 model GPU Inference Latency: {:.3f} ms\".format(model_quant_fp32_gpu_inference_latency[0]))\n",
    "print(\"Modified FP32 model test accuracy: {:.2f}%\".format(acc_model_quant_cifar_fp32))\n",
    "print(\"Modified FP32 model training time: {:.3f} s\\n\".format(train_time_model_quant_cifar_fp32/1000))\n",
    "\n",
    "print(\"Modified INT8 model static quant CPU Inference Latency: {:.3f} ms\".format(model_st_quant_int8_cpu_inference_latency[0]))\n",
    "print(\"Modified INT8 model static quant test accuracy: {:.2f}%\\n\".format(acc_model_st_quant_cifar_int8))\n",
    "\n",
    "print(\"Modified INT8 model QAT CPU Inference Latency: {:.3f} ms\".format(model_qat_quant_int8_cpu_inference_latency[0]))\n",
    "print(\"Modified INT8 model QAT test accuracy: {:.2f}%\".format(acc_model_qat_quant_cifar_int8))\n",
    "print(\"Modified INT8 model QAT training time: {:.3f} s\\n\".format(train_time_model_qat_quant_cifar_int8/1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
