{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook to demonstrate PhiNet's problems with quantization and the changes adopted to solve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import phinet_quant\n",
    "import micromind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    \"\"\"\n",
    "    Print the size of the model.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size_in_bytes = os.path.getsize(\"temp.p\")\n",
    "    if size_in_bytes < 1048576:\n",
    "        size_in_kb = size_in_bytes / 1024\n",
    "        print(\"{:.3f} KB\".format(size_in_kb))\n",
    "    else:\n",
    "        size_in_mb = size_in_bytes / 1048576\n",
    "        print(\"{:.3f} MB\".format(size_in_mb))\n",
    "    os.remove('temp.p')\n",
    "\n",
    "\n",
    "def measure_inference_latency(model, input_shape, device = None, repetitions=100, warmup_it = 10):\n",
    "    \"\"\"\n",
    "    Measures the inference time of the provided neural network model.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to evaluate.\n",
    "        input_shape: The shape of the input data expected by the model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the mean and standard deviation of the inference time\n",
    "               measured in milliseconds.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device.type  # Get the device where the model is located\n",
    "    \n",
    "    dummy_input = torch.randn(1, *input_shape, dtype=torch.float).to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # GPU warm-up\n",
    "    for _ in range(warmup_it):\n",
    "        _ = model(dummy_input)\n",
    "\n",
    "    # Measure inference time\n",
    "    timings = []\n",
    "    with torch.no_grad():\n",
    "        if device == 'cuda':\n",
    "            starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "            for rep in range(repetitions):\n",
    "                starter.record()\n",
    "                _ = model(dummy_input)\n",
    "                ender.record()\n",
    "                torch.cuda.synchronize()\n",
    "                curr_time = starter.elapsed_time(ender)\n",
    "                timings.append(curr_time)\n",
    "        else:  # CPU\n",
    "            for rep in range(repetitions):\n",
    "                start_time = time.time()\n",
    "                _ = model(dummy_input)\n",
    "                end_time = time.time()\n",
    "                elapsed_time = (end_time - start_time) * 1000.0  # Convert to milliseconds\n",
    "                timings.append(elapsed_time)\n",
    "\n",
    "    # Calculate mean and std\n",
    "    mean_time = np.mean(timings)\n",
    "    std_time = np.std(timings)\n",
    "\n",
    "    return mean_time, std_time\n",
    "\n",
    "\n",
    "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32), verbose=False):\n",
    "    \"\"\"\n",
    "    Tests whether two models are equivalent by comparing their outputs on random inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    model_1.to(device)\n",
    "    model_2.to(device)\n",
    "\n",
    "    for i in range(num_tests):\n",
    "        print(f\"Running test {i+1}/{num_tests}\") if verbose else None\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        y1 = model_1(x).detach().cpu().numpy()\n",
    "        y2 = model_2(x).detach().cpu().numpy()\n",
    "        print(\"Difference: \", np.max(np.abs(y1-y2))) if verbose else None\n",
    "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
    "            print(\"Model equivalence test sample failed: \")\n",
    "            print(y1)\n",
    "            print(y2)\n",
    "            return False\n",
    "    print(\"Model equivalence test passed!\")\n",
    "    return True\n",
    "\n",
    "def set_random_seeds(random_seed=0):\n",
    "    \"\"\"\n",
    "    Set all random seeds to a fixed value to make results reproducible.\n",
    "    \"\"\"\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization\n",
    "Original PhiNet definition not quantizable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): HSwish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): DepthwiseConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): HSwish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), groups=416, bias=False)\n",
      "        (6): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 401, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(401, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(401, 401, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=401, bias=False)\n",
      "        (5): BatchNorm2d(401, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(401, 66, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(66, 401, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(401, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 385, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(385, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(385, 385, kernel_size=(3, 3), stride=(2, 2), groups=385, bias=False)\n",
      "        (6): BatchNorm2d(385, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(385, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 385, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(385, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740, bias=False)\n",
      "        (5): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(740, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709, bias=False)\n",
      "        (6): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1357, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1357, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(1357, 1357, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1357, bias=False)\n",
      "        (5): BatchNorm2d(1357, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1357, 226, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(226, 1357, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(1357, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), groups=1296, bias=False)\n",
      "        (6): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(576, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = micromind.PhiNet(input_shape = [3, 224, 224], num_layers = 7, alpha = 3, beta = 0.75,t_zero = 6.0, include_top= True, num_classes = 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhiNet Convolutional Block 1\n",
    "First of two versions of PhiNet's convolutional blocks. In this case, the depth-wise convolutional block is defined as follows:\n",
    "\n",
    "```python\n",
    "DepthwiseConv2d(ch, ch, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1357, bias=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiNetConvBlock(\n",
      "  (_layers): ModuleList(\n",
      "    (0): Conv2d(144, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): HSwish()\n",
      "    (3): Dropout2d(p=0.05, inplace=False)\n",
      "    (4): DepthwiseConv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740, bias=False)\n",
      "    (5): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (6): HSwish()\n",
      "    (7): SEBlock(\n",
      "      (se_conv): Conv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (se_conv2): Conv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): HSwish()\n",
      "    )\n",
      "    (8): Conv2d(740, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "block1 = model._layers[6]\n",
    "print(block1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the original PhiNet convolutional block 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined Block\n",
      "PhiNetConv1(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740, bias=False)\n",
      "    (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(740, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Orginal BLock\n",
      "PhiNetConvBlock(\n",
      "  (_layers): ModuleList(\n",
      "    (0): Conv2d(144, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): HSwish()\n",
      "    (3): Dropout2d(p=0.05, inplace=False)\n",
      "    (4): DepthwiseConv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740, bias=False)\n",
      "    (5): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (6): HSwish()\n",
      "    (7): SEBlock(\n",
      "      (se_conv): Conv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (se_conv2): Conv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): HSwish()\n",
      "    )\n",
      "    (8): Conv2d(740, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PhiNetConv1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pointconv1 = nn.Sequential(\n",
    "                nn.Conv2d(144, 740, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(740, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "            )\n",
    "        self.drop = nn.Dropout2d(p=0.05)\n",
    "        self.depthwise = nn.Sequential(\n",
    "                nn.Conv2d(740,740,kernel_size=3,stride=1,padding=1,groups=740,bias=False),\n",
    "                nn.BatchNorm2d(740, eps=1e-3, momentum=0.999),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.se = torch.nn.Sequential(\n",
    "                nn.Conv2d(740, 123, kernel_size=1, stride=1, bias=False),\n",
    "                nn.Conv2d(123, 740, kernel_size=1, stride=1, bias=False),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.pointconv2 = nn.Sequential(\n",
    "                nn.Conv2d(740, 144, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(144, eps=1e-3, momentum=0.999),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pointconv1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pointconv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = PhiNetConv1()\n",
    "print(\"Defined Block\")\n",
    "print(test)\n",
    "print(\"Orginal BLock\")\n",
    "print(block1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization of the original convolutional block 1 of PhiNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1_fp32:\n",
      " PhiNetConv1(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740, bias=False)\n",
      "    (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(740, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Conv1_fp32_fused:\n",
      " PhiNetConv1(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 740, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740)\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(740, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " Sequential(\n",
      "  (0): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (1): PhiNetConv1(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): Conv2d(\n",
      "        144, 740, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2d(\n",
      "        740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): Conv2d(\n",
      "        740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Conv2d(\n",
      "        123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): Conv2d(\n",
      "        740, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantStub()\n",
      ")\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteotremonti/anaconda3/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1_int8: \n",
      " Sequential(\n",
      "  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (1): PhiNetConv1(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): QuantizedConv2d(144, 740, kernel_size=(1, 1), stride=(1, 1), scale=0.020500173792243004, zero_point=68)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): QuantizedConv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), scale=0.009569868445396423, zero_point=63, padding=(1, 1), groups=740)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): QuantizedConv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), scale=0.0015119541203603148, zero_point=60, bias=False)\n",
      "      (1): QuantizedConv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), scale=0.0008994026575237513, zero_point=62, bias=False)\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): QuantizedConv2d(740, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.00022830525995232165, zero_point=62)\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantize()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "Conv1_fp32 = PhiNetConv1().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "Conv1_fp32_fused = copy.deepcopy(Conv1_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "Conv1_fp32.eval()\n",
    "Conv1_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "Conv1_fp32_fused = torch.ao.quantization.fuse_modules(Conv1_fp32_fused, [['pointconv1.0', 'pointconv1.1'], ['depthwise.0', 'depthwise.1'], ['pointconv2.0', 'pointconv2.1']])\n",
    "\n",
    "print(f\"Conv1_fp32:\\n {Conv1_fp32}\\n\")\n",
    "print(f\"Conv1_fp32_fused:\\n {Conv1_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=Conv1_fp32, model_2=Conv1_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,144,28,28)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "Conv1_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  Conv1_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "Conv1_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(Conv1_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {Conv1_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,144, 28, 28)\n",
    "    Conv1_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "Conv1_int8 = torch.ao.quantization.convert(Conv1_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv1_int8: \\n {Conv1_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = Conv1_int8(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the size of FP32 and INT8 blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 size:\n",
      "1.563 MB\n",
      "INT8 size:\n",
      "445.729 KB\n"
     ]
    }
   ],
   "source": [
    "print(f\"FP32 size:\")\n",
    "print_size_of_model(Conv1_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(Conv1_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the inference time of FP32 and INT8 blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 2.485 ms\n",
      "INT8 CPU Inference Latency: 18.881 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv1_fp32, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv1_int8, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dimensions of both the FP32 and INT8 models, the INT8 is indeed more compact in size, as expected. However, contrary to what the documentation suggests, the INT8 model has slower inference times compared to the original FP32 model. The documentation indicated an expected reduction in inference time of about half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification of PhiNet original convolutional block 1\n",
    "\n",
    "One of the problems could be that the number of channels is not divisible by 8. Therefore, we modify the convolutional block by making the channels divisible by 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined Block\n",
      "PhiNetConv1(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744, bias=False)\n",
      "    (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(744, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(123, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Orginal BLock\n",
      "PhiNetConvBlock(\n",
      "  (_layers): ModuleList(\n",
      "    (0): Conv2d(144, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): HSwish()\n",
      "    (3): Dropout2d(p=0.05, inplace=False)\n",
      "    (4): DepthwiseConv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740, bias=False)\n",
      "    (5): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (6): HSwish()\n",
      "    (7): SEBlock(\n",
      "      (se_conv): Conv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (se_conv2): Conv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): HSwish()\n",
      "    )\n",
      "    (8): Conv2d(740, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PhiNetConv1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pointconv1 = nn.Sequential(\n",
    "                nn.Conv2d(144, 744, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(744, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "            )\n",
    "        self.drop = nn.Dropout2d(p=0.05)\n",
    "        self.depthwise = nn.Sequential(\n",
    "                nn.Conv2d(744,744,kernel_size=3,stride=1,padding=1,groups=744,bias=False),\n",
    "                nn.BatchNorm2d(744, eps=1e-3, momentum=0.999),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.se = torch.nn.Sequential(\n",
    "                nn.Conv2d(744, 123, kernel_size=1, stride=1, bias=False),\n",
    "                nn.Conv2d(123, 744, kernel_size=1, stride=1, bias=False),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.pointconv2 = nn.Sequential(\n",
    "                nn.Conv2d(744, 144, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(144, eps=1e-3, momentum=0.999),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pointconv1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pointconv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = PhiNetConv1()\n",
    "print(\"Defined Block\")\n",
    "print(test)\n",
    "print(\"Orginal BLock\")\n",
    "print(block1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1_fp32:\n",
      " PhiNetConv1(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744, bias=False)\n",
      "    (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(744, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(123, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Conv1_fp32_fused:\n",
      " PhiNetConv1(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744)\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(744, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(123, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " Sequential(\n",
      "  (0): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (1): PhiNetConv1(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): Conv2d(\n",
      "        144, 744, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2d(\n",
      "        744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): Conv2d(\n",
      "        744, 123, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Conv2d(\n",
      "        123, 744, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): Conv2d(\n",
      "        744, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantStub()\n",
      ")\n",
      "\n",
      "Conv1_int8: \n",
      " Sequential(\n",
      "  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (1): PhiNetConv1(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): QuantizedConv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), scale=0.02039351500570774, zero_point=69)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): QuantizedConv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), scale=0.00912468321621418, zero_point=52, padding=(1, 1), groups=744)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): QuantizedConv2d(744, 123, kernel_size=(1, 1), stride=(1, 1), scale=0.001644390751607716, zero_point=66, bias=False)\n",
      "      (1): QuantizedConv2d(123, 744, kernel_size=(1, 1), stride=(1, 1), scale=0.0008745527593418956, zero_point=62, bias=False)\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): QuantizedConv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.0002712075365707278, zero_point=61)\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantize()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "Conv1_fp32 = PhiNetConv1().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "Conv1_fp32_fused = copy.deepcopy(Conv1_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "Conv1_fp32.eval()\n",
    "Conv1_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "Conv1_fp32_fused = torch.ao.quantization.fuse_modules(Conv1_fp32_fused, [['pointconv1.0', 'pointconv1.1'], ['depthwise.0', 'depthwise.1'], ['pointconv2.0', 'pointconv2.1']])\n",
    "\n",
    "print(f\"Conv1_fp32:\\n {Conv1_fp32}\\n\")\n",
    "print(f\"Conv1_fp32_fused:\\n {Conv1_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=Conv1_fp32, model_2=Conv1_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,144,28,28)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "Conv1_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  Conv1_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "Conv1_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(Conv1_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {Conv1_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,144, 28, 28)\n",
    "    Conv1_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "Conv1_int8 = torch.ao.quantization.convert(Conv1_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv1_int8: \\n {Conv1_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = Conv1_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 size:\n",
      "1.571 MB\n",
      "INT8 size:\n",
      "447.729 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(Conv1_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(Conv1_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 2.424 ms\n",
      "INT8 CPU Inference Latency: 1.871 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv1_fp32, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv1_int8, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 324.768 ms\n",
      "INT8 CPU Inference Latency: 114.423 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv1_fp32, device =\"cpu\", input_shape=(144, 224, 224))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv1_int8, device =\"cpu\", input_shape=(144, 224, 224))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, after the number of channels has been made divisible by 8, the model in INT8 is smaller in terms of memory. In contrast to the original block, the inference time is also reduced, with the reduction increasing as the input resolution increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhiNet Convolutional Block 2\n",
    "\n",
    "Second versions of PhiNet's convolutional blocks. In this case, the depth-wise convolutional block is defined as follows:\n",
    "\n",
    "```python\n",
    "DepthwiseConv2d(ch, ch, kernel_size=(3, 3), stride=(2, 2), groups=ch, bias=False)\n",
    "```\n",
    "Diffrently from the PhiNet Convolutional Block 1, here we have ` stride=(2, 2)` and `padding = 0`. Moreover, before the `Dropout2d(p=0.05)` it is inserted the following layer:\n",
    "\n",
    "```python \n",
    "ZeroPad2d((1, 1, 1, 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiNetConvBlock(\n",
      "  (_layers): ModuleList(\n",
      "    (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): HSwish()\n",
      "    (3): ZeroPad2d((1, 1, 1, 1))\n",
      "    (4): Dropout2d(p=0.05, inplace=False)\n",
      "    (5): DepthwiseConv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709, bias=False)\n",
      "    (6): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (7): HSwish()\n",
      "    (8): SEBlock(\n",
      "      (se_conv): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (se_conv2): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): HSwish()\n",
      "    )\n",
      "    (9): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "block2 = model._layers[7]\n",
    "print(block2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the original PhiNet convolutional block 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined Block\n",
      "PhiNetConv2(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709, bias=False)\n",
      "    (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Orginal BLock\n",
      "PhiNetConvBlock(\n",
      "  (_layers): ModuleList(\n",
      "    (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): HSwish()\n",
      "    (3): ZeroPad2d((1, 1, 1, 1))\n",
      "    (4): Dropout2d(p=0.05, inplace=False)\n",
      "    (5): DepthwiseConv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709, bias=False)\n",
      "    (6): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (7): HSwish()\n",
      "    (8): SEBlock(\n",
      "      (se_conv): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (se_conv2): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): HSwish()\n",
      "    )\n",
      "    (9): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PhiNetConv2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pointconv1 = nn.Sequential(\n",
    "                nn.Conv2d(144, 709, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(709, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "            )\n",
    "        self.pad = nn.ZeroPad2d(1)\n",
    "        self.drop = nn.Dropout2d(p=0.05)\n",
    "        self.depthwise = nn.Sequential(\n",
    "                nn.Conv2d(709,709,kernel_size=3,stride=2,padding=0,groups=709,bias=False),\n",
    "                nn.BatchNorm2d(709, eps=1e-3, momentum=0.999),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.se = torch.nn.Sequential(\n",
    "                nn.Conv2d(709, 118, kernel_size=1, stride=1, bias=False),\n",
    "                nn.Conv2d(118, 709, kernel_size=1, stride=1, bias=False),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.pointconv2 = nn.Sequential(\n",
    "                nn.Conv2d(709, 288, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(288, eps=1e-3, momentum=0.999),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pointconv1(x)\n",
    "        x = self.pad(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pointconv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = PhiNetConv2()\n",
    "print(\"Defined Block\")\n",
    "print(test)\n",
    "print(\"Orginal BLock\")\n",
    "print(block2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization of the original convolutional block 2 of PhiNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2_fp32:\n",
      " PhiNetConv2(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709, bias=False)\n",
      "    (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Conv2_fp32_fused:\n",
      " PhiNetConv2(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709)\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " Sequential(\n",
      "  (0): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (1): PhiNetConv2(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): Conv2d(\n",
      "        144, 709, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2d(\n",
      "        709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): Conv2d(\n",
      "        709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Conv2d(\n",
      "        118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): Conv2d(\n",
      "        709, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantStub()\n",
      ")\n",
      "\n",
      "Conv2_int8: \n",
      " Sequential(\n",
      "  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (1): PhiNetConv2(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): QuantizedConv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), scale=0.020521609112620354, zero_point=68)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): QuantizedConv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), scale=0.011568625457584858, zero_point=83, groups=709)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): QuantizedConv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), scale=0.0015906733460724354, zero_point=64, bias=False)\n",
      "      (1): QuantizedConv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), scale=0.001034931861795485, zero_point=65, bias=False)\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): QuantizedConv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.0002751421998254955, zero_point=66)\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantize()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "Conv2_fp32 = PhiNetConv2().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "Conv2_fp32_fused = copy.deepcopy(Conv2_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "Conv2_fp32.eval()\n",
    "Conv2_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "Conv2_fp32_fused = torch.ao.quantization.fuse_modules(Conv2_fp32_fused, [['pointconv1.0', 'pointconv1.1'], ['depthwise.0', 'depthwise.1'], ['pointconv2.0', 'pointconv2.1']])\n",
    "\n",
    "print(f\"Conv2_fp32:\\n {Conv2_fp32}\\n\")\n",
    "print(f\"Conv2_fp32_fused:\\n {Conv2_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=Conv2_fp32, model_2=Conv2_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,144,28,28)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "Conv2_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  Conv2_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "Conv2_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(Conv2_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {Conv2_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,144, 28, 28)\n",
    "    Conv2_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "Conv2_int8 = torch.ao.quantization.convert(Conv2_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv2_int8: \\n {Conv2_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = Conv2_int8(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the size of FP32 and INT8 blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 size:\n",
      "1.862 MB\n",
      "INT8 size:\n",
      "523.104 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(Conv2_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(Conv2_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the inference time of FP32 and INT8 blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 1.963 ms\n",
      "INT8 CPU Inference Latency: 5.173 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv2_fp32, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv2_int8, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dimensions of both the FP32 and INT8 models, the INT8 is indeed more compact in size, as expected. However, contrary to what the documentation suggests, the INT8 model has slower inference times compared to the original FP32 model. The documentation indicated an expected reduction in inference time of about half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification of PhiNet original convolutional block 2\n",
    "\n",
    "One of the problems could be that the number of channels is not divisible by 8. Therefore, we modify the convolutional block by making the channels divisible by 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined Block\n",
      "PhiNetConv2(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), groups=712, bias=False)\n",
      "    (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Orginal BLock\n",
      "PhiNetConvBlock(\n",
      "  (_layers): ModuleList(\n",
      "    (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): HSwish()\n",
      "    (3): ZeroPad2d((1, 1, 1, 1))\n",
      "    (4): Dropout2d(p=0.05, inplace=False)\n",
      "    (5): DepthwiseConv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709, bias=False)\n",
      "    (6): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (7): HSwish()\n",
      "    (8): SEBlock(\n",
      "      (se_conv): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (se_conv2): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): HSwish()\n",
      "    )\n",
      "    (9): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PhiNetConv2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pointconv1 = nn.Sequential(\n",
    "                nn.Conv2d(144, 712, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(712, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "            )\n",
    "        self.pad = nn.ZeroPad2d(1)\n",
    "        self.drop = nn.Dropout2d(p=0.05)\n",
    "        self.depthwise = nn.Sequential(\n",
    "                nn.Conv2d(712,712,kernel_size=3,stride=2,padding=0,groups=712,bias=False),\n",
    "                nn.BatchNorm2d(712, eps=1e-3, momentum=0.999),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.se = torch.nn.Sequential(\n",
    "                nn.Conv2d(712, 118, kernel_size=1, stride=1, bias=False),\n",
    "                nn.Conv2d(118, 712, kernel_size=1, stride=1, bias=False),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.pointconv2 = nn.Sequential(\n",
    "                nn.Conv2d(712, 288, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(288, eps=1e-3, momentum=0.999),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pointconv1(x)\n",
    "        x = self.pad(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pointconv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = PhiNetConv2()\n",
    "print(\"Defined Block\")\n",
    "print(test)\n",
    "print(\"Orginal BLock\")\n",
    "print(block2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2_fp32:\n",
      " PhiNetConv2(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), groups=712, bias=False)\n",
      "    (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Conv2_fp32_fused:\n",
      " PhiNetConv2(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), groups=712)\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " Sequential(\n",
      "  (0): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (1): PhiNetConv2(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): Conv2d(\n",
      "        144, 712, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2d(\n",
      "        712, 712, kernel_size=(3, 3), stride=(2, 2), groups=712\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): Conv2d(\n",
      "        712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Conv2d(\n",
      "        118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): Conv2d(\n",
      "        712, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantStub()\n",
      ")\n",
      "\n",
      "Conv2_int8: \n",
      " Sequential(\n",
      "  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (1): PhiNetConv2(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): QuantizedConv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), scale=0.02047821506857872, zero_point=68)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (pad): ZeroPad2d((1, 1, 1, 1))\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): QuantizedConv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), scale=0.009935283102095127, zero_point=68, groups=712)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): QuantizedConv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), scale=0.0016505611129105091, zero_point=65, bias=False)\n",
      "      (1): QuantizedConv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), scale=0.0009333100751973689, zero_point=61, bias=False)\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): QuantizedConv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.0002705159713514149, zero_point=62)\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantize()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "Conv2_fp32 = PhiNetConv2().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "Conv2_fp32_fused = copy.deepcopy(Conv2_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "Conv2_fp32.eval()\n",
    "Conv2_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "Conv2_fp32_fused = torch.ao.quantization.fuse_modules(Conv2_fp32_fused, [['pointconv1.0', 'pointconv1.1'], ['depthwise.0', 'depthwise.1'], ['pointconv2.0', 'pointconv2.1']])\n",
    "\n",
    "print(f\"Conv2_fp32:\\n {Conv2_fp32}\\n\")\n",
    "print(f\"Conv2_fp32_fused:\\n {Conv2_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=Conv2_fp32, model_2=Conv2_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,144,28,28)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "Conv2_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  Conv2_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "Conv2_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(Conv2_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {Conv2_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,144, 28, 28)\n",
    "    Conv2_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "Conv2_int8 = torch.ao.quantization.convert(Conv2_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv2_int8: \\n {Conv2_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = Conv2_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 size:\n",
      "1.870 MB\n",
      "INT8 size:\n",
      "524.979 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(Conv2_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(Conv2_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 1.940 ms\n",
      "INT8 CPU Inference Latency: 5.491 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv2_fp32, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv2_int8, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the original convolutional block 1 of PhiNet. Making the channels divisible by 8 is not enough. In fact, the model in INT8 is inferior in terms of memomics but slower in terms of inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification of PhiNet original convolutional block 2 (V2)\n",
    "The number of channels of the convolutional blocks is made divisible by 8 and `nn.ZeroPad2d(1)` is removed and placed inside the Depthwise Convolution using `Conv2d(padding = 1)`. The depth-wise convolution becomes:\n",
    "\n",
    "```python\n",
    "Conv2d(ch, ch, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=ch, bias=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined Block\n",
      "PhiNetConv2(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712, bias=False)\n",
      "    (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Orginal BLock\n",
      "PhiNetConvBlock(\n",
      "  (_layers): ModuleList(\n",
      "    (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): HSwish()\n",
      "    (3): ZeroPad2d((1, 1, 1, 1))\n",
      "    (4): Dropout2d(p=0.05, inplace=False)\n",
      "    (5): DepthwiseConv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709, bias=False)\n",
      "    (6): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (7): HSwish()\n",
      "    (8): SEBlock(\n",
      "      (se_conv): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (se_conv2): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): HSwish()\n",
      "    )\n",
      "    (9): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define PhiNet Convolutional Block without number of channel divisible by 8\n",
    "class PhiNetConv2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized\n",
    "        self.pointconv1 = nn.Sequential(\n",
    "                nn.Conv2d(144, 712, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(712, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "            )\n",
    "        self.drop = nn.Dropout2d(p=0.05)\n",
    "        self.depthwise = nn.Sequential(\n",
    "                nn.Conv2d(712,712,kernel_size=3,stride=2,padding=1,groups=712,bias=False),\n",
    "                nn.BatchNorm2d(712, eps=1e-3, momentum=0.999),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.se = torch.nn.Sequential(\n",
    "                nn.Conv2d(712, 118, kernel_size=1, stride=1, bias=False),\n",
    "                nn.Conv2d(118, 712, kernel_size=1, stride=1, bias=False),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.pointconv2 = nn.Sequential(\n",
    "                nn.Conv2d(712, 288, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(288, eps=1e-3, momentum=0.999),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pointconv1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pointconv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = PhiNetConv2()\n",
    "print(\"Defined Block\")\n",
    "print(test)\n",
    "print(\"Orginal BLock\")\n",
    "print(block2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2_fp32:\n",
      " PhiNetConv2(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712, bias=False)\n",
      "    (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Conv2_fp32_fused:\n",
      " PhiNetConv2(\n",
      "  (pointconv1): Sequential(\n",
      "    (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (drop): Dropout2d(p=0.05, inplace=False)\n",
      "  (depthwise): Sequential(\n",
      "    (0): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712)\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (se): Sequential(\n",
      "    (0): Conv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (pointconv2): Sequential(\n",
      "    (0): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " Sequential(\n",
      "  (0): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (1): PhiNetConv2(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): Conv2d(\n",
      "        144, 712, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2d(\n",
      "        712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): Conv2d(\n",
      "        712, 118, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Conv2d(\n",
      "        118, 712, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): Conv2d(\n",
      "        712, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantStub()\n",
      ")\n",
      "\n",
      "Conv2_int8: \n",
      " Sequential(\n",
      "  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (1): PhiNetConv2(\n",
      "    (pointconv1): Sequential(\n",
      "      (0): QuantizedConv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), scale=0.02047821506857872, zero_point=68)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (drop): Dropout2d(p=0.05, inplace=False)\n",
      "    (depthwise): Sequential(\n",
      "      (0): QuantizedConv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), scale=0.009935283102095127, zero_point=68, padding=(1, 1), groups=712)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (se): Sequential(\n",
      "      (0): QuantizedConv2d(712, 118, kernel_size=(1, 1), stride=(1, 1), scale=0.0016505611129105091, zero_point=65, bias=False)\n",
      "      (1): QuantizedConv2d(118, 712, kernel_size=(1, 1), stride=(1, 1), scale=0.0009333100751973689, zero_point=61, bias=False)\n",
      "      (2): QuantizedHardswish()\n",
      "    )\n",
      "    (pointconv2): Sequential(\n",
      "      (0): QuantizedConv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.0002705159713514149, zero_point=62)\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantize()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "Conv2_fp32 = PhiNetConv2().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "Conv2_fp32_fused = copy.deepcopy(Conv2_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "Conv2_fp32.eval()\n",
    "Conv2_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "Conv2_fp32_fused = torch.ao.quantization.fuse_modules(Conv2_fp32_fused, [['pointconv1.0', 'pointconv1.1'], ['depthwise.0', 'depthwise.1'], ['pointconv2.0', 'pointconv2.1']])\n",
    "\n",
    "print(f\"Conv2_fp32:\\n {Conv2_fp32}\\n\")\n",
    "print(f\"Conv2_fp32_fused:\\n {Conv2_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=Conv2_fp32, model_2=Conv2_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,144,28,28)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "Conv2_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  Conv2_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "Conv2_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(Conv2_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {Conv2_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,144, 28, 28)\n",
    "    Conv2_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "Conv2_int8 = torch.ao.quantization.convert(Conv2_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv2_int8: \\n {Conv2_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = Conv2_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 size:\n",
      "1.571 MB\n",
      "INT8 size:\n",
      "447.729 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(Conv1_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(Conv1_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 1.651 ms\n",
      "INT8 CPU Inference Latency: 1.069 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv2_fp32, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv2_int8, device =\"cpu\", input_shape=(144, 28, 28))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 158.514 ms\n",
      "INT8 CPU Inference Latency: 53.861 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv2_fp32, device =\"cpu\", input_shape=(144, 224, 224))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv2_int8, device =\"cpu\", input_shape=(144, 224, 224))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, after making the number of channels divisible by 8 and removing `ZeroPad2d`, the model in INT8 is smaller in terms of memory and faster in terms of inference time, with the reduction increasing as the input resolution increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhiNet Convolutional Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the `layer` variable, I check whether the changes are effective on all PhiNet blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined Block\n",
      "ConvBlock(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "    (3): Dropout2d(p=0.05, inplace=False)\n",
      "    (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416, bias=False)\n",
      "    (5): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (6): Hardswish()\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Orginal Block\n",
      "PhiNetConvBlock(\n",
      "  (_layers): ModuleList(\n",
      "    (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): HSwish()\n",
      "    (3): ZeroPad2d((1, 1, 1, 1))\n",
      "    (4): Dropout2d(p=0.05, inplace=False)\n",
      "    (5): DepthwiseConv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), groups=416, bias=False)\n",
      "    (6): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (7): HSwish()\n",
      "    (8): SEBlock(\n",
      "      (se_conv): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (se_conv2): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): HSwish()\n",
      "    )\n",
      "    (9): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "layer = 3\n",
    "block = model._layers[layer]\n",
    "resolution = [112, 56, 56, 28, 28, 14, 14, 7]\n",
    "res = resolution[layer-3]\n",
    "\n",
    "input = block._layers[0].in_channels\n",
    "channel = _make_divisible(block._layers[0].out_channels, 8)\n",
    "stride = block._layers[-6].stride\n",
    "if isinstance(block._layers[3], nn.ZeroPad2d):\n",
    "    padding = 1\n",
    "else:\n",
    "    padding = block._layers[-6].padding\n",
    "se_ch = block._layers[-3].se_conv.out_channels\n",
    "output = block._layers[-2].out_channels\n",
    "\n",
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "                nn.Conv2d(input, channel, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(channel, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "                nn.Dropout2d(p=0.05),\n",
    "                nn.Conv2d(channel,channel,kernel_size=3,stride=stride,padding=padding,groups=channel,bias=False),\n",
    "                nn.BatchNorm2d(channel, eps=1e-3, momentum=0.999),\n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "                torch.nn.Sequential(\n",
    "                    nn.Conv2d(channel, se_ch, kernel_size=1, stride=1, bias=False),\n",
    "                    nn.Conv2d(se_ch, channel, kernel_size=1, stride=1, bias=False),\n",
    "                    torch.nn.Hardswish(inplace=True),\n",
    "                ),\n",
    "                nn.Conv2d(channel, output, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(output, eps=1e-3, momentum=0.999),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = ConvBlock()\n",
    "print(\"Defined Block\")\n",
    "print(test)\n",
    "print(\"Orginal Block\")\n",
    "print(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2_fp32:\n",
      " ConvBlock(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "    (3): Dropout2d(p=0.05, inplace=False)\n",
      "    (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416, bias=False)\n",
      "    (5): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (6): Hardswish()\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Conv2_fp32_fused:\n",
      " ConvBlock(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "    (2): Hardswish()\n",
      "    (3): Dropout2d(p=0.05, inplace=False)\n",
      "    (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416)\n",
      "    (5): Identity()\n",
      "    (6): Hardswish()\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (9): Identity()\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " Sequential(\n",
      "  (0): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (1): ConvBlock(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(\n",
      "        72, 416, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (3): Dropout2d(p=0.05, inplace=False)\n",
      "      (4): Conv2d(\n",
      "        416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (5): Identity()\n",
      "      (6): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): Conv2d(\n",
      "          416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "        (1): Conv2d(\n",
      "          69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "        (2): Hardswish(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (8): Conv2d(\n",
      "        416, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (9): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantStub()\n",
      ")\n",
      "\n",
      "Conv2_int8: \n",
      " Sequential(\n",
      "  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (1): ConvBlock(\n",
      "    (conv): Sequential(\n",
      "      (0): QuantizedConv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), scale=0.01926155760884285, zero_point=65)\n",
      "      (1): Identity()\n",
      "      (2): QuantizedHardswish()\n",
      "      (3): Dropout2d(p=0.05, inplace=False)\n",
      "      (4): QuantizedConv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), scale=0.009451590478420258, zero_point=73, padding=(1, 1), groups=416)\n",
      "      (5): Identity()\n",
      "      (6): QuantizedHardswish()\n",
      "      (7): Sequential(\n",
      "        (0): QuantizedConv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), scale=0.001384903909638524, zero_point=58, bias=False)\n",
      "        (1): QuantizedConv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), scale=0.0009201678331010044, zero_point=63, bias=False)\n",
      "        (2): QuantizedHardswish()\n",
      "      )\n",
      "      (8): QuantizedConv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.000275766768027097, zero_point=66)\n",
      "      (9): Identity()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantize()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "Conv_fp32 = ConvBlock().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "Conv_fp32_fused = copy.deepcopy(Conv_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "Conv_fp32.eval()\n",
    "Conv_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "Conv_fp32_fused = torch.ao.quantization.fuse_modules(Conv_fp32_fused, [['conv.0', 'conv.1'], ['conv.4', 'conv.5'], ['conv.8', 'conv.9']])\n",
    "\n",
    "print(f\"Conv2_fp32:\\n {Conv_fp32}\\n\")\n",
    "print(f\"Conv2_fp32_fused:\\n {Conv_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=Conv_fp32, model_2=Conv_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,input,res,res)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "Conv_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  Conv_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "Conv_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(Conv_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {Conv_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,input, res, res)\n",
    "    Conv_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "Conv_int8 = torch.ao.quantization.convert(Conv_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv2_int8: \\n {Conv_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "result = Conv_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 size:\n",
      "492.038 KB\n",
      "INT8 size:\n",
      "151.104 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(Conv_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(Conv_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 14.512 ms\n",
      "INT8 CPU Inference Latency: 4.516 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(Conv_fp32, device =\"cpu\", input_shape=(input,res,res))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(Conv_int8, device =\"cpu\", input_shape=(input,res,res))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SeparableConvolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also check the Separable Convolutional BLock in the PhiNet input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputBlock(\n",
      "  (pad): ZeroPad2d((0, 1, 0, 1))\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "    (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (3): Hardswish()\n",
      "  )\n",
      ")\n",
      "SeparableConv2d(\n",
      "  (_layers): ModuleList(\n",
      "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "    (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (3): HSwish()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class InputBlock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pad = nn.ZeroPad2d((0,1,0,1))\n",
    "        self.conv = nn.Sequential(\n",
    "                nn.Conv2d(3, 3, kernel_size=3, stride=2, padding=0, groups=3, bias=False),\n",
    "                nn.Conv2d(3, 144, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(144, eps=1e-3, momentum=0.999),    \n",
    "                torch.nn.Hardswish(inplace=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pad(x)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "test = InputBlock()\n",
    "print(test)\n",
    "\n",
    "block = model._layers[1]\n",
    "print(block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2_fp32:\n",
      " InputBlock(\n",
      "  (pad): ZeroPad2d((0, 1, 0, 1))\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "    (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "    (3): Hardswish()\n",
      "  )\n",
      ")\n",
      "\n",
      "Conv2_fp32_fused:\n",
      " InputBlock(\n",
      "  (pad): ZeroPad2d((0, 1, 0, 1))\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "    (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): Identity()\n",
      "    (3): Hardswish()\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " Sequential(\n",
      "  (0): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (1): InputBlock(\n",
      "    (pad): ZeroPad2d((0, 1, 0, 1))\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(\n",
      "        3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Conv2d(\n",
      "        3, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (2): Identity()\n",
      "      (3): Hardswish(\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantStub()\n",
      ")\n",
      "\n",
      "Conv2_int8: \n",
      " Sequential(\n",
      "  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (1): InputBlock(\n",
      "    (pad): ZeroPad2d((0, 1, 0, 1))\n",
      "    (conv): Sequential(\n",
      "      (0): QuantizedConv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), scale=0.011397001333534718, zero_point=59, groups=3, bias=False)\n",
      "      (1): QuantizedConv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.00975711364299059, zero_point=63)\n",
      "      (2): Identity()\n",
      "      (3): QuantizedHardswish()\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantize()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "# Create the model to be quantized\n",
    "InConv_fp32 = InputBlock().to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "InConv_fp32_fused = copy.deepcopy(InConv_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "InConv_fp32.eval()\n",
    "InConv_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet Conv1 block\n",
    "InConv_fp32_fused = torch.ao.quantization.fuse_modules(InConv_fp32_fused, [['conv.1', 'conv.2']])\n",
    "\n",
    "print(f\"Conv2_fp32:\\n {InConv_fp32}\\n\")\n",
    "print(f\"Conv2_fp32_fused:\\n {InConv_fp32_fused}\\n\")\n",
    "\n",
    "# Check that the fused model is equivalent to the original model\n",
    "assert model_equivalence(model_1=InConv_fp32, model_2=InConv_fp32_fused, device=\"cpu\", rtol=5e-03, atol=5e-06, num_tests=100, input_size=(1,3,224,224)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "InConv_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  InConv_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "InConv_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(InConv_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {InConv_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,3, 224, 224)\n",
    "    InConv_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "InConv_int8 = torch.ao.quantization.convert(InConv_int8, inplace=True)\n",
    "\n",
    "print(f\"Conv2_int8: \\n {InConv_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "result = InConv_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 size:\n",
      "6.117 KB\n",
      "INT8 size:\n",
      "7.079 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(InConv_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(InConv_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 2.351 ms\n",
      "INT8 CPU Inference Latency: 2.299 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(InConv_fp32, device =\"cpu\", input_shape=(3,224,224))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(InConv_int8, device =\"cpu\", input_shape=(3,224,224))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the FP32 block, the INT8 block is larger in terms of B. This could be due to the addition of the two fake quantization blocks. In addition, the slightly worse inference time is due to the fact that the channels are not divisible by 8 in depth convolution. Although the performance in general is slightly worse, in the PhiNet complex, it does not affect the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete PhiNet Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phinet_quant import PhiNetConvBlock, SeparableConv2d, DepthwiseConv2d\n",
    "\n",
    "def phinet_fuse_modules(model):\n",
    "    for basic_block_name, basic_block in model._layers.named_children():\n",
    "        if isinstance(basic_block, SeparableConv2d):\n",
    "            torch.ao.quantization.fuse_modules(basic_block, [[\"_layers.1\", \"_layers.2\"]], inplace=True)\n",
    "        if isinstance(basic_block, PhiNetConvBlock) and len(basic_block._layers) == 6:\n",
    "            torch.ao.quantization.fuse_modules(basic_block, [[\"_layers.1\", \"_layers.2\"], [\"_layers.4\", \"_layers.5\"]], inplace=True)\n",
    "        elif isinstance(basic_block, PhiNetConvBlock):\n",
    "            torch.ao.quantization.fuse_modules(basic_block, [[\"_layers.0\", \"_layers.1\"], [\"_layers.4\", \"_layers.5\"], [\"_layers.8\", \"_layers.9\"]], inplace=True)\n",
    "\n",
    "def remove_depthwise(model):\n",
    "    def convert_to_conv2d(depthwise_conv2d):\n",
    "        in_channels = depthwise_conv2d.in_channels\n",
    "        depth_multiplier = depthwise_conv2d.out_channels // in_channels\n",
    "        kernel_size = depthwise_conv2d.kernel_size\n",
    "        stride = depthwise_conv2d.stride\n",
    "        padding = depthwise_conv2d.padding\n",
    "        dilation = depthwise_conv2d.dilation\n",
    "        bias = depthwise_conv2d.bias is not None\n",
    "        padding_mode = depthwise_conv2d.padding_mode\n",
    "\n",
    "        # Create an equivalent nn.Conv2d layer\n",
    "        conv2d_layer = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=in_channels * depth_multiplier,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=in_channels,  # Set groups to in_channels for depthwise convolution\n",
    "            bias=bias,\n",
    "            padding_mode=padding_mode,\n",
    "        )\n",
    "\n",
    "        # If bias was not used in the original depthwise_conv2d, set bias to None in conv2d_layer\n",
    "        if not bias:\n",
    "            conv2d_layer.bias = None\n",
    "\n",
    "        return conv2d_layer\n",
    "\n",
    "    for name, module in model._layers.named_children():\n",
    "        if isinstance(module, PhiNetConvBlock):\n",
    "            for i, layer in enumerate(module._layers.children()):\n",
    "                if isinstance(layer, DepthwiseConv2d):\n",
    "                    module._layers[i] = convert_to_conv2d(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of PhiNet with the modifications shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal Model: PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): DepthwiseConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416, bias=False)\n",
      "        (5): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400, bias=False)\n",
      "        (5): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "        (5): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744, bias=False)\n",
      "        (5): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(128, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712, bias=False)\n",
      "        (5): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(120, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)\n",
      "        (5): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(224, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296, bias=False)\n",
      "        (5): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(576, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Model without Depthwise: PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416, bias=False)\n",
      "        (5): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400, bias=False)\n",
      "        (5): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "        (5): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744, bias=False)\n",
      "        (5): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(128, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712, bias=False)\n",
      "        (5): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(120, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)\n",
      "        (5): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(224, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296, bias=False)\n",
      "        (5): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(576, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(0)\n",
    "\n",
    "# Import the quantizeble model\n",
    "model_fp32 = phinet_quant.PhiNet(input_shape = [3, 224, 224], num_layers=7, alpha= 3.0, beta= 0.75,t_zero= 6.0, include_top= True, num_classes=10, divisor=8)\n",
    "print(f\"Orginal Model: {model_fp32}\\n\")\n",
    "remove_depthwise(model_fp32)\n",
    "print(f\"Model without Depthwise: {model_fp32}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fp32:\n",
      " PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416, bias=False)\n",
      "        (5): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400, bias=False)\n",
      "        (5): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "        (5): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744, bias=False)\n",
      "        (5): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(128, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712, bias=False)\n",
      "        (5): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(120, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)\n",
      "        (5): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(224, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296, bias=False)\n",
      "        (5): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(576, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "model_fp32_fused:\n",
      " PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Identity()\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)\n",
      "        (2): Identity()\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (5): Identity()\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(128, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(120, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(224, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " Sequential(\n",
      "  (0): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (1): PhiNet(\n",
      "    (_layers): ModuleList(\n",
      "      (0): ZeroPad2d((0, 1, 0, 1))\n",
      "      (1): SeparableConv2d(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Conv2d(\n",
      "            3, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (2): Identity()\n",
      "          (3): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Dropout2d(p=0.05, inplace=False)\n",
      "          (1): Conv2d(\n",
      "            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (2): Identity()\n",
      "          (3): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (4): Conv2d(\n",
      "            144, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            72, 416, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            416, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            72, 400, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              400, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              64, 400, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            400, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (5): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            72, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            384, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            144, 744, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              744, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              128, 744, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            744, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (7): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            144, 712, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              712, 120, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              120, 712, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            712, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            288, 1360, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              1360, 224, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              224, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            1360, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (9): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            288, 1296, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            1296, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): Linear(\n",
      "        in_features=576, out_features=10, bias=True\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantStub()\n",
      ")\n",
      "\n",
      "model_int8:\n",
      " Sequential(\n",
      "  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (1): PhiNet(\n",
      "    (_layers): ModuleList(\n",
      "      (0): ZeroPad2d((0, 1, 0, 1))\n",
      "      (1): SeparableConv2d(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), scale=0.011379190720617771, zero_point=59, groups=3, bias=False)\n",
      "          (1): QuantizedConv2d(3, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.00973123125731945, zero_point=63)\n",
      "          (2): Identity()\n",
      "          (3): QuantizedHardswish()\n",
      "        )\n",
      "      )\n",
      "      (2): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Dropout2d(p=0.05, inplace=False)\n",
      "          (1): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.0030095383990556, zero_point=69, padding=(1, 1), groups=144)\n",
      "          (2): Identity()\n",
      "          (3): QuantizedHardswish()\n",
      "          (4): QuantizedConv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.0007092884625308216, zero_point=67)\n",
      "          (5): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), scale=0.000491232902277261, zero_point=60)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), scale=0.00020571867935359478, zero_point=57, padding=(1, 1), groups=416)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), scale=1.7227965145139024e-05, zero_point=59, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), scale=6.447161467804108e-06, zero_point=62, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=5.1495193474693224e-05, zero_point=57\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), scale=1.9136259652441368e-05, zero_point=65)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), scale=1.1468764569144696e-05, zero_point=65)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), scale=4.657508725358639e-06, zero_point=67, padding=(1, 1), groups=400)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(400, 64, kernel_size=(1, 1), stride=(1, 1), scale=4.860241347159899e-07, zero_point=63, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(64, 400, kernel_size=(1, 1), stride=(1, 1), scale=1.6266623958927084e-07, zero_point=57, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=1.1643547850326286e-06, zero_point=67\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), scale=4.6508134232681186e-07, zero_point=67)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=1.9246745068812743e-05, zero_point=65\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), scale=1.1760723282350227e-05, zero_point=69)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), scale=6.875463895994471e-06, zero_point=51, padding=(1, 1), groups=384)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=6.394291744982183e-07, zero_point=51, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=1.6890734855223855e-07, zero_point=63, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=1.7128385252362932e-06, zero_point=51\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), scale=4.98038275509316e-07, zero_point=67)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), scale=2.9413408242362493e-07, zero_point=65)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), scale=1.355235923483633e-07, zero_point=58, padding=(1, 1), groups=744)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(744, 128, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=8, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(128, 744, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=3, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=1.1920928955078125e-07, zero_point=17\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=7)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=4.993797233510122e-07, zero_point=67\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (7): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), scale=3.0273736228991766e-07, zero_point=63)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), scale=1.417962067762346e-07, zero_point=46, padding=(1, 1), groups=712)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(712, 120, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=6, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(120, 712, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=3, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=1.1920928955078125e-07, zero_point=14\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=7)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=4)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=2, padding=(1, 1), groups=1360)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(1360, 224, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=0, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(224, 1360, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=0, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=1.1920928955078125e-07, zero_point=0\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=0)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=1.1920928955078125e-07, zero_point=7\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (9): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=4)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), scale=1.1920928955078125e-07, zero_point=2, padding=(1, 1), groups=1296)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=0, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=0, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=1.1920928955078125e-07, zero_point=0\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), scale=1.1920928955078125e-07, zero_point=0)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): QuantizedLinear(in_features=576, out_features=10, scale=0.0006252946914173663, zero_point=61, qscheme=torch.per_channel_affine)\n",
      "    )\n",
      "  )\n",
      "  (2): DeQuantize()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=0)\n",
    "\n",
    "model_fp32.to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "model_fp32_fused = copy.deepcopy(model_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "model_fp32.eval()\n",
    "model_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet model\n",
    "phinet_fuse_modules(model_fp32_fused)\n",
    "\n",
    "print(f\"model_fp32:\\n {model_fp32}\\n\")\n",
    "print(f\"model_fp32_fused:\\n {model_fp32_fused}\\n\")\n",
    "\n",
    "assert model_equivalence(model_1=model_fp32, model_2=model_fp32_fused, device=\"cpu\", rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,224,224)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "model_int8 = nn.Sequential(torch.ao.quantization.QuantStub(), \n",
    "                  model_fp32_fused, \n",
    "                  torch.ao.quantization.DeQuantStub())\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "model_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(model_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {model_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "with torch.inference_mode():\n",
    "  for _ in range(20):\n",
    "    x = torch.rand(1,3, 224, 224)\n",
    "    model_int8(x)\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "model_int8 = torch.ao.quantization.convert(model_int8, inplace=True)\n",
    "\n",
    "print(f\"model_int8:\\n {model_int8}\\n\")\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "result = model_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 size:\n",
      "16.952 MB\n",
      "INT8 size:\n",
      "4.580 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(model_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(model_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 CPU Inference Latency: 29.540 ms\n",
      "FP32 GPU Inference Latency: 3.490 ms\n",
      "INT8 CPU Inference Latency: 14.413 ms\n"
     ]
    }
   ],
   "source": [
    "fp32_cpu_inference_latency  = measure_inference_latency(model_fp32, device =\"cpu\", input_shape=(3,224,224))\n",
    "fp32_gpu_inference_latency  = measure_inference_latency(model_fp32, device =\"cuda\", input_shape=(3,224,224))\n",
    "int8_cpu_inference_latency  = measure_inference_latency(model_int8, device =\"cpu\", input_shape=(3,224,224))\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.3f} ms\".format(fp32_cpu_inference_latency[0]))\n",
    "print(\"FP32 GPU Inference Latency: {:.3f} ms\".format(fp32_gpu_inference_latency[0]))\n",
    "print(\"INT8 CPU Inference Latency: {:.3f} ms\".format(int8_cpu_inference_latency[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, after the modification, the model in INT8 is smaller in terms of memory. In contrast to the original model, the inference time on CPU is also reduced, with the reduction increasing as the input resolution increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train(model, train_loader, device, learning_rate=1e-1, num_epochs=200, save_dir=\"saved_models\"):\n",
    "    # Create the save directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    tot_exp_time = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Initialize the timer\n",
    "        starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "        # Start Recording the time\n",
    "        starter.record()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            prob = nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = prob.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "\n",
    "        # Save the model state dictionary at the end of each epoch\n",
    "        model_save_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pt\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "        # Save some statics the be saved in output\n",
    "        tot_exp_time += curr_time\n",
    "        accuracy = 100.0 * correct / total\n",
    "        average_loss = train_loss / len(train_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} : Train accuracy {accuracy:.2f}%, Train loss {average_loss:.4f}, Training Time: {curr_time/1000:.3f} s\")\n",
    "\n",
    "    return model, tot_exp_time\n",
    "\n",
    "def test(model, test_loader, device, criterion= nn.CrossEntropyLoss()):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            prob = nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = prob.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    average_loss = test_loss / len(test_loader)\n",
    "\n",
    "    return accuracy, average_loss\n",
    "\n",
    "def calibrate_model(model, loader, device=torch.device(\"cpu\")):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        _ = model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 7 #4\n",
    "alpha = 3 # 0.5\n",
    "beta = 0.75 # 1\n",
    "t_zero = 6 # 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256):\n",
    "\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = (0.1307,), std = (0.3081,))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_set = torchvision.datasets.MNIST(root='./data', train=True, transform=train_transform, download=True)\n",
    "    test_set = torchvision.datasets.MNIST(root='./data', train=False, transform=train_transform)\n",
    "\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set, batch_size=train_batch_size,\n",
    "        sampler=train_sampler, num_workers=num_workers)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_set, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the original PhiNet model on the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (1): Conv2d(1, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): HSwish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): DepthwiseConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): HSwish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), groups=416, bias=False)\n",
      "        (6): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(416, 69, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(69, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 401, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(401, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(401, 401, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=401, bias=False)\n",
      "        (5): BatchNorm2d(401, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(401, 66, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(66, 401, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(401, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 385, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(385, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(385, 385, kernel_size=(3, 3), stride=(2, 2), groups=385, bias=False)\n",
      "        (6): BatchNorm2d(385, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(385, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 385, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(385, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(740, 740, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=740, bias=False)\n",
      "        (5): BatchNorm2d(740, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(740, 123, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(123, 740, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(740, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(709, 709, kernel_size=(3, 3), stride=(2, 2), groups=709, bias=False)\n",
      "        (6): BatchNorm2d(709, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(709, 118, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(118, 709, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(709, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1357, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1357, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(1357, 1357, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1357, bias=False)\n",
      "        (5): BatchNorm2d(1357, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): HSwish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1357, 226, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(226, 1357, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (8): Conv2d(1357, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): HSwish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), groups=1296, bias=False)\n",
      "        (6): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): HSwish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): HSwish()\n",
      "        )\n",
      "        (9): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(576, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/10 : Train accuracy 92.86%, Train loss 0.2503, Training Time: 16.240 s\n",
      "Epoch 2/10 : Train accuracy 97.50%, Train loss 0.0816, Training Time: 16.180 s\n",
      "Epoch 3/10 : Train accuracy 97.77%, Train loss 0.0782, Training Time: 17.802 s\n",
      "Epoch 4/10 : Train accuracy 98.36%, Train loss 0.0549, Training Time: 18.455 s\n",
      "Epoch 5/10 : Train accuracy 98.59%, Train loss 0.0469, Training Time: 18.495 s\n",
      "Epoch 6/10 : Train accuracy 98.65%, Train loss 0.0464, Training Time: 18.443 s\n",
      "Epoch 7/10 : Train accuracy 98.70%, Train loss 0.0444, Training Time: 18.484 s\n",
      "Epoch 8/10 : Train accuracy 98.81%, Train loss 0.0402, Training Time: 18.450 s\n",
      "Epoch 9/10 : Train accuracy 98.94%, Train loss 0.0353, Training Time: 18.537 s\n",
      "Epoch 10/10 : Train accuracy 98.84%, Train loss 0.0386, Training Time: 18.530 s\n",
      "Training Time: 179.616 s\n",
      "Test Accuracy: 98.68%\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "model_mnist_fp32 = micromind.PhiNet(input_shape = [1, 28, 28], num_layers = num_layers, alpha = alpha, beta = beta, t_zero = t_zero, include_top = True, num_classes = 10)\n",
    "print(model_mnist_fp32)\n",
    "\n",
    "train_loader, test_loader = prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_mnist_fp32, train_time_model_mnist_fp32 = train(model_mnist_fp32, train_loader, device, learning_rate=0.01, num_epochs=10, save_dir=\"orignal_model\")\n",
    "\n",
    "acc_model_mnist_fp32, _ = test(model_mnist_fp32, test_loader, device)\n",
    "\n",
    "print(f\"Training Time: {train_time_model_mnist_fp32/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_mnist_fp32:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the modified PhiNet model on the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (1): Conv2d(1, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416, bias=False)\n",
      "        (5): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400, bias=False)\n",
      "        (5): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "        (5): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744, bias=False)\n",
      "        (5): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(128, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712, bias=False)\n",
      "        (5): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(120, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)\n",
      "        (5): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(224, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296, bias=False)\n",
      "        (5): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(576, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 : Train accuracy 92.95%, Train loss 0.2240, Training Time: 14.333 s\n",
      "Epoch 2/10 : Train accuracy 97.88%, Train loss 0.0673, Training Time: 14.769 s\n",
      "Epoch 3/10 : Train accuracy 98.52%, Train loss 0.0472, Training Time: 16.209 s\n",
      "Epoch 4/10 : Train accuracy 98.76%, Train loss 0.0402, Training Time: 16.608 s\n",
      "Epoch 5/10 : Train accuracy 99.02%, Train loss 0.0321, Training Time: 16.536 s\n",
      "Epoch 6/10 : Train accuracy 99.04%, Train loss 0.0312, Training Time: 16.584 s\n",
      "Epoch 7/10 : Train accuracy 99.14%, Train loss 0.0276, Training Time: 16.621 s\n",
      "Epoch 8/10 : Train accuracy 99.21%, Train loss 0.0246, Training Time: 16.431 s\n",
      "Epoch 9/10 : Train accuracy 99.32%, Train loss 0.0214, Training Time: 16.583 s\n",
      "Epoch 10/10 : Train accuracy 99.27%, Train loss 0.0229, Training Time: 16.565 s\n",
      "Training Time: 161.239 s\n",
      "Test Accuracy: 98.59%\n"
     ]
    }
   ],
   "source": [
    "# set the seed for reproducibility\n",
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "model_quant_mnist_fp32 = phinet_quant.PhiNet(input_shape = [1, 28, 28], num_layers = num_layers, alpha = alpha, beta = beta, t_zero = t_zero, include_top = True, num_classes = 10, divisor=8)\n",
    "remove_depthwise(model_quant_mnist_fp32)\n",
    "print(model_quant_mnist_fp32)\n",
    "\n",
    "train_loader, test_loader = prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_quant_mnist_fp32, train_time_model_quant_mnist_fp32 = train(model_quant_mnist_fp32, train_loader, device, learning_rate=0.001, num_epochs=10, save_dir=\"modified_model\")\n",
    "\n",
    "acc_model_quant_mnist_fp32, _ = test(model_quant_mnist_fp32, test_loader, device)\n",
    "\n",
    "print(f\"Training Time: {train_time_model_quant_mnist_fp32/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_quant_mnist_fp32:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static Quantization of the modified PhiNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiNetQuant(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(PhiNetQuant, self).__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized.\n",
    "        # This will only be used for inputs.\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        # FP32 model\n",
    "        self.model_fp32 = model_fp32\n",
    "        # DeQuantStub converts tensors from quantized to floating point.\n",
    "        # This will only be used for outputs.\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fp32:\n",
      " PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (1): Conv2d(1, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416, bias=False)\n",
      "        (5): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400, bias=False)\n",
      "        (5): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "        (5): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744, bias=False)\n",
      "        (5): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(128, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712, bias=False)\n",
      "        (5): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(120, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)\n",
      "        (5): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(224, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296, bias=False)\n",
      "        (5): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(576, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "model_fp32_fused:\n",
      " PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (1): Conv2d(1, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Identity()\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)\n",
      "        (2): Identity()\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (5): Identity()\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(128, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(120, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(224, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Model equivalence test passed!\n",
      "After preparation, note fake-quantization modules:\n",
      " PhiNetQuant(\n",
      "  (quant): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (model_fp32): PhiNet(\n",
      "    (_layers): ModuleList(\n",
      "      (0): ZeroPad2d((0, 1, 0, 1))\n",
      "      (1): SeparableConv2d(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            1, 1, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Conv2d(\n",
      "            1, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (2): Identity()\n",
      "          (3): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Dropout2d(p=0.05, inplace=False)\n",
      "          (1): Conv2d(\n",
      "            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (2): Identity()\n",
      "          (3): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (4): Conv2d(\n",
      "            144, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            72, 416, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            416, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            72, 400, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              400, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              64, 400, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            400, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (5): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            72, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            384, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            144, 744, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              744, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              128, 744, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            744, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (7): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            144, 712, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              712, 120, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              120, 712, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            712, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            288, 1360, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              1360, 224, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              224, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            1360, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (9): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            288, 1296, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            1296, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): Linear(\n",
      "        in_features=576, out_features=10, bias=True\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteotremonti/anaconda3/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_int8:\n",
      " PhiNetQuant(\n",
      "  (quant): Quantize(scale=tensor([0.0255]), zero_point=tensor([17]), dtype=torch.quint8)\n",
      "  (model_fp32): PhiNet(\n",
      "    (_layers): ModuleList(\n",
      "      (0): ZeroPad2d((0, 1, 0, 1))\n",
      "      (1): SeparableConv2d(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), scale=0.06646763533353806, zero_point=17, bias=False)\n",
      "          (1): QuantizedConv2d(1, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.13807225227355957, zero_point=72)\n",
      "          (2): Identity()\n",
      "          (3): QuantizedHardswish()\n",
      "        )\n",
      "      )\n",
      "      (2): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Dropout2d(p=0.05, inplace=False)\n",
      "          (1): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.13718992471694946, zero_point=67, padding=(1, 1), groups=144)\n",
      "          (2): Identity()\n",
      "          (3): QuantizedHardswish()\n",
      "          (4): QuantizedConv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.12022707611322403, zero_point=65)\n",
      "          (5): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), scale=0.1507801115512848, zero_point=60)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), scale=0.17452795803546906, zero_point=63, padding=(1, 1), groups=416)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.5313815474510193, zero_point=47, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), scale=1.7765659093856812, zero_point=61, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.08945583552122116, zero_point=4\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.14930950105190277, zero_point=59)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), scale=0.1369718462228775, zero_point=58)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), scale=0.1631508469581604, zero_point=65, padding=(1, 1), groups=400)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(400, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.5923176407814026, zero_point=50, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(64, 400, kernel_size=(1, 1), stride=(1, 1), scale=1.3154823780059814, zero_point=70, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.08230117708444595, zero_point=5\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.10561347752809525, zero_point=64)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=0.18598704040050507, zero_point=60\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.14031414687633514, zero_point=64)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), scale=0.1818360686302185, zero_point=58, padding=(1, 1), groups=384)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.31895712018013, zero_point=61, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.783915102481842, zero_point=63, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.08202663064002991, zero_point=5\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.13738572597503662, zero_point=62)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), scale=0.13994856178760529, zero_point=61)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), scale=0.1797332763671875, zero_point=59, padding=(1, 1), groups=744)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(744, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.4856511056423187, zero_point=45, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(128, 744, kernel_size=(1, 1), stride=(1, 1), scale=1.7224634885787964, zero_point=56, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.10160400718450546, zero_point=4\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.08054937422275543, zero_point=67)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=0.15100890398025513, zero_point=63\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (7): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), scale=0.1470053493976593, zero_point=60)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), scale=0.1541248857975006, zero_point=60, padding=(1, 1), groups=712)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(712, 120, kernel_size=(1, 1), stride=(1, 1), scale=1.886536955833435, zero_point=53, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(120, 712, kernel_size=(1, 1), stride=(1, 1), scale=19.988658905029297, zero_point=62, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.09297077357769012, zero_point=4\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.11561570316553116, zero_point=66)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), scale=0.10911354422569275, zero_point=54)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), scale=0.20578476786613464, zero_point=37, padding=(1, 1), groups=1360)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(1360, 224, kernel_size=(1, 1), stride=(1, 1), scale=2.383960485458374, zero_point=45, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(224, 1360, kernel_size=(1, 1), stride=(1, 1), scale=13.373237609863281, zero_point=67, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.1552070528268814, zero_point=2\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.05371829494833946, zero_point=66)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=0.12001831084489822, zero_point=65\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (9): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), scale=0.13406753540039062, zero_point=52)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), scale=0.1352463960647583, zero_point=58, padding=(1, 1), groups=1296)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), scale=3.8029913902282715, zero_point=51, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), scale=42.67746353149414, zero_point=64, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.08054210990667343, zero_point=5\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.032483719289302826, zero_point=65)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): QuantizedLinear(in_features=576, out_features=10, scale=0.3063695728778839, zero_point=33, qscheme=torch.per_channel_affine)\n",
      "    )\n",
      "  )\n",
      "  (dequant): DeQuantize()\n",
      ")\n",
      "\n",
      "Test Accuracy: 98.36%\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "# Copy the pretrained model for static quantization\n",
    "model_st_quant_mnist_fp32 = copy.deepcopy(model_quant_mnist_fp32)\n",
    "model_st_quant_mnist_fp32.to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "model_st_quant_mnist_fp32_fused = copy.deepcopy(model_st_quant_mnist_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "model_st_quant_mnist_fp32.eval()\n",
    "model_st_quant_mnist_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet model\n",
    "phinet_fuse_modules(model_st_quant_mnist_fp32_fused)\n",
    "\n",
    "print(f\"model_fp32:\\n {model_st_quant_mnist_fp32}\\n\")\n",
    "print(f\"model_fp32_fused:\\n {model_st_quant_mnist_fp32_fused}\\n\")\n",
    "\n",
    "assert model_equivalence(model_1=model_st_quant_mnist_fp32, model_2=model_st_quant_mnist_fp32_fused, device=\"cpu\", rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,1,28,28)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "model_st_quant_mnist_int8 = PhiNetQuant(model_st_quant_mnist_fp32_fused)\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "model_st_quant_mnist_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(model_st_quant_mnist_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {model_st_quant_mnist_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "calibrate_model(model_st_quant_mnist_int8, train_loader, device=\"cpu\")\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "model_st_quant_mnist_int8 = torch.ao.quantization.convert(model_st_quant_mnist_int8, inplace=True)\n",
    "\n",
    "print(f\"model_int8:\\n {model_st_quant_mnist_int8}\\n\")\n",
    "\n",
    "acc_model_st_quant_mnist_int8, _ = test(model_st_quant_mnist_int8, test_loader, \"cpu\")\n",
    "\n",
    "print(f\"Test Accuracy: {acc_model_st_quant_mnist_int8:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization awere training of the modified model on the MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fp32:\n",
      " PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (1): Conv2d(1, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (2): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416, bias=False)\n",
      "        (5): BatchNorm2d(416, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400, bias=False)\n",
      "        (5): BatchNorm2d(400, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(72, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "        (5): BatchNorm2d(384, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744, bias=False)\n",
      "        (5): BatchNorm2d(744, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(128, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(144, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712, bias=False)\n",
      "        (5): BatchNorm2d(712, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(120, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360, bias=False)\n",
      "        (5): BatchNorm2d(1360, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(224, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(288, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296, bias=False)\n",
      "        (5): BatchNorm2d(1296, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(576, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "model_fp32_fused:\n",
      " PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (1): Conv2d(1, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Identity()\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)\n",
      "        (2): Identity()\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (5): Identity()\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(416, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 400, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(400, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(400, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(72, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(384, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 744, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(744, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(128, 744, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(744, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(144, 712, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(712, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(120, 712, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(712, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1360, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(224, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "      (op): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): Conv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296)\n",
      "        (5): Identity()\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "          (mult): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (9): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=576, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "After preparation, note fake-quantization modules:\n",
      " PhiNetQuant(\n",
      "  (quant): QuantStub(\n",
      "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (model_fp32): PhiNet(\n",
      "    (_layers): ModuleList(\n",
      "      (0): ZeroPad2d((0, 1, 0, 1))\n",
      "      (1): SeparableConv2d(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            1, 1, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Conv2d(\n",
      "            1, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (2): Identity()\n",
      "          (3): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Dropout2d(p=0.05, inplace=False)\n",
      "          (1): Conv2d(\n",
      "            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (2): Identity()\n",
      "          (3): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (4): Conv2d(\n",
      "            144, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            72, 416, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            416, 416, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=416\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              416, 72, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              72, 416, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            416, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            72, 400, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              400, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              64, 400, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            400, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (5): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            72, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            384, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            144, 744, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            744, 744, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=744\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              744, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              128, 744, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            744, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (7): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            144, 712, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            712, 712, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=712\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              712, 120, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              120, 712, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            712, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            288, 1360, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            1360, 1360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1360\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              1360, 224, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              224, 1360, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            1360, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): FloatFunctional(\n",
      "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (9): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Conv2d(\n",
      "            288, 1296, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): Conv2d(\n",
      "            1296, 1296, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1296\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (5): Identity()\n",
      "          (6): Hardswish(\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (7): SEBlock(\n",
      "            (se_conv): Conv2d(\n",
      "              1296, 216, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (se_conv2): Conv2d(\n",
      "              216, 1296, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (activation): Hardswish(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "            (mult): FloatFunctional(\n",
      "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (8): Conv2d(\n",
      "            1296, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): Linear(\n",
      "        in_features=576, out_features=10, bias=True\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "\n",
      "Epoch 1/10 : Train accuracy 98.29%, Train loss 0.0644, Training Time: 51.036 s\n",
      "Epoch 2/10 : Train accuracy 98.68%, Train loss 0.0471, Training Time: 56.921 s\n",
      "Epoch 3/10 : Train accuracy 98.93%, Train loss 0.0394, Training Time: 62.434 s\n",
      "Epoch 4/10 : Train accuracy 98.89%, Train loss 0.0402, Training Time: 63.052 s\n",
      "Epoch 5/10 : Train accuracy 98.97%, Train loss 0.0376, Training Time: 62.741 s\n",
      "Epoch 6/10 : Train accuracy 99.21%, Train loss 0.0283, Training Time: 63.097 s\n",
      "Epoch 7/10 : Train accuracy 99.14%, Train loss 0.0305, Training Time: 64.617 s\n",
      "Epoch 8/10 : Train accuracy 99.18%, Train loss 0.0295, Training Time: 64.244 s\n",
      "Epoch 9/10 : Train accuracy 99.33%, Train loss 0.0259, Training Time: 64.303 s\n",
      "Epoch 10/10 : Train accuracy 99.23%, Train loss 0.0277, Training Time: 64.606 s\n",
      "model_int8:\n",
      " PhiNetQuant(\n",
      "  (quant): Quantize(scale=tensor([0.0255]), zero_point=tensor([17]), dtype=torch.quint8)\n",
      "  (model_fp32): PhiNet(\n",
      "    (_layers): ModuleList(\n",
      "      (0): ZeroPad2d((0, 1, 0, 1))\n",
      "      (1): SeparableConv2d(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), scale=0.03755145147442818, zero_point=21, bias=False)\n",
      "          (1): QuantizedConv2d(1, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.08627311885356903, zero_point=67)\n",
      "          (2): Identity()\n",
      "          (3): QuantizedHardswish()\n",
      "        )\n",
      "      )\n",
      "      (2): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): Dropout2d(p=0.05, inplace=False)\n",
      "          (1): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.09894920140504837, zero_point=63, padding=(1, 1), groups=144)\n",
      "          (2): Identity()\n",
      "          (3): QuantizedHardswish()\n",
      "          (4): QuantizedConv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.09516549855470657, zero_point=68)\n",
      "          (5): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), scale=0.1495964378118515, zero_point=63)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(416, 416, kernel_size=(3, 3), stride=(2, 2), scale=0.20918308198451996, zero_point=72, padding=(1, 1), groups=416)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.18145200610160828, zero_point=76, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(72, 416, kernel_size=(1, 1), stride=(1, 1), scale=0.24132810533046722, zero_point=60, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.09139741957187653, zero_point=4\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(416, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.1236380785703659, zero_point=58)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(72, 400, kernel_size=(1, 1), stride=(1, 1), scale=0.17833265662193298, zero_point=58)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), scale=0.26323142647743225, zero_point=68, padding=(1, 1), groups=400)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(400, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.2623458504676819, zero_point=53, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(64, 400, kernel_size=(1, 1), stride=(1, 1), scale=0.2474445402622223, zero_point=59, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.07893193513154984, zero_point=5\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(400, 72, kernel_size=(1, 1), stride=(1, 1), scale=0.08542223274707794, zero_point=66)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=0.15096785128116608, zero_point=60\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(72, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.15330110490322113, zero_point=62)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), scale=0.22472967207431793, zero_point=74, padding=(1, 1), groups=384)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.15895244479179382, zero_point=84, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.16623340547084808, zero_point=64, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.06563684344291687, zero_point=6\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(384, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.11029088497161865, zero_point=63)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(144, 744, kernel_size=(1, 1), stride=(1, 1), scale=0.2185855358839035, zero_point=61)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(744, 744, kernel_size=(3, 3), stride=(1, 1), scale=0.3649260997772217, zero_point=61, padding=(1, 1), groups=744)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(744, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.7287967801094055, zero_point=56, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(128, 744, kernel_size=(1, 1), stride=(1, 1), scale=1.8268612623214722, zero_point=65, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.08999106287956238, zero_point=4\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(744, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.08941362053155899, zero_point=67)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=0.14511406421661377, zero_point=64\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (7): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(144, 712, kernel_size=(1, 1), stride=(1, 1), scale=0.2739301323890686, zero_point=58)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(712, 712, kernel_size=(3, 3), stride=(2, 2), scale=0.49084940552711487, zero_point=73, padding=(1, 1), groups=712)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(712, 120, kernel_size=(1, 1), stride=(1, 1), scale=0.5476863980293274, zero_point=60, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(120, 712, kernel_size=(1, 1), stride=(1, 1), scale=1.4833662509918213, zero_point=67, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.1808459460735321, zero_point=2\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(712, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.19793710112571716, zero_point=62)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(288, 1360, kernel_size=(1, 1), stride=(1, 1), scale=0.6877002716064453, zero_point=57)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(1360, 1360, kernel_size=(3, 3), stride=(1, 1), scale=1.218188762664795, zero_point=69, padding=(1, 1), groups=1360)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(1360, 224, kernel_size=(1, 1), stride=(1, 1), scale=2.324974775314331, zero_point=50, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(224, 1360, kernel_size=(1, 1), stride=(1, 1), scale=7.83658504486084, zero_point=66, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.16227497160434723, zero_point=2\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(1360, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.08963149785995483, zero_point=65)\n",
      "          (9): Identity()\n",
      "        )\n",
      "        (op): QFunctional(\n",
      "          scale=0.17711007595062256, zero_point=63\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (9): PhiNetConvBlock(\n",
      "        (_layers): ModuleList(\n",
      "          (0): QuantizedConv2d(288, 1296, kernel_size=(1, 1), stride=(1, 1), scale=0.5747877955436707, zero_point=57)\n",
      "          (1): Identity()\n",
      "          (2): QuantizedHardswish()\n",
      "          (3): Dropout2d(p=0.05, inplace=False)\n",
      "          (4): QuantizedConv2d(1296, 1296, kernel_size=(3, 3), stride=(2, 2), scale=0.9636286497116089, zero_point=66, padding=(1, 1), groups=1296)\n",
      "          (5): Identity()\n",
      "          (6): QuantizedHardswish()\n",
      "          (7): SEBlock(\n",
      "            (se_conv): QuantizedConv2d(1296, 216, kernel_size=(1, 1), stride=(1, 1), scale=5.021620273590088, zero_point=55, bias=False)\n",
      "            (se_conv2): QuantizedConv2d(216, 1296, kernel_size=(1, 1), stride=(1, 1), scale=19.722625732421875, zero_point=66, bias=False)\n",
      "            (activation): QuantizedHardswish()\n",
      "            (mult): QFunctional(\n",
      "              scale=0.1786821037530899, zero_point=2\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (8): QuantizedConv2d(1296, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.23465873301029205, zero_point=63)\n",
      "          (9): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): QuantizedLinear(in_features=576, out_features=10, scale=0.8011595010757446, zero_point=31, qscheme=torch.per_channel_affine)\n",
      "    )\n",
      "  )\n",
      "  (dequant): DeQuantize()\n",
      ")\n",
      "\n",
      "Training Time: 617.050 s\n",
      "Test Accuracy: 98.81%\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "model_qat_quant_mnist_fp32 = phinet_quant.PhiNet(input_shape = [1, 28, 28], num_layers = num_layers, alpha = alpha, beta = beta, t_zero = t_zero, include_top = True, num_classes = 10, divisor=8)\n",
    "remove_depthwise(model_qat_quant_mnist_fp32)\n",
    "\n",
    "load_epoch = 2\n",
    "\n",
    "if load_epoch is not None:\n",
    "    saved_model_path = f\"modified_model/model_epoch_{load_epoch}.pt\" \n",
    "    checkpoint = torch.load(saved_model_path)\n",
    "    model_qat_quant_mnist_fp32.load_state_dict(checkpoint)\n",
    "\n",
    "model_qat_quant_mnist_fp32.to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "model_qat_quant_mnist_fp32_fused = copy.deepcopy(model_qat_quant_mnist_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "model_qat_quant_mnist_fp32.eval()\n",
    "model_qat_quant_mnist_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet model\n",
    "phinet_fuse_modules(model_qat_quant_mnist_fp32_fused)\n",
    "\n",
    "print(f\"model_fp32:\\n {model_qat_quant_mnist_fp32}\\n\")\n",
    "print(f\"model_fp32_fused:\\n {model_qat_quant_mnist_fp32_fused}\\n\")\n",
    "\n",
    "# Insert stubs ion the model and in the forward pass\n",
    "model_qat_quant_mnist_int8 = PhiNetQuant(model_qat_quant_mnist_fp32_fused)\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "model_qat_quant_mnist_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare_qat(model_qat_quant_mnist_int8.train(), inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {model_qat_quant_mnist_int8}\\n\")\n",
    "\n",
    "# Quantization awere training and calibrate the model\n",
    "model_qat_quant_mnist_int8, train_time_model_qat_quant_mnist_int8 = train(model_qat_quant_mnist_int8, train_loader, device = \"cuda\", learning_rate=0.001, num_epochs=10, save_dir=\"qat_model\")\n",
    "\n",
    "model_qat_quant_mnist_int8.to(\"cpu\")\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "model_qat_quant_mnist_int8 = torch.ao.quantization.convert(model_qat_quant_mnist_int8, inplace=True)\n",
    "\n",
    "print(f\"model_int8:\\n {model_qat_quant_mnist_int8}\\n\")\n",
    "\n",
    "acc_model_qat_quant_mnist_int8, _ = test(model_qat_quant_mnist_int8, test_loader, \"cpu\")\n",
    "\n",
    "print(f\"Training Time: {train_time_model_qat_quant_mnist_int8/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_qat_quant_mnist_int8:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare results on **MNIST**:\n",
    "- Orginal model FP32;\n",
    "- Modified model FP32;\n",
    "- Static Quantization of the modified model INT8;\n",
    "- Quantization aware training of the modified model INT8;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal FP32 model CPU Inference Latency: 9.244 ms\n",
      "Orginal FP32 model GPU Inference Latency: 5.900 ms\n",
      "Orginal FP32 model test accuracy: 98.68%\n",
      "Original FP32 model training time: 179.616 s\n",
      "\n",
      "Modified FP32 model CPU Inference Latency: 7.669 ms\n",
      "Modified FP32 model GPU Inference Latency: 3.823 ms\n",
      "Modified FP32 model test accuracy: 98.59%\n",
      "Modified FP32 model training time: 161.239 s\n",
      "\n",
      "Modified INT8 model static quant CPU Inference Latency: 3.453 ms\n",
      "Modified INT8 model static quant test accuracy: 98.36%\n",
      "\n",
      "Modified INT8 model QAT CPU Inference Latency: 4.760 ms\n",
      "Modified INT8 model QAT test accuracy: 98.81%\n",
      "Modified INT8 model QAT training time: 617.050 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_fp32_cpu_inference_latency  = measure_inference_latency(model_mnist_fp32, device=\"cpu\", input_shape=(1, 28, 28))\n",
    "model_fp32_gpu_inference_latency  = measure_inference_latency(model_mnist_fp32, device=\"cuda\", input_shape=(1, 28, 28))\n",
    "\n",
    "model_quant_fp32_cpu_inference_latency  = measure_inference_latency(model_quant_mnist_fp32, device=\"cpu\", input_shape=(1, 28, 28))\n",
    "model_quant_fp32_gpu_inference_latency  = measure_inference_latency(model_quant_mnist_fp32, device=\"cuda\", input_shape=(1, 28, 28))\n",
    "\n",
    "model_st_quant_int8_cpu_inference_latency  = measure_inference_latency(model_st_quant_mnist_int8, device=\"cpu\", input_shape=(1, 28, 28))\n",
    "model_qat_quant_int8_cpu_inference_latency  = measure_inference_latency(model_qat_quant_mnist_int8, device=\"cpu\", input_shape=(1, 28, 28))\n",
    "\n",
    "print(\"Orginal FP32 model CPU Inference Latency: {:.3f} ms\".format(model_fp32_cpu_inference_latency[0]))\n",
    "print(\"Orginal FP32 model GPU Inference Latency: {:.3f} ms\".format(model_fp32_gpu_inference_latency[0]))\n",
    "print(\"Orginal FP32 model test accuracy: {:.2f}%\".format(acc_model_mnist_fp32))\n",
    "print(\"Original FP32 model training time: {:.3f} s\\n\".format(train_time_model_mnist_fp32/1000))\n",
    "\n",
    "print(\"Modified FP32 model CPU Inference Latency: {:.3f} ms\".format(model_quant_fp32_cpu_inference_latency[0]))\n",
    "print(\"Modified FP32 model GPU Inference Latency: {:.3f} ms\".format(model_quant_fp32_gpu_inference_latency[0]))\n",
    "print(\"Modified FP32 model test accuracy: {:.2f}%\".format(acc_model_quant_mnist_fp32))\n",
    "print(\"Modified FP32 model training time: {:.3f} s\\n\".format(train_time_model_quant_mnist_fp32/1000))\n",
    "\n",
    "print(\"Modified INT8 model static quant CPU Inference Latency: {:.3f} ms\".format(model_st_quant_int8_cpu_inference_latency[0]))\n",
    "print(\"Modified INT8 model static quant test accuracy: {:.2f}%\\n\".format(acc_model_st_quant_mnist_int8))\n",
    "\n",
    "print(\"Modified INT8 model QAT CPU Inference Latency: {:.3f} ms\".format(model_qat_quant_int8_cpu_inference_latency[0]))\n",
    "print(\"Modified INT8 model QAT test accuracy: {:.2f}%\".format(acc_model_qat_quant_mnist_int8))\n",
    "print(\"Modified INT8 model QAT training time: {:.3f} s\\n\".format(train_time_model_qat_quant_mnist_int8/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the CIFAR10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256):\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    train_set = torchvision.datasets.CIFAR10(root='./data', train=True, transform=train_transform, download=True)\n",
    "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, transform=train_transform)\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set, batch_size=train_batch_size,\n",
    "        sampler=train_sampler, num_workers=num_workers)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_set, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the original PhiNet model on the CIFAR10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micromind import PhiNet\n",
    "\n",
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "model_cifar_fp32 = PhiNet(input_shape = [3, 32, 32], num_layers = 7, alpha = 3, beta = 0.75, t_zero = 6, include_top = True, num_classes = 10)\n",
    "print(model_cifar_fp32)\n",
    "\n",
    "train_loader, test_loader = prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_cifar_fp32, train_time_model_cifar_fp32 = train(model_cifar_fp32, train_loader, device, learning_rate=0.01, num_epochs=10, save_dir=\"orignal_model\")\n",
    "\n",
    "acc_model_cifar_fp32, _ = test(model_cifar_fp32, test_loader, device)\n",
    "\n",
    "print(f\"Training Time: {train_time_model_cifar_fp32/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_cifar_fp32:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the modified PhiNet model on the CIFAR10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phinet_quant import PhiNet\n",
    "\n",
    "# set the seed for reproducibility\n",
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "model_quant_cifar_fp32 = PhiNet(input_shape = [3, 32, 32], num_layers = 7, alpha = 3, beta = 0.75, t_zero = 6, include_top = True, num_classes = 10)\n",
    "remove_depthwise(model_quant_cifar_fp32)\n",
    "print(model_quant_cifar_fp32)\n",
    "\n",
    "train_loader, test_loader = prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_quant_cifar_fp32, train_time_model_quant_cifar_fp32 = train(model_quant_cifar_fp32, train_loader, device, learning_rate=0.01, num_epochs=10, save_dir=\"modified_model\")\n",
    "\n",
    "acc_model_quant_cifar_fp32, _ = test(model_quant_cifar_fp32, test_loader, device)\n",
    "\n",
    "print(f\"Training Time: {train_time_model_quant_cifar_fp32/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_quant_cifar_fp32:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static Quantization of the modified PhiNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiNetQuant(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(PhiNetQuant, self).__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized.\n",
    "        # This will only be used for inputs.\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        # FP32 model\n",
    "        self.model_fp32 = model_fp32\n",
    "        # DeQuantStub converts tensors from quantized to floating point.\n",
    "        # This will only be used for outputs.\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "# Copy the pretrained model for static quantization\n",
    "model_st_quant_cifar_fp32 = copy.deepcopy(model_quant_cifar_fp32)\n",
    "model_st_quant_cifar_fp32.to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "model_st_quant_cifar_fp32_fused = copy.deepcopy(model_st_quant_cifar_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "model_st_quant_cifar_fp32.eval()\n",
    "model_st_quant_cifar_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet model\n",
    "phinet_fuse_modules(model_st_quant_cifar_fp32_fused)\n",
    "\n",
    "print(f\"model_fp32:\\n {model_st_quant_cifar_fp32}\\n\")\n",
    "print(f\"model_fp32_fused:\\n {model_st_quant_cifar_fp32_fused}\\n\")\n",
    "\n",
    "assert model_equivalence(model_1=model_st_quant_cifar_fp32, model_2=model_st_quant_cifar_fp32_fused, device=\"cpu\", rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "model_st_quant_cifar_int8 = PhiNetQuant(model_st_quant_cifar_fp32_fused)\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "model_st_quant_cifar_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(model_st_quant_cifar_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {model_st_quant_cifar_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "calibrate_model(model_st_quant_cifar_int8, train_loader, device=\"cpu\")\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "model_st_quant_cifar_int8 = torch.ao.quantization.convert(model_st_quant_cifar_int8, inplace=True)\n",
    "\n",
    "print(f\"model_int8:\\n {model_st_quant_cifar_int8}\\n\")\n",
    "\n",
    "acc_model_st_quant_cifar_int8, _ = test(model_st_quant_cifar_int8, test_loader, \"cpu\")\n",
    "\n",
    "print(f\"Test Accuracy: {acc_model_st_quant_cifar_int8:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization awere training of the modified model on the CIFAR10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phinet_quant import PhiNet\n",
    "\n",
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "model_qat_quant_cifar_fp32 = PhiNet(input_shape = [3, 32, 32], num_layers = 7, alpha = 3, beta = 0.75, t_zero = 6, include_top = True, num_classes = 10)\n",
    "remove_depthwise(model_qat_quant_cifar_fp32)\n",
    "\n",
    "load_epoch = 2\n",
    "\n",
    "if load_epoch is not None:\n",
    "    saved_model_path = f\"modified_model/model_epoch_{load_epoch}.pt\" \n",
    "    checkpoint = torch.load(saved_model_path)\n",
    "    model_qat_quant_cifar_fp32.load_state_dict(checkpoint)\n",
    "\n",
    "model_qat_quant_cifar_fp32.to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "model_qat_quant_cifar_fp32_fused = copy.deepcopy(model_qat_quant_cifar_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "model_qat_quant_cifar_fp32.eval()\n",
    "model_qat_quant_cifar_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN modules in the PhiNet model\n",
    "phinet_fuse_modules(model_qat_quant_cifar_fp32_fused)\n",
    "\n",
    "print(f\"model_fp32:\\n {model_qat_quant_cifar_fp32}\\n\")\n",
    "print(f\"model_fp32_fused:\\n {model_qat_quant_cifar_fp32_fused}\\n\")\n",
    "\n",
    "# Insert stubs ion the model and in the forward pass\n",
    "model_qat_quant_cifar_int8 = PhiNetQuant(model_qat_quant_cifar_fp32_fused)\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "model_qat_quant_cifar_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare_qat(model_qat_quant_cifar_int8.train(), inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {model_qat_quant_cifar_int8}\\n\")\n",
    "\n",
    "# Quantization awere training and calibrate the model\n",
    "model_qat_quant_cifar_int8, train_time_model_qat_quant_cifar_int8 = train(model_qat_quant_cifar_int8, train_loader, device = \"cuda\", learning_rate=0.001, num_epochs=10, save_dir=\"qat_model\")\n",
    "\n",
    "model_qat_quant_cifar_int8.to(\"cpu\")\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "model_qat_quant_cifar_int8 = torch.ao.quantization.convert(model_qat_quant_cifar_int8, inplace=True)\n",
    "\n",
    "print(f\"model_int8:\\n {model_qat_quant_cifar_int8}\\n\")\n",
    "\n",
    "acc_model_qat_quant_cifar_int8, _ = test(model_qat_quant_cifar_int8, test_loader, \"cpu\")\n",
    "\n",
    "print(f\"Training Time: {train_time_model_qat_quant_cifar_int8/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_qat_quant_cifar_int8:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare results on **CIFAR10**:\n",
    "- Orginal model FP32;\n",
    "- Modified model FP32;\n",
    "- Static Quantization of the modified model INT8;\n",
    "- Quantization aware training of the modified model INT8;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32_cpu_inference_latency  = measure_inference_latency(model_cifar_fp32, device=\"cpu\", input_shape=(3, 32, 32))\n",
    "model_fp32_gpu_inference_latency  = measure_inference_latency(model_cifar_fp32, device=\"cuda\", input_shape=(3, 32, 32))\n",
    "\n",
    "model_quant_fp32_cpu_inference_latency  = measure_inference_latency(model_quant_cifar_fp32, device=\"cpu\", input_shape=(3, 32, 32))\n",
    "model_quant_fp32_gpu_inference_latency  = measure_inference_latency(model_quant_cifar_fp32, device=\"cuda\", input_shape=(3, 32, 32))\n",
    "\n",
    "model_st_quant_int8_cpu_inference_latency  = measure_inference_latency(model_st_quant_cifar_int8, device=\"cpu\", input_shape=(3, 32, 32))\n",
    "model_qat_quant_int8_cpu_inference_latency  = measure_inference_latency(model_qat_quant_cifar_int8, device=\"cpu\", input_shape=(3, 32, 32))\n",
    "\n",
    "print(\"Original FP32 model CPU Inference Latency: {:.3f} ms\".format(model_fp32_cpu_inference_latency[0]))\n",
    "print(\"Original FP32 model GPU Inference Latency: {:.3f} ms\".format(model_fp32_gpu_inference_latency[0]))\n",
    "print(\"Original FP32 model test accuracy: {:.2f}%\".format(acc_model_cifar_fp32))\n",
    "print(\"Original FP32 model training time: {:.3f} s\\n\".format(train_time_model_cifar_fp32/1000))\n",
    "\n",
    "print(\"Modified FP32 model CPU Inference Latency: {:.3f} ms\".format(model_quant_fp32_cpu_inference_latency[0]))\n",
    "print(\"Modified FP32 model GPU Inference Latency: {:.3f} ms\".format(model_quant_fp32_gpu_inference_latency[0]))\n",
    "print(\"Modified FP32 model test accuracy: {:.2f}%\".format(acc_model_quant_cifar_fp32))\n",
    "print(\"Modified FP32 model training time: {:.3f} s\\n\".format(train_time_model_quant_cifar_fp32/1000))\n",
    "\n",
    "print(\"Modified INT8 model static quant CPU Inference Latency: {:.3f} ms\".format(model_st_quant_int8_cpu_inference_latency[0]))\n",
    "print(\"Modified INT8 model static quant test accuracy: {:.2f}%\\n\".format(acc_model_st_quant_cifar_int8))\n",
    "\n",
    "print(\"Modified INT8 model QAT CPU Inference Latency: {:.3f} ms\".format(model_qat_quant_int8_cpu_inference_latency[0]))\n",
    "print(\"Modified INT8 model QAT test accuracy: {:.2f}%\".format(acc_model_qat_quant_cifar_int8))\n",
    "print(\"Modified INT8 model QAT training time: {:.3f} s\\n\".format(train_time_model_qat_quant_cifar_int8/1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
