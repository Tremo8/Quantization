{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook to show the difference in performance between the original model and the model obtained with modifications for quantization. \n",
    "\n",
    "Making the channels divisible by 8 greatly reduced the inference time on both GPU and CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import phinet_quant\n",
    "import micromind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    \"\"\"\n",
    "    Print the size of the model.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size_in_bytes = os.path.getsize(\"temp.p\")\n",
    "    if size_in_bytes < 1048576:\n",
    "        size_in_kb = size_in_bytes / 1024\n",
    "        print(\"{:.3f} KB\".format(size_in_kb))\n",
    "    else:\n",
    "        size_in_mb = size_in_bytes / 1048576\n",
    "        print(\"{:.3f} MB\".format(size_in_mb))\n",
    "    os.remove('temp.p')\n",
    "\n",
    "\n",
    "def measure_inference_latency(model, input_shape, device = None, repetitions=100, warmup_it = 10):\n",
    "    \"\"\"\n",
    "    Measures the inference time of the provided neural network model.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to evaluate.\n",
    "        input_shape: The shape of the input data expected by the model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the mean and standard deviation of the inference time\n",
    "               measured in milliseconds.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device.type  # Get the device where the model is located\n",
    "    \n",
    "    dummy_input = torch.randn(1, *input_shape, dtype=torch.float).to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # GPU warm-up\n",
    "    for _ in range(warmup_it):\n",
    "        _ = model(dummy_input)\n",
    "\n",
    "    # Measure inference time\n",
    "    timings = []\n",
    "    with torch.no_grad():\n",
    "        if device == 'cuda':\n",
    "            starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "            for rep in range(repetitions):\n",
    "                starter.record()\n",
    "                _ = model(dummy_input)\n",
    "                ender.record()\n",
    "                torch.cuda.synchronize()\n",
    "                curr_time = starter.elapsed_time(ender)\n",
    "                timings.append(curr_time)\n",
    "        else:  # CPU\n",
    "            for rep in range(repetitions):\n",
    "                start_time = time.time()\n",
    "                _ = model(dummy_input)\n",
    "                end_time = time.time()\n",
    "                elapsed_time = (end_time - start_time) * 1000.0  # Convert to milliseconds\n",
    "                timings.append(elapsed_time)\n",
    "\n",
    "    # Calculate mean and std\n",
    "    mean_time = np.mean(timings)\n",
    "    std_time = np.std(timings)\n",
    "\n",
    "    return mean_time, std_time\n",
    "\n",
    "def set_random_seeds(random_seed=0):\n",
    "    \"\"\"\n",
    "    Set all random seeds to a fixed value to make results reproducible.\n",
    "    \"\"\"\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orginal PhiNet VS Modified PhiNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of the original PhiNet model and the model modified to make quantization effective (channels divisible by 8 and `padding` within `Conv2d`). The following values are compared:\n",
    "- Number of parameters;\n",
    "- Number of operations;\n",
    "- Inference time;\n",
    "\n",
    "**The comparison is made with both models in FP32**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Model\n",
    "`input_shape = [3, 224, 224], num_layers = 7, alpha = 3, beta = 0.75, t_zero = 6, include_top = True, num_classes = 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orginal PhiNet\n",
    "model = micromind.PhiNet(input_shape = [3, 224, 224], num_layers = 7, alpha = 3, beta = 0.75, t_zero = 6, include_top = True, num_classes = 10)\n",
    "# Modified PhiNet\n",
    "model_quant = phinet_quant.PhiNet(input_shape = [3, 224, 224], num_layers = 7, alpha = 3, beta = 0.75, t_zero = 6, include_top = True, num_classes = 10, divisor=8)\n",
    "\n",
    "# Print the models\n",
    "print(f\"Origina model:\\n{model}\\n\")\n",
    "print(f\"Modified model:\\n{model_quant}\")\n",
    "\n",
    "# Get the summary of the models\n",
    "org_model = summary(model, input_size=(1, 3, 224, 224), verbose=0)\n",
    "mod_model = summary(model_quant, input_size=(1, 3, 224, 224), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small models\n",
    "`input_shape = [3, 224, 224], num_layers = 7, alpha = 2.3, beta = 0.75,t_zero = 5, include_top = True, num_classes = 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orginal PhiNet\n",
    "model_S = micromind.PhiNet(input_shape = [3, 224, 224], num_layers=7, alpha= 2.3, beta= 0.75,t_zero= 5, include_top= True, num_classes=10)\n",
    "# Modified PhiNet\n",
    "model_quant_S = phinet_quant.PhiNet(input_shape = [3, 224, 224], num_layers=7, alpha= 2.3, beta= 0.75,t_zero= 5, include_top= True, num_classes=10, divisor=8)\n",
    "\n",
    "# Print the models\n",
    "print(f\"Original model:\\n{model_S}\\n\")\n",
    "print(f\"Modified model:\\n{model_quant_S}\\n\")\n",
    "\n",
    "# Get the summary of the models\n",
    "org_model_S = summary(model_S, input_size=(1, 3, 224, 224), verbose=0)\n",
    "mod_model_S = summary(model_quant_S, input_size=(1, 3, 224, 224), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gpu_inference_latency = measure_inference_latency(model, device=\"cuda\", input_shape=(3, 224, 224))\n",
    "model_quant_gpu_inference_latency = measure_inference_latency(model_quant, device=\"cuda\", input_shape=(3, 224, 224))\n",
    "\n",
    "model_cpu_inference_latency = measure_inference_latency(model, device=\"cpu\", input_shape=(3, 224, 224))\n",
    "model_quant_cpu_inference_latency = measure_inference_latency(model_quant, device=\"cpu\", input_shape=(3, 224, 224))\n",
    "\n",
    "print(\"LARGE MODEL\")\n",
    "print(f\"input_shape: {model.input_shape}, num_layers: {model.num_layers}, alpha: {model.alpha}, beta: {model.beta}, t_zero: {model.t_zero}\\n\")\n",
    "\n",
    "print(\"Number of parameters in the original model: {:,}\".format(org_model.total_params))\n",
    "print(\"Number of MAC operations in the original model: {:,}\".format(org_model.total_mult_adds))\n",
    "print(\"Original model GPU Inference Latency: {:.3f} ms\".format(model_gpu_inference_latency[0]))\n",
    "print(\"Original model CPU Inference Latency: {:.3f} ms\\n\".format(model_cpu_inference_latency[0]))\n",
    "\n",
    "print(\"Number of parameters in the modified model: {:,}\".format(mod_model.total_params))\n",
    "print(\"Number of MAC operations in the modified model: {:,}\".format(mod_model.total_mult_adds))\n",
    "print(\"Modified model GPU Inference Latency: {:.3f} ms\".format(model_quant_gpu_inference_latency[0]))\n",
    "print(\"Modified model CPU Inference Latency: {:.3f} ms\\n\".format(model_quant_cpu_inference_latency[0]))\n",
    "\n",
    "print(\"SMALL MODEL\")\n",
    "print(f\"input_shape: {model_S.input_shape}, num_layers: {model_S.num_layers}, alpha: {model_S.alpha}, beta: {model_S.beta}, t_zero: {model_S.t_zero}\\n\")\n",
    "\n",
    "model_gpu_inference_latency = measure_inference_latency(model_S, device=\"cuda\", input_shape=(3, 224, 224))\n",
    "model_quant_gpu_inference_latency = measure_inference_latency(model_quant_S, device=\"cuda\", input_shape=(3, 224, 224))\n",
    "\n",
    "model_cpu_inference_latency = measure_inference_latency(model_S, device=\"cpu\", input_shape=(3, 224, 224))\n",
    "model_quant_cpu_inference_latency = measure_inference_latency(model_quant_S, device=\"cpu\", input_shape=(3, 224, 224))\n",
    "\n",
    "print(\"Number of parameters in the original model: {:,}\".format(org_model_S.total_params))\n",
    "print(\"Number of MAC operations in the original model: {:,}\".format(org_model_S.total_mult_adds))\n",
    "print(\"Original model GPU Inference Latency: {:.3f} ms\".format(model_gpu_inference_latency[0]))\n",
    "print(\"Original model CPU Inference Latency: {:.3f} ms\\n\".format(model_cpu_inference_latency[0]))\n",
    "\n",
    "print(\"Number of parameters in the modified model: {:,}\".format(mod_model_S.total_params))\n",
    "print(\"Number of MAC operations in the modified model: {:,}\".format(mod_model_S.total_mult_adds))\n",
    "print(\"Modified model GPU Inference Latency: {:.3f} ms\".format(model_quant_gpu_inference_latency[0]))\n",
    "print(\"Modified model CPU Inference Latency: {:.3f} ms\".format(model_quant_cpu_inference_latency[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model performance\n",
    "\n",
    "Compare the performance of the original model and the modified model by changing:\n",
    "- Number of layers;\n",
    "- $\\alpha$;\n",
    "- $\\beta$;\n",
    "- t<sub>0</sub>;\n",
    "\n",
    "<img src=\"image.png\" alt=\"Alt text\" width=80% height=80%>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(opt_values_x, opt_values_y, org_values_x, org_values_y, opt_label, org_label, x_label, y_label, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(opt_values_x, opt_values_y, color='orange', label=opt_label)\n",
    "    plt.scatter(org_values_x, org_values_y, color='blue', label=org_label)\n",
    "    plt.legend()\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def get_inference_data(num_layers = 7, alpha = 3, beta = 0.75, t_zero = 6):\n",
    "    # Create an optimized model\n",
    "    model_opt = phinet_quant.PhiNet(input_shape=[3, 224, 224], num_layers=num_layers, alpha=alpha, beta=beta, t_zero=t_zero, include_top=True, num_classes=10)\n",
    "    mod_model = summary(model_opt, input_size=(1, 3, 224, 224), verbose=0)\n",
    "\n",
    "    opt_inference_time_GPU = measure_inference_latency(model_opt, device=\"cuda\", input_shape=(3, 224, 224))[0]\n",
    "    opt_inference_time_CPU = measure_inference_latency(model_opt, device=\"cpu\", input_shape=(3, 224, 224))[0]\n",
    "    opt_num_params = mod_model.total_params\n",
    "    opt_num_macs = mod_model.total_mult_adds\n",
    "\n",
    "    # Create a non-optimized model\n",
    "    model = micromind.PhiNet(input_shape=[3, 224, 224], num_layers=num_layers, alpha=alpha, beta=beta, t_zero=t_zero, include_top=True, num_classes=10)\n",
    "    mod_model = summary(model, input_size=(1, 3, 224, 224), verbose=0)\n",
    "\n",
    "    inference_time_GPU = measure_inference_latency(model, device=\"cuda\", input_shape=(3, 224, 224))[0]\n",
    "    inference_time_CPU = measure_inference_latency(model, device=\"cpu\", input_shape=(3, 224, 224))[0]\n",
    "    num_params = mod_model.total_params\n",
    "    num_macs = mod_model.total_mult_adds\n",
    "\n",
    "    return opt_inference_time_GPU, opt_inference_time_CPU, opt_num_params, opt_num_macs, inference_time_GPU, inference_time_CPU, num_params, num_macs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_layers = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "(opt_inference_time_list_GPU, opt_inference_time_list_CPU, opt_num_params_list, opt_num_macs_list,\n",
    " inference_time_list_GPU, inference_time_list_CPU, num_params_list, num_macs_list) = zip(*[get_inference_data(num_layers = layer) for layer in nr_layers])\n",
    "\n",
    "# Plot inference_time_list_GPU vs num_params_list\n",
    "plot(opt_num_params_list, \n",
    "                                        opt_inference_time_list_GPU, \n",
    "                                        num_params_list,\n",
    "                                        inference_time_list_GPU,                                        \n",
    "                                        \"Optimized PhiNet\", \n",
    "                                        \"Original PhiNet\",\n",
    "                                        \"Number of Parameters\", \n",
    "                                        \"GPU Inference Time (ms)\", \n",
    "                                        \"GPU Inference Time vs Number of Parameters\"\n",
    ")\n",
    "\n",
    "# Plot inference_time_list_CPU vs num_params_list\n",
    "plot(opt_num_params_list, \n",
    "                                        opt_inference_time_list_CPU, \n",
    "                                        num_params_list,\n",
    "                                        inference_time_list_CPU,                                        \n",
    "                                        \"Optimized PhiNet\", \n",
    "                                        \"Original PhiNet\",\n",
    "                                        \"Number of Parameters\", \n",
    "                                        \"CPU Inference Time (ms)\", \n",
    "                                        \"CPU Inference Time vs Number of Parameters\"\n",
    ")\n",
    "\n",
    "# Plot inference_time_list_GPU vs num_macs_list\n",
    "plot(opt_num_macs_list, \n",
    "                                        opt_inference_time_list_GPU, \n",
    "                                        num_macs_list,\n",
    "                                        inference_time_list_GPU,                                        \n",
    "                                        \"Optimized PhiNet\", \n",
    "                                        \"Original PhiNet\",\n",
    "                                        \"Number of MACs\", \n",
    "                                        \"GPU Inference Time (ms)\", \n",
    "                                        \"GPU Inference Time vs Number of MACs\"\n",
    ")\n",
    "\n",
    "# Plot inference_time_list_CPU vs num_macs_list\n",
    "plot(opt_num_macs_list, \n",
    "                                        opt_inference_time_list_CPU, \n",
    "                                        num_macs_list,\n",
    "                                        inference_time_list_CPU,                                        \n",
    "                                        \"Optimized PhiNet\", \n",
    "                                        \"Original PhiNet\",\n",
    "                                        \"Number of MACs\", \n",
    "                                        \"CPU Inference Time (ms)\", \n",
    "                                        \"CPU Inference Time vs Number of MACs\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace(0.25, 10, 15)\n",
    "\n",
    "(opt_inference_time_list_GPU, opt_inference_time_list_CPU, opt_num_params_list, opt_num_macs_list,\n",
    " inference_time_list_GPU, inference_time_list_CPU, num_params_list, num_macs_list) = zip(*[get_inference_data(alpha = alpha) for alpha in alphas])\n",
    "\n",
    "# Plot inference_time_list_GPU vs num_params_list\n",
    "plot(opt_num_params_list, \n",
    "                                        opt_inference_time_list_GPU, \n",
    "                                        num_params_list,\n",
    "                                        inference_time_list_GPU,                                        \n",
    "                                        \"Optimized PhiNet\", \n",
    "                                        \"Original PhiNet\",\n",
    "                                        \"Number of Parameters\", \n",
    "                                        \"GPU Inference Time (ms)\", \n",
    "                                        \"GPU Inference Time vs Number of Parameters\"\n",
    ")\n",
    "\n",
    "# Plot inference_time_list_CPU vs num_params_list\n",
    "plot(opt_num_params_list, \n",
    "                                        opt_inference_time_list_CPU, \n",
    "                                        num_params_list,\n",
    "                                        inference_time_list_CPU,                                        \n",
    "                                        \"Optimized PhiNet\", \n",
    "                                        \"Original PhiNet\",\n",
    "                                        \"Number of Parameters\", \n",
    "                                        \"CPU Inference Time (ms)\", \n",
    "                                        \"CPU Inference Time vs Number of Parameters\"\n",
    ")\n",
    "\n",
    "# Plot inference_time_list_GPU vs num_macs_list\n",
    "plot(opt_num_macs_list, \n",
    "                                        opt_inference_time_list_GPU, \n",
    "                                        num_macs_list,\n",
    "                                        inference_time_list_GPU,                                        \n",
    "                                        \"Optimized PhiNet\", \n",
    "                                        \"Original PhiNet\",\n",
    "                                        \"Number of MACs\", \n",
    "                                        \"GPU Inference Time (ms)\", \n",
    "                                        \"GPU Inference Time vs Number of MACs\"\n",
    ")\n",
    "\n",
    "# Plot inference_time_list_CPU vs num_macs_list\n",
    "plot(opt_num_macs_list, \n",
    "                                        opt_inference_time_list_CPU, \n",
    "                                        num_macs_list,\n",
    "                                        inference_time_list_CPU,                                        \n",
    "                                        \"Optimized PhiNet\", \n",
    "                                        \"Original PhiNet\",\n",
    "                                        \"Number of MACs\", \n",
    "                                        \"CPU Inference Time (ms)\", \n",
    "                                        \"CPU Inference Time vs Number of MACs\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = np.linspace(0.25, 1, 15)\n",
    "\n",
    "(opt_inference_time_list_GPU, opt_inference_time_list_CPU, opt_num_params_list, opt_num_macs_list,\n",
    " inference_time_list_GPU, inference_time_list_CPU, num_params_list, num_macs_list) = zip(*[get_inference_data(beta = beta) for beta in betas])\n",
    "\n",
    "# Plot inference_time_list_GPU vs num_params_list\n",
    "plot(opt_num_params_list, \n",
    "                                        opt_inference_time_list_GPU, \n",
    "                                        num_params_list,\n",
    "                                        inference_time_list_GPU,                                        \n",
    "                                        \"Optimized PhiNet\", \n",
    "                                        \"Original PhiNet\",\n",
    "                                        \"Number of Parameters\", \n",
    "                                        \"GPU Inference Time (ms)\", \n",
    "                                        \"GPU Inference Time vs Number of Parameters\"\n",
    ")\n",
    "\n",
    "# Plot inference_time_list_CPU vs num_params_list\n",
    "plot(opt_num_params_list, \n",
    "                                        opt_inference_time_list_CPU, \n",
    "                                        num_params_list,\n",
    "                                        inference_time_list_CPU,                                        \n",
    "                                        \"Optimized PhiNet\", \n",
    "                                        \"Original PhiNet\",\n",
    "                                        \"Number of Parameters\", \n",
    "                                        \"CPU Inference Time (ms)\", \n",
    "                                        \"CPU Inference Time vs Number of Parameters\"\n",
    ")\n",
    "\n",
    "# Plot inference_time_list_GPU vs num_macs_list\n",
    "plot(opt_num_macs_list, \n",
    "                                        opt_inference_time_list_GPU, \n",
    "                                        num_macs_list,\n",
    "                                        inference_time_list_GPU,                                        \n",
    "                                        \"Optimized PhiNet\", \n",
    "                                        \"Original PhiNet\",\n",
    "                                        \"Number of MACs\", \n",
    "                                        \"GPU Inference Time (ms)\", \n",
    "                                        \"GPU Inference Time vs Number of MACs\"\n",
    ")\n",
    "\n",
    "# Plot inference_time_list_CPU vs num_macs_list\n",
    "plot(opt_num_macs_list, \n",
    "                                        opt_inference_time_list_CPU, \n",
    "                                        num_macs_list,\n",
    "                                        inference_time_list_CPU,                                        \n",
    "                                        \"Optimized PhiNet\", \n",
    "                                        \"Original PhiNet\",\n",
    "                                        \"Number of MACs\", \n",
    "                                        \"CPU Inference Time (ms)\", \n",
    "                                        \"CPU Inference Time vs Number of MACs\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t<sub>0</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_zeros = np.linspace(1, 8, 7)\n",
    "\n",
    "(opt_inference_time_list_GPU, opt_inference_time_list_CPU, opt_num_params_list, opt_num_macs_list,\n",
    " inference_time_list_GPU, inference_time_list_CPU, num_params_list, num_macs_list) = zip(*[get_inference_data(t_zero= t_zero) for t_zero in t_zeros])\n",
    "\n",
    "# Plot inference_time_list_GPU vs num_params_list\n",
    "plot(opt_num_params_list, \n",
    "                                        opt_inference_time_list_GPU, \n",
    "                                        num_params_list,\n",
    "                                        inference_time_list_GPU,                                        \n",
    "                                        \"Optimized PhiNet\", \n",
    "                                        \"Original PhiNet\",\n",
    "                                        \"Number of Parameters\", \n",
    "                                        \"GPU Inference Time (ms)\", \n",
    "                                        \"GPU Inference Time vs Number of Parameters\"\n",
    ")\n",
    "\n",
    "# Plot inference_time_list_CPU vs num_params_list\n",
    "plot(opt_num_params_list, \n",
    "                                        opt_inference_time_list_CPU, \n",
    "                                        num_params_list,\n",
    "                                        inference_time_list_CPU,                                        \n",
    "                                        \"Optimized PhiNet\", \n",
    "                                        \"Original PhiNet\",\n",
    "                                        \"Number of Parameters\", \n",
    "                                        \"CPU Inference Time (ms)\", \n",
    "                                        \"CPU Inference Time vs Number of Parameters\"\n",
    ")\n",
    "\n",
    "# Plot inference_time_list_GPU vs num_macs_list\n",
    "plot(opt_num_macs_list, \n",
    "                                        opt_inference_time_list_GPU, \n",
    "                                        num_macs_list,\n",
    "                                        inference_time_list_GPU,                                        \n",
    "                                        \"Optimized PhiNet\", \n",
    "                                        \"Original PhiNet\",\n",
    "                                        \"Number of MACs\", \n",
    "                                        \"GPU Inference Time (ms)\", \n",
    "                                        \"GPU Inference Time vs Number of MACs\"\n",
    ")\n",
    "\n",
    "# Plot inference_time_list_CPU vs num_macs_list\n",
    "plot(opt_num_macs_list, \n",
    "                                        opt_inference_time_list_CPU, \n",
    "                                        num_macs_list,\n",
    "                                        inference_time_list_CPU,                                        \n",
    "                                        \"Optimized PhiNet\", \n",
    "                                        \"Original PhiNet\",\n",
    "                                        \"Number of MACs\", \n",
    "                                        \"CPU Inference Time (ms)\", \n",
    "                                        \"CPU Inference Time vs Number of MACs\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
