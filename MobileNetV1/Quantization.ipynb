{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNetV1 Quantization\n",
    "Notebook to quantize MobileNetV1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from pytorchcv.models.mobilenet import mobilenet_w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    \"\"\"\n",
    "    Print the size of the model.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size_in_bytes = os.path.getsize(\"temp.p\")\n",
    "    if size_in_bytes < 1048576:\n",
    "        size_in_kb = size_in_bytes / 1024\n",
    "        print(\"{:.3f} KB\".format(size_in_kb))\n",
    "    else:\n",
    "        size_in_mb = size_in_bytes / 1048576\n",
    "        print(\"{:.3f} MB\".format(size_in_mb))\n",
    "    os.remove('temp.p')\n",
    "\n",
    "\n",
    "def measure_inference_latency(model, input_shape, device = None, repetitions=100, warmup_it = 10):\n",
    "    \"\"\"\n",
    "    Measures the inference time of the provided neural network model.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to evaluate.\n",
    "        input_shape: The shape of the input data expected by the model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the mean and standard deviation of the inference time\n",
    "               measured in milliseconds.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device.type  # Get the device where the model is located\n",
    "    \n",
    "    dummy_input = torch.randn(1, *input_shape, dtype=torch.float).to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # GPU warm-up\n",
    "    for _ in range(warmup_it):\n",
    "        _ = model(dummy_input)\n",
    "\n",
    "    # Measure inference time\n",
    "    timings = []\n",
    "    with torch.no_grad():\n",
    "        if device == 'cuda':\n",
    "            starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "            for rep in range(repetitions):\n",
    "                starter.record()\n",
    "                _ = model(dummy_input)\n",
    "                ender.record()\n",
    "                torch.cuda.synchronize()\n",
    "                curr_time = starter.elapsed_time(ender)\n",
    "                timings.append(curr_time)\n",
    "        else:  # CPU\n",
    "            for rep in range(repetitions):\n",
    "                start_time = time.time()\n",
    "                _ = model(dummy_input)\n",
    "                end_time = time.time()\n",
    "                elapsed_time = (end_time - start_time) * 1000.0  # Convert to milliseconds\n",
    "                timings.append(elapsed_time)\n",
    "\n",
    "    # Calculate mean and std\n",
    "    mean_time = np.mean(timings)\n",
    "    std_time = np.std(timings)\n",
    "\n",
    "    return mean_time, std_time\n",
    "\n",
    "\n",
    "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32), verbose=False):\n",
    "    \"\"\"\n",
    "    Tests whether two models are equivalent by comparing their outputs on random inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    model_1.to(device)\n",
    "    model_2.to(device)\n",
    "\n",
    "    for i in range(num_tests):\n",
    "        print(f\"Running test {i+1}/{num_tests}\") if verbose else None\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        y1 = model_1(x).detach().cpu().numpy()\n",
    "        y2 = model_2(x).detach().cpu().numpy()\n",
    "        print(\"Difference: \", np.max(np.abs(y1-y2))) if verbose else None\n",
    "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
    "            print(\"Model equivalence test sample failed: \")\n",
    "            print(y1)\n",
    "            print(y2)\n",
    "            return False\n",
    "    print(\"Model equivalence test passed!\")\n",
    "    return True\n",
    "\n",
    "def set_random_seeds(random_seed=0):\n",
    "    \"\"\"\n",
    "    Set all random seeds to a fixed value to make results reproducible.\n",
    "    \"\"\"\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train(model, train_loader, device, learning_rate=1e-1, num_epochs=200, save_dir=\"saved_models\"):\n",
    "    # Create the save directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    tot_exp_time = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Initialize the timer\n",
    "        starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "        # Start Recording the time\n",
    "        starter.record()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            prob = nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = prob.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "\n",
    "        # Save the model state dictionary at the end of each epoch\n",
    "        model_save_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pt\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "        # Save some statics the be saved in output\n",
    "        tot_exp_time += curr_time\n",
    "        accuracy = 100.0 * correct / total\n",
    "        average_loss = train_loss / len(train_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} : Train accuracy {accuracy:.2f}%, Train loss {average_loss:.4f}, Training Time: {curr_time/1000:.3f} s\")\n",
    "\n",
    "    return model, tot_exp_time\n",
    "\n",
    "def test(model, test_loader, device, criterion= nn.CrossEntropyLoss()):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            prob = nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = prob.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    average_loss = test_loss / len(test_loader)\n",
    "\n",
    "    return accuracy, average_loss\n",
    "\n",
    "def calibrate_model(model, loader, device=torch.device(\"cpu\")):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        _ = model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the CIFAR10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(num_workers=1, train_batch_size=128, eval_batch_size=256):\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    train_set = torchvision.datasets.CIFAR10(root='./data', train=True, transform=train_transform, download=True)\n",
    "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, transform=train_transform)\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set, batch_size=train_batch_size,\n",
    "        sampler=train_sampler, num_workers=num_workers)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_set, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the function to fuse the MobileNetV1 layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchcv.models.common import ConvBlock\n",
    "\n",
    "def fuse_mobilenet(model):\n",
    "    for basic_block in model.children():\n",
    "        if isinstance(basic_block, ConvBlock):\n",
    "            torch.ao.quantization.fuse_modules(basic_block, [[\"conv\", \"bn\", \"activ\"]], inplace=True)\n",
    "        else:\n",
    "            basic_block = fuse_mobilenet(basic_block)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of MobileNetV1 on CIFAR10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cifar_fp32 = mobilenet_w1(pretrained=True)\n",
    "print(model_cifar_fp32)\n",
    "\n",
    "train_loader, test_loader = prepare_dataloader(num_workers=1, train_batch_size=256, eval_batch_size=256)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_cifar_fp32, train_time_model_cifar_fp32 = train(model_cifar_fp32, train_loader, device, learning_rate=0.001, num_epochs=10, save_dir=\"orignal_model\")\n",
    "\n",
    "acc_model_cifar_fp32, _ = test(model_cifar_fp32, test_loader, device)\n",
    "\n",
    "print(f\"Training Time: {train_time_model_cifar_fp32/1000:.3f} s\")\n",
    "print(f\"Test Accuracy: {acc_model_cifar_fp32:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static Quantization of the modified PhiNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelQuant(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(ModelQuant, self).__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized.\n",
    "        # This will only be used for inputs.\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        # FP32 model\n",
    "        self.model_fp32 = model_fp32\n",
    "        # DeQuantStub converts tensors from quantized to floating point.\n",
    "        # This will only be used for outputs.\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(random_seed=1)\n",
    "\n",
    "# Copy the pretrained model for static quantization\n",
    "model_st_cifar_fp32 = copy.deepcopy(model_cifar_fp32)\n",
    "model_st_cifar_fp32.to(\"cpu\")\n",
    "\n",
    "# Create a copy of the model for fusion\n",
    "model_st_cifar_fp32_fused = copy.deepcopy(model_st_cifar_fp32)\n",
    "\n",
    "# Set the models to eval mode (important for fusion)\n",
    "model_st_cifar_fp32.eval()\n",
    "model_st_cifar_fp32_fused.eval()\n",
    "\n",
    "# Fuse Conv, BN, ReLu modules in the MobileNetV1 model\n",
    "fuse_mobilenet(model_st_cifar_fp32_fused)\n",
    "\n",
    "print(f\"model_fp32:\\n {model_st_cifar_fp32}\\n\")\n",
    "print(f\"model_fp32_fused:\\n {model_st_cifar_fp32_fused}\\n\")\n",
    "\n",
    "assert model_equivalence(model_1=model_st_cifar_fp32, model_2=model_st_cifar_fp32_fused, device=\"cpu\", rtol=1e-01, atol=1e-04, num_tests=100, input_size=(1,3,224,224)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "# Insert stubs\n",
    "model_st_cifar_int8 = ModelQuant(model_st_cifar_fp32_fused)\n",
    "\n",
    "quantization_config = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "model_st_cifar_int8.qconfig = quantization_config\n",
    "\n",
    "# Prepare the model for static quantization. \n",
    "torch.ao.quantization.prepare(model_st_cifar_int8, inplace=True)\n",
    "\n",
    "print(f\"After preparation, note fake-quantization modules:\\n {model_st_cifar_int8}\\n\")\n",
    "\n",
    "# Calibrate the model\n",
    "calibrate_model(model_st_cifar_int8, train_loader, device=\"cpu\")\n",
    "\n",
    "# Convert the observed model to a quantized model.\n",
    "model_st_cifar_int8 = torch.ao.quantization.convert(model_st_cifar_int8, inplace=True)\n",
    "\n",
    "print(f\"model_int8:\\n {model_st_cifar_int8}\\n\")\n",
    "\n",
    "acc_model_st_cifar_int8, _ = test(model_st_cifar_int8, test_loader, \"cpu\")\n",
    "\n",
    "print(f\"Test Accuracy: {acc_model_st_cifar_int8:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comper the dimension of the FP32 model and of the INT8 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FP32 size:\")\n",
    "print_size_of_model(model_cifar_fp32)\n",
    "\n",
    "print(\"INT8 size:\")\n",
    "print_size_of_model(model_st_cifar_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare results on **CIFAR10**:\n",
    "- Orginal model FP32;\n",
    "- Static Quantization of the modified model INT8;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32_cpu_inference_latency  = measure_inference_latency(model_cifar_fp32, device=\"cpu\", input_shape=(3, 224, 224))\n",
    "model_fp32_gpu_inference_latency  = measure_inference_latency(model_cifar_fp32, device=\"cuda\", input_shape=(3, 224, 224))\n",
    "\n",
    "model_st_int8_cpu_inference_latency  = measure_inference_latency(model_st_cifar_int8, device=\"cpu\", input_shape=(3, 224, 224))\n",
    "\n",
    "print(\"FP32 model CPU Inference Latency: {:.3f} ms\".format(model_fp32_cpu_inference_latency[0]))\n",
    "print(\"FP32 model GPU Inference Latency: {:.3f} ms\".format(model_fp32_gpu_inference_latency[0]))\n",
    "print(\"FP32 model test accuracy: {:.2f}%\".format(acc_model_cifar_fp32))\n",
    "print(\"FP32 model training time: {:.3f} s\\n\".format(train_time_model_cifar_fp32/1000))\n",
    "\n",
    "\n",
    "print(\"INT8 model static quant CPU Inference Latency: {:.3f} ms\".format(model_st_int8_cpu_inference_latency[0]))\n",
    "print(\"INT8 model static quant test accuracy: {:.2f}%\\n\".format(acc_model_st_cifar_int8))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
